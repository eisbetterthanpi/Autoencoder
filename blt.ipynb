{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4n3vdnZvBrBT",
        "6e_ZnJPtzrzr",
        "SqVRFiA3j9gR"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2517ec32177947ccbcd3622357123c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9eb5cacf796413d99ee7f949a507613",
              "IPY_MODEL_61b1cdf1961f48649623ec7271e20a78",
              "IPY_MODEL_db2fac4c647c4b6997bf2b16d8a36daa"
            ],
            "layout": "IPY_MODEL_39321216ca844c48b12b1785bc7a93cb"
          }
        },
        "a9eb5cacf796413d99ee7f949a507613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebf6c3eee0bc4974b8c9757ea6983b1b",
            "placeholder": "​",
            "style": "IPY_MODEL_bd7b56962e044f7db1adac35f4744892",
            "value": "README.md: 100%"
          }
        },
        "61b1cdf1961f48649623ec7271e20a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7064d06749ec4bc5853584e806020455",
            "max": 7351,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61c822aede8045ec842b5e5cf20449f7",
            "value": 7351
          }
        },
        "db2fac4c647c4b6997bf2b16d8a36daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15d2236a35f048fc885f0146e819a5f3",
            "placeholder": "​",
            "style": "IPY_MODEL_e96242ad5fab48c58f4493d1860c7fa5",
            "value": " 7.35k/7.35k [00:00&lt;00:00, 786kB/s]"
          }
        },
        "39321216ca844c48b12b1785bc7a93cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebf6c3eee0bc4974b8c9757ea6983b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd7b56962e044f7db1adac35f4744892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7064d06749ec4bc5853584e806020455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61c822aede8045ec842b5e5cf20449f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15d2236a35f048fc885f0146e819a5f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e96242ad5fab48c58f4493d1860c7fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68a938faed934041aa9bf230905818db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e29a5f5de9ee415eb7f7e75d755ddb3c",
              "IPY_MODEL_e8c46ddb24e74d768a92f23baa9dd015",
              "IPY_MODEL_e04899a7ada84a21855e3fd1bc10069b"
            ],
            "layout": "IPY_MODEL_76d10d1f614c4a8a822c18aff41b2a39"
          }
        },
        "e29a5f5de9ee415eb7f7e75d755ddb3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38b489825c9a43969ae8217b9697e6b4",
            "placeholder": "​",
            "style": "IPY_MODEL_c57ac4b2ae234ca08ed96022841598d1",
            "value": "openwebtext.py: 100%"
          }
        },
        "e8c46ddb24e74d768a92f23baa9dd015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05fe568b09784260bd32b7a47cd837be",
            "max": 2726,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_667315d3df7d44ed83027819a74bf030",
            "value": 2726
          }
        },
        "e04899a7ada84a21855e3fd1bc10069b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_680fa34e03fc489ab15a3c6a4fe44c52",
            "placeholder": "​",
            "style": "IPY_MODEL_70961e8ecaa44bbf9cd039554e365f87",
            "value": " 2.73k/2.73k [00:00&lt;00:00, 77.7kB/s]"
          }
        },
        "76d10d1f614c4a8a822c18aff41b2a39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38b489825c9a43969ae8217b9697e6b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c57ac4b2ae234ca08ed96022841598d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05fe568b09784260bd32b7a47cd837be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "667315d3df7d44ed83027819a74bf030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "680fa34e03fc489ab15a3c6a4fe44c52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70961e8ecaa44bbf9cd039554e365f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/Autoencoder/blob/main/blt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## shelf"
      ],
      "metadata": {
        "id": "7iTtdzCk_iQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hf stream dataset me\n",
        "!pip install -qU datasets # restart?\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import tiktoken # https://github.com/openai/tiktoken/tree/main\n",
        "\n",
        "class StreamDataset(Dataset):\n",
        "    def __init__(self, dataset, seq_len, buffer_size):\n",
        "        self.enc = tiktoken.get_encoding(\"gpt2\") # https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "        self.vocab_size = self.enc.n_vocab # gpt2:50257\n",
        "        self.dataset = dataset\n",
        "        self.data = iter(dataset)\n",
        "        self.seq_len = seq_len\n",
        "        self.buffer_size = buffer_size  # must be ≥ seq_len\n",
        "        self.buffer = []  # token buffer\n",
        "        self.fill_buffer()\n",
        "\n",
        "    def fill_buffer(self):\n",
        "        while len(self.buffer) < self.buffer_size:\n",
        "            x = next(self.data)\n",
        "            tokens = self.enc.encode(x[\"text\"]) # tiktoken\n",
        "            self.buffer.extend(tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        # /4.5/(4/3)\n",
        "        return 128000000\n",
        "        # return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print('get', idx)\n",
        "        if idx == 0: self.data = iter(self.dataset)\n",
        "        if len(self.buffer) < self.seq_len: self.fill_buffer()\n",
        "        if len(self.buffer) < self.seq_len:\n",
        "            raise StopIteration\n",
        "        x = self.buffer[:self.seq_len]\n",
        "        self.buffer = self.buffer[self.seq_len:]\n",
        "        return torch.tensor(x)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # print(batch)\n",
        "    return torch.stack(batch)\n",
        "\n",
        "# dataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\", streaming=True, cache_dir=\"/content/hf\")\n",
        "# dataset = load_dataset(\"maxtli/OpenWebText-2M\", split=\"train\", streaming=True, cache_dir=\"/content/hf\") # 8.7,3.8\n",
        "dataset = load_dataset(\"Skylion007/openwebtext\", trust_remote_code=True, split=\"train\", streaming=True, cache_dir=\"/content/hf\") # 8.7,3.8\n",
        "\n",
        "seq_len = 128\n",
        "buffer_size = seq_len*1\n",
        "train_data = StreamDataset(dataset, seq_len, buffer_size) # train_data = StreamDataset(dataset[\"train\"], seq_len, buffer_size)\n",
        "del dataset\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=2)\n",
        "del train_data\n",
        "def encode(context):\n",
        "    if type(context) == str: return torch.tensor([train_loader.dataset.enc.encode(context)], device=device)\n",
        "    elif type(context) == list: return train_loader.dataset.enc.encode_batch(context)\n",
        "    else: raise Exception\n",
        "def decode(x): return train_loader.dataset.enc.decode(list(x))\n",
        "# for x,y in train_loader:\n",
        "#     break\n",
        "# print(train_data.vocab_size)\n"
      ],
      "metadata": {
        "id": "WM-Y1OVbJ53X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463,
          "referenced_widgets": [
            "2517ec32177947ccbcd3622357123c1b",
            "a9eb5cacf796413d99ee7f949a507613",
            "61b1cdf1961f48649623ec7271e20a78",
            "db2fac4c647c4b6997bf2b16d8a36daa",
            "39321216ca844c48b12b1785bc7a93cb",
            "ebf6c3eee0bc4974b8c9757ea6983b1b",
            "bd7b56962e044f7db1adac35f4744892",
            "7064d06749ec4bc5853584e806020455",
            "61c822aede8045ec842b5e5cf20449f7",
            "15d2236a35f048fc885f0146e819a5f3",
            "e96242ad5fab48c58f4493d1860c7fa5",
            "68a938faed934041aa9bf230905818db",
            "e29a5f5de9ee415eb7f7e75d755ddb3c",
            "e8c46ddb24e74d768a92f23baa9dd015",
            "e04899a7ada84a21855e3fd1bc10069b",
            "76d10d1f614c4a8a822c18aff41b2a39",
            "38b489825c9a43969ae8217b9697e6b4",
            "c57ac4b2ae234ca08ed96022841598d1",
            "05fe568b09784260bd32b7a47cd837be",
            "667315d3df7d44ed83027819a74bf030",
            "680fa34e03fc489ab15a3c6a4fe44c52",
            "70961e8ecaa44bbf9cd039554e365f87"
          ]
        },
        "outputId": "511abf6e-6e44-4a98-8c9f-8241688ee112",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/7.35k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2517ec32177947ccbcd3622357123c1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "openwebtext.py:   0%|          | 0.00/2.73k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68a938faed934041aa9bf230905818db"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title -1,1 code quant\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "def soft_clamp_relu(x, lower=None, upper=None):\n",
        "    if lower != None: x = lower+F.relu(x-lower)\n",
        "    if upper != None: x = upper-F.relu(upper-x)\n",
        "    return x\n",
        "\n",
        "class codequant(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        # self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "    @property\n",
        "    def codebook(self): return self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = soft_clamp_relu(zhat,-1,1)\n",
        "        zhat = (zhat*torch.pi/2).sin() # cos to unif\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1).round().int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # return codes / self.half_width - 1\n",
        "        codes = codes / self.half_width - 1\n",
        "        return soft_clamp_relu(codes,-1,1).asin()/torch.pi*2 # unif to cos\n",
        "\n",
        "    def forward(self, z): # round code to nearest code\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        z = soft_clamp_relu(z,-1,1)\n",
        "        z = (z*torch.pi/2).sin() # cos to unif\n",
        "        # print('codequant fwd', z)\n",
        "        bound = z * self.half_width + offset\n",
        "        # print('codequant fwd', bound)\n",
        "        quantized = ste_round(bound)\n",
        "        # return (quantized-offset) / self.half_width # split [-1,1]\n",
        "        zhat = (quantized-offset) / self.half_width # split [-1,1]\n",
        "        return soft_clamp_relu(zhat,-1,1).asin()/torch.pi*2 # unif to cos\n",
        "\n",
        "# cq = codequant(levels = [128,4,3,2])\n",
        "# # print(cq.codebook)\n",
        "# batch_size, seq_len = 2, 4\n",
        "# # x = torch.linspace(-1.2,1.2,23).repeat(4,1).T\n",
        "# x = torch.linspace(-1.2,1.2,29).repeat(4,1).T\n",
        "# # x=la\n",
        "# la = cq(x)\n",
        "# print(la)\n",
        "# lact = cq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "# la = cq.indexes_to_codes(lact)\n",
        "# print(la)\n"
      ],
      "metadata": {
        "id": "S-yPGFICB3bD",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TTLinear\n",
        "# Tensor Train embedding https://arxiv.org/pdf/1901.10787\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def make_einsum(num_tensors):\n",
        "    a = 97\n",
        "    R = chr(a+25) # 'z'\n",
        "    lhs = [chr(a)+R]\n",
        "    for i in range(1, num_tensors-1):lhs.append(R+chr(a+i)+R)\n",
        "    lhs.append(R+chr(a+num_tensors-1))\n",
        "    return ','.join(lhs) + '->' + ''.join([chr(a+i) for i in range(num_tensors)]) # az,zbz,zcz,zd->abcd\n",
        "\n",
        "class TTLinear(nn.Module):\n",
        "    def __init__(self, in_features=None, out_features=None, rank=256, std=1):\n",
        "        super().__init__()\n",
        "        self.lfeat = len(in_features)\n",
        "        if self.lfeat==1: lst = in_features + out_features\n",
        "        elif self.lfeat>=2: lst = [i*j for i, j in zip(in_features, out_features)]\n",
        "        last = len(lst)\n",
        "        var = last/rank**(1/(2*(std**.5)*last))\n",
        "        c=1/last\n",
        "        self.params = nn.ParameterList([nn.Parameter(torch.randn(lst[0], rank).clamp(-c,c)*var),\n",
        "            *[nn.Parameter(torch.randn(rank, ij, rank).clamp(-c,c)*var) for ij in lst[1:-1]],\n",
        "            nn.Parameter(torch.randn(rank, lst[-1]).clamp(-c,c)*var)])\n",
        "        self.einsum_str = make_einsum(last)\n",
        "        self.shape = [p for ij in zip(in_features, out_features) for p in ij]\n",
        "        self.permute = list(range(0, 2*self.lfeat - 1, 2)) + list(range(1, 2*self.lfeat, 2))\n",
        "    def weight(self): return torch.einsum(self.einsum_str, *self.params).reshape(self.shape).permute(self.permute).flatten(0,self.lfeat-1).flatten(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.weight()\n",
        "        return x.to(weight.dtype) @ weight\n",
        "\n",
        "\n",
        "import math\n",
        "class TTEmbedding(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, rank=256):\n",
        "        super().__init__()\n",
        "        self.ttlin = TTLinear(in_dim, d_model, rank, std=1) # https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "        self.weight = self.ttlin.weight\n",
        "        self.num_classes = math.prod(in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ttlin(F.one_hot(x, self.num_classes))\n",
        "# self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "\n",
        "# in_features=(3,4,5,6); out_features=(2,3,4,5)\n",
        "# in_features=[120]; out_features=[300]\n",
        "# rank=16\n",
        "# std=.5\n",
        "# lin = TTLinear(in_features, out_features, rank, std).to(device)\n",
        "# # x = torch.rand(4,math.prod((3,4,5,6)))\n",
        "# x = torch.rand(4,7,math.prod(in_features), device=device)\n",
        "# print(lin.params[0].device)\n",
        "# out = lin(x)\n",
        "# print(out.shape)\n",
        "# print(lin.ttlin.params[0].device)\n",
        "\n",
        "# emb = TTEmbedding(in_features, out_features, rank).to(device)\n",
        "# x = torch.randint(0, math.prod(in_features), (2, 5), device=device)\n",
        "# out = emb(x)\n",
        "# print(out.shape)\n",
        "\n",
        "# o=lin.weight\n",
        "# print(o.mean().item(), o.std().item(), o.min().item(), o.max().item())\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "# # plt.hist(o.flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# # plt.hist(o[:100,:100].flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# x = torch.randn(100,100)*std\n",
        "# # plt.hist(x.flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# plt.hist([o[:100,:100].flatten().tolist(), x.flatten().tolist()], bins=20, alpha=.5, label='context mask')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "q5bGqKAJzN7I",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[1] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:,:seq_len]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xdxN9d9tIrqL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PerceiverIO\n",
        "# https://github.com/lucidrains/perceiver-pytorch/blob/main/perceiver_pytorch/perceiver_io.py\n",
        "# https://arxiv.org/pdf/2107.14795\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, n_heads=None, d_head=8, dropout=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=64, drop=0):\n",
        "        super().__init__()\n",
        "        d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        # self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or query_dim, 2*d_model, bias=False)\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        k, v = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "        # q @ k.T # [qlen,d]@[d,klen]=[qlen,klen]\n",
        "        # q-> num_latent # k->seq_len\n",
        "        if mask is not None: mask = mask.unsqueeze(1)\n",
        "        # print('attn fwd',q.shape, k.shape, v.shape)\n",
        "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0) # F->mask # [batch, len_q, len_v]\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, d_head, ff_dim=None, dropout=0.):\n",
        "    def __init__(self, d_model, n_heads ,cond_dim=None, ff_dim=None, drop=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.cond_dim = cond_dim\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        if cond_dim!=None: self.norm2 = nn.RMSNorm(cond_dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.attn = Attention(d_model, cond_dim, n_heads=n_heads, d_head=d_model//n_heads)\n",
        "        act = nn.ReLU()\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act,\n",
        "            nn.RMSNorm(ff_dim), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # print('attblk fwd', x.shape, cond.shape if cond!=None else None, mask.shape if mask!=None else None)\n",
        "        if self.cond_dim==None: x = x + self.attn(self.norm1(x), mask)\n",
        "        else: x = x + self.attn(self.norm1(x), self.norm2(cond), mask) # maybe no res for decoder\n",
        "        x = x + self.ff(x) # maybe no ff for decoder?\n",
        "        return x\n",
        "\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            if 'mask' in layer._fwdparams: args.append(mask)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "model = Attention(64)\n",
        "\n"
      ],
      "metadata": {
        "id": "RAa6htQ58GKn",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Attention with kvcache window\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, n_heads=None, d_head=8, dropout=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=64, drop=0, w=128):\n",
        "        super().__init__()\n",
        "        d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        # self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or query_dim, 2*d_model, bias=False)\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        # self.cache = {'x':None, 'xq':None, 'xk':None, 'xv':None}\n",
        "        self.cache = {'x':None, 'q':None, 'kv':None}\n",
        "# gen: xqkv, patch: xkv ~pq, gen: xqkv pvv\n",
        "# so save xqkv\n",
        "        self.w = w\n",
        "        self.midx = torch.arange(w, dtype=torch.int32).repeat(t,1) + torch.arange(1-w, t-w+1, dtype=torch.int32).unsqueeze(-1) # [t,w,1]\n",
        "        self.scale = self.d_head**-.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t,d], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        # m = torch.rand_like(x)<.00001\n",
        "        # if self.cache['x']!=None and x.shape==self.cache['x'].shape and (x[m]==self.cache['x'][m]).all():\n",
        "        if self.cache['x']!=None and x.shape==self.cache['x'].shape and (x[0,0,0]==self.cache['x'][0,0,0]).all():\n",
        "            # print('cache')\n",
        "            q = self.cache['q']\n",
        "            if cond==None: kv = self.cache['kv']\n",
        "            else: kv = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2)#.chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "        else:\n",
        "            # print('no')\n",
        "            q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "            kv = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2)#.chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "            self.cache = {'x':x, 'q':q, 'kv':kv}\n",
        "        # q @ k.T # [qlen,d]@[d,klen]=[qlen,klen]\n",
        "        # q-> num_latent # k->seq_len\n",
        "        if mask is not None: mask = mask.unsqueeze(1)\n",
        "        # # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # if mask != None: attn = attn.masked_fill(mask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "\n",
        "        # print('attn fwd',q.shape, k.shape, v.shape)\n",
        "        b,t,d = x.shape\n",
        "        # print(min(self.w,t))\n",
        "        print(self.w,t)\n",
        "        midx = self.midx[:t,-min(self.w,t):] # [t,w,1]\n",
        "        kv = kv[torch.arange(b)[:,None,None,None], torch.arange(h)[None,:,None,None], midx[None,None,...]]\n",
        "        mk, mv = kv.chunk(2, dim=-1) # [b,h,t,w,d]\n",
        "\n",
        "        attn = q.unsqueeze(3) @ mk.transpose(-2,-1) * self.scale # [b,h,t,1,d] @ [b,h,t,d,w] = [b,h,t,1,w]\n",
        "        attention = torch.softmax(attn, dim=-1) # [b,h,t,1,w]\n",
        "        out = (self.drop(attention) @ mv).squeeze(-2) # [b,h,t,1,w]@[b,h,t,w,d]=[b,h,t,1,d]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "dim = 512\n",
        "model = Attention(dim) # 257 ms\n",
        "x = torch.rand(64,100,dim)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "PTMZq6e4TzFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title entropy -> mask\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/data/patcher.py\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def entropy(scores): # [batch, seq_len, vocab] # Shannon entropy; input is raw logits\n",
        "    log_probs = F.log_softmax(scores, dim=-1) # log(softmax(x))\n",
        "    probs = torch.exp(log_probs)\n",
        "    # print('in etp', log_probs, probs, p_log_p)\n",
        "    entropy = -(probs*log_probs).sum(dim=-1)\n",
        "    return entropy # [batch, seq_len]\n",
        "\n",
        "# def patch_start_mask_global_and_monotonicity(entropies, t, t_add=0): # [batch, seq_len]\n",
        "def patchify(entropies, t, t_add=0): # [batch, seq_len]\n",
        "    bs, seq_len = entropies.shape\n",
        "    if seq_len == 0: return entropies > t # threshold\n",
        "    mask = torch.zeros_like(entropies, dtype=torch.bool)\n",
        "    mask[:, 0] = True\n",
        "    differences = entropies[:, 1:] - entropies[:, :-1]\n",
        "    # Calculate conditions for all elements except the first one in each sequence\n",
        "    # condition = (differences > t_add) & (entropies[:, 1:] > t) & (~mask[:, :-1])\n",
        "    # print('patchify',differences)\n",
        "    condition = differences > t_add\n",
        "    mask[:, 1:] = condition\n",
        "    return mask # [batch, seq_len] # T at start of each patch\n",
        "\n",
        "def padded_block_mask(mask): # [b,t]\n",
        "    B, N = mask.shape\n",
        "    block_id = mask.cumsum(dim=1) - 1          # (B, N)\n",
        "    K = mask.sum(dim=1) # [b] # num blocks per sample\n",
        "    N = max(K)\n",
        "    offset = (N - K).unsqueeze(1) # (B,1)\n",
        "    row_idx = block_id + offset # (B, N) # shift blocks to the end\n",
        "    i = torch.arange(N, device=mask.device)[None,:,None] # [1,N,1] # broadcast compare: for each batch b, out[b,i,j] = (i == row_idx[b,j])\n",
        "    out = i == row_idx.unsqueeze(1)                            # (B, N, N)\n",
        "    return out # [batch ,num_patch, seq_len]\n",
        "\n",
        "def block_diag(mask): # [b,t]\n",
        "    \"\"\"Convert a 1D bool mask to a block diagonal matrix, where each `True` starts a new block.\n",
        "    mask = [1,0,1,1] ->\n",
        "    [[1,1,0,0],\n",
        "     [1,1,0,0],\n",
        "     [0,0,1,0],\n",
        "     [0,0,0,1]]\"\"\"\n",
        "    block_ids = mask.cumsum(1) # Assign a unique group index to each block e.g., [0, 0, 1, 2]\n",
        "    row_ids = block_ids.unsqueeze(2)  # [b,t,1]\n",
        "    col_ids = block_ids.unsqueeze(1)  # [b,1,t]\n",
        "    out = row_ids == col_ids # Compare row and col IDs — same block → True\n",
        "    return out # [b,t,t]\n",
        "\n",
        "x = torch.rand(2,3,4) # [b,t,d]\n",
        "x = torch.tensor([-1,1,-1,-1]).float().repeat(2,3,1)*3 # 1.3863\n",
        "\n",
        "print('max:',-torch.tensor(1/4).log())\n",
        "# min=0\n",
        "\n",
        "entropies = entropy(x)\n",
        "print(entropies)\n",
        "# mask = patchify(entropies, 1.) # [b, t]\n",
        "# print(mask)\n",
        "# # mask = padded_block_mask(mask) # [b ,num_patch, t]\n",
        "# # print(mask)\n",
        "# mask = block_diag(mask) # [b ,t, t]\n",
        "# print(mask)\n",
        "# xx = x.unsqueeze(1) *mask.unsqueeze(-1) # [b ,num_patch, t, d]\n",
        "# print(xx.shape)\n",
        "# print(xx.mean(2).shape)\n",
        "# print(xx.mean(2)) # [b ,num_patch, d]\n",
        "\n",
        "# [b,t,t]\n",
        "\n",
        "# x = torch.rand(2,5,) # [b,t,d]\n",
        "# mask=(x>.7)\n",
        "# out = mask.unsqueeze(1) | mask.unsqueeze(2) # [b,t]->[b,t,t]\n",
        "# print(out)\n"
      ],
      "metadata": {
        "id": "aEu8sDWjB92A",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a16fb2c-59d9-445b-d75e-79bf353c0fb4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max: tensor(1.3863)\n",
            "tensor([[0.0517, 0.0517, 0.0517],\n",
            "        [0.0517, 0.0517, 0.0517]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLTlayer transformer fsq noloss save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class BLTlayer(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=None, quant=6, num_layers=1):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_dim\n",
        "        self.pos_enc = RoPE(d_model, base=100) # 10000\n",
        "        # self.latent = nn.Parameter(torch.randn(1,1,d_model)*.02) # init\n",
        "        self.perceiver = Seq(*[AttentionBlock(d_model, n_heads=4, cond_dim=d_model) for _ in range(num_layers)])\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.patch_enc = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.conv_emb = nn.Conv1d(d_model, d_model, 1)\n",
        "        # self.conv_emb = nn.Conv1d(d_model, d_model, 7, 7//2)\n",
        "        self.vq = codequant(levels = d_model*[quant])\n",
        "        self.max_etp = 1 - torch.tensor([1-1/(quant-1)], device=device).asin()/torch.pi*2 # cos max etp\n",
        "        # self.max_etp = self.max_etp**2\n",
        "        # self.max_etp = 1/(quant-1) # unif max etp\n",
        "        # self.max_etp = -torch.tensor(1/quant).log() # max shannon etp\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t,d], [b,t,t]\n",
        "        x = self.pos_enc(x)\n",
        "        b,t,d = x.shape\n",
        "        # if mask==None: mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # F->mask # for F.scaled_dot_product_attention\n",
        "        # print('BLTlayer',x.shape, cond.shape if cond!=None else None, mask.shape if mask!=None else None)\n",
        "        x = self.perceiver(x, cond=cond if cond!=None else x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        out = self.lin(x)\n",
        "        return out # [b,t,d]\n",
        "\n",
        "    def to_patch(self, x, out=None, entropies=None): # [b,t,d]\n",
        "        if entropies==None: _, entropies = self.entropy(out) # [b,t] float\n",
        "        mask = patchify(entropies, 1.) # [b,t] # T at start of each patch, F otherwise\n",
        "        patch_mask = padded_block_mask(mask) # [b ,num_patch, t]\n",
        "        # print('lyr to_patch', x.shape, out.shape, patch_mask.shape)\n",
        "\n",
        "        latent = (x.unsqueeze(1) *patch_mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "        # latent = self.latent.repeat(1,mask.shape[1],1) # [b ,num_patch, d]\n",
        "        # print('lyr to_patch', latent)\n",
        "        latent = latent + self.patch_enc\n",
        "        out = self.conv_emb(x.transpose(-2,-1)).transpose(-2,-1) # instead of hashing\n",
        "        patches = self.forward(latent, cond=out, mask=patch_mask) # [batch, num_patch, latent_dim/d_model]\n",
        "        # patches = self.vqp(patches)\n",
        "        b,t = mask.shape\n",
        "        s = mask.sum(-1)\n",
        "        mask = torch.arange(t, device=device).repeat(b,1)>=(t-s).unsqueeze(-1)\n",
        "        mask = mask[:,-s.max():] # [b,num_patch] # T-> can attend\n",
        "\n",
        "        return patches, mask, patch_mask # [b, num_patch, out_dim], [b,t], [b ,num_patch, t]\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "\n",
        "\n",
        "# generate one tok unless uncertain ? then patch up?\n",
        "# gen one only bec blt need to tok\n",
        "# or blt feed only one tok? nope, should parallelise\n",
        "# feed all toks bec topatch need out\n",
        "    # def generate(self, x=None, cond=None, patch=None): # [1,t,d], [1,len_cond,d], [1,1,d]\n",
        "    def generate(self, x=None, cond=None, patch=None, mask=None, patch_mask=None): # [b,t,d], [b,len_cond,d], [b,len_patch,d], [b,t], [b,t,len_patch]\n",
        "        # print('lyr gen1 x cond patch mask patch_mask', x.shape, cond.shape if cond!=None else None, patch.shape if patch!=None else None, mask.shape if mask!=None else None, patch_mask.shape if patch_mask!=None else None)\n",
        "        b,t,d = x.shape\n",
        "        if cond==None:\n",
        "            cond = x\n",
        "            # mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # no cond => is causal\n",
        "        # else:\n",
        "        causal_mask = torch.tril(torch.ones((b,t,cond.shape[1]), dtype=bool, device=device)) # [1,t,n_cond+n_patch] # got cond, all can attend to cond\n",
        "        if mask!=None: mask = causal_mask & mask.unsqueeze(1) # mask left side, tril is lower left\n",
        "        # if mask!=None: mask = causal_mask & mask # mask left side, tril is lower left\n",
        "        else: mask = causal_mask\n",
        "\n",
        "        if patch!=None:\n",
        "            cond = torch.cat([cond, patch], dim=1)\n",
        "            if patch_mask==None:\n",
        "                patch_mask = torch.zeros((b,t,patch.shape[1]), dtype=bool, device=device)\n",
        "                patch_mask[:,-1] = True # patch is only for last (aka new) tok\n",
        "                # patch_mask = torch.ones((b,t,patch.shape[1]), dtype=bool, device=device) # patch is for all toks\n",
        "\n",
        "            # print('lyr gen1 x cond patch', x.shape, cond.shape, patch.shape, mask.shape, pmask.shape)\n",
        "            mask = torch.cat([mask, patch_mask], dim=-1) # [b, t, t+len_patch]\n",
        "        # print('lyr gen1', x.shape, cond.shape, mask.shape)\n",
        "        out = self.forward(x, cond=cond, mask=mask)\n",
        "        qout, etp = self.entropy(out)\n",
        "        return qout, etp\n",
        "# no need loss. gen one only; if etp high, blt patch_up lvl\n",
        "\n",
        "    def entropy(self, out):\n",
        "        qout = self.vq(out)\n",
        "        etp = (out-qout).abs().mean(-1) # [b,1] float\n",
        "        # etp = (out-qout).pow(2).mean(-1)\n",
        "        # etp = entropy(out)\n",
        "        etp /= self.max_etp # normalise to [0,1]\n",
        "        return qout, etp\n",
        "\n",
        "\n",
        "# bltlyr: btd->btd+bpd , gentrain: shift1, artrain:btd+bpd->btd\n",
        "\n",
        "# in: qunatseq\n",
        "# fwd: qunatx -> y_\n",
        "# to_patch: x, y_ -> patches, mask\n",
        "# quant at patch only? since tok emb quants for x\n",
        "# need to quant out for to_patch\n",
        "# for train, gen should mix in results from up, scaled by etp\n",
        "# fwd, to_patch,\n",
        "\n",
        "\n",
        "batch=2\n",
        "d_model = 16\n",
        "in_dim=128\n",
        "\n",
        "model = BLTlayer(in_dim, d_model).to(device)\n",
        "# # model = BLTlayer(d_model).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# optim = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # 1e-4 #3e-4\n",
        "\n",
        "x = torch.rand((batch, 7, d_model), device=device)\n",
        "# out = model(x)\n",
        "out, entropies = model.generate(x) #\n",
        "# out, _ = model.generate(x, patch=patch) # patch cond from last tok onwards\n",
        "# out, _ = model.generate(x[:,-1:], cond=x[:,:-1], patch=patch) # got cond no need to cond on all\n",
        "# out, _ = model.generate(x[:,-1:], cond=x[:,:-1])\n",
        "print(out.shape, entropies.shape)\n",
        "# print('out', out.shape)\n",
        "\n",
        "patches, mask, patch_mask = model.to_patch(x, entropies=entropies) # [b, num_patch, out_dim], [b,t], [b ,num_patch, t]\n",
        "\n",
        "print(mask)\n",
        "\n",
        "print('patches, mask',patches.shape, mask.shape, patch_mask.shape)\n",
        "\n",
        "\n",
        "patches, _ = model.generate(patches, mask=mask) # upper lvl\n",
        "# print(out.shape, entropies.shape)\n",
        "# patches, mask, patch_mask = model.to_patch(x, entropies=entropies)\n",
        "# print('patches, mask',patches.shape, mask.shape)\n",
        "print(x[:,-1:].shape, x[:,:-1].shape, patches[:,-1:].shape)\n",
        "# out_, _ = model.generate(x[:,-1:], cond=x[:,:-1], patch=patches[:,-1:]) # got cond no need to cond on all\n",
        "out_, _ = model.generate(x, patch=patches, patch_mask=patch_mask.transpose(-2,-1))\n",
        "# out_, _ = model.generate(x, patch=patches) # got cond no need to cond on all\n",
        "# g = -entropies[:,-1:] # certainty\n",
        "# print(g)\n",
        "# out = g*out[:,-1:] + (1-g)*out_[:,-1:]\n",
        "\n",
        "# patch_mask\n",
        "\n",
        "# out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "# print(patches.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9H4coT-kcM-",
        "outputId": "a706c52a-f9b4-4d03-9ddb-7b9af82677a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 7, 16]) torch.Size([2, 7])\n",
            "tensor([[False,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True]], device='cuda:0')\n",
            "patches, mask torch.Size([2, 5, 16]) torch.Size([2, 5]) torch.Size([2, 5, 7])\n",
            "torch.Size([2, 1, 16]) torch.Size([2, 6, 16]) torch.Size([2, 1, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLT me fsq noloss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class BLT(nn.Module):\n",
        "    def __init__(self, d_model, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([BLTlayer(d_model, d_model, quant=6*2**i) for i in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,t,d], [b,t]\n",
        "        blocks = []\n",
        "        pmasks = []\n",
        "        masks = []\n",
        "        outs = []\n",
        "        # etps = []\n",
        "        for i, level in enumerate(self.layer[:-1]):\n",
        "            blocks.append(x)\n",
        "            masks.append(mask)\n",
        "            out, entropies = level.generate(x, mask=mask)\n",
        "            # out, entropies, kvcache = level.generate(x, mask=mask)\n",
        "            outs.append(out)\n",
        "            # etps.append(entropies)\n",
        "            x, mask, patch_mask = level.to_patch(x, entropies=entropies)\n",
        "            # x, mask, patch_mask = level.to_patch(x, entropies=entropies, kvcache)\n",
        "            # print('blt_fwd_down', x.shape, mask.shape, patch_mask.shape)\n",
        "            pmasks.append(patch_mask)\n",
        "        out, _ = self.layer[-1].generate(x, mask=mask)\n",
        "        out, _ = self.layer[-1].generate(x, mask=mask)\n",
        "        x = torch.cat([x[:,:1], out], dim=1)\n",
        "        for i, level in enumerate(reversed(self.layer[:-1])):\n",
        "\n",
        "\n",
        "            # out[:,-1:] * entropies[:,-1:]\n",
        "            v = blocks[-i-1] #+ outs\n",
        "            # print('blt_fwd_up', v.shape, x.shape, masks[-i-1].shape, pmasks[-i-1].shape)\n",
        "            # print('blt_fwd_up', pmasks[-i-1])\n",
        "\n",
        "            patch_mask = pmasks[-i-1]\n",
        "            b,p,t = patch_mask.shape\n",
        "            add_mask = torch.zeros((b,1,t), dtype=bool, device=device)\n",
        "            add_mask[...,-1] = True # patch is only for last (aka new) tok\n",
        "            patch_mask = torch.cat([patch_mask, add_mask], dim=1)\n",
        "            # print('blt_fwd_up', v.shape, x.shape, masks[-i-1].shape if masks[-i-1]!=None else None, patch_mask.shape)\n",
        "\n",
        "            out, _ = level.generate(v, patch=x, mask=masks[-i-1], patch_mask=patch_mask.transpose(-2,-1))\n",
        "            # out, _ = level.generate(v, patch=x, mask=masks[-i-1], patch_mask=patch_mask.transpose(-2,-1), kvcache)\n",
        "            # x = torch.cat([v,x[:,-1:]], dim=1)\n",
        "            # # x = torch.cat([x, blocks[-i-1]*2**-.5], dim=1)\n",
        "            # x = torch.cat([x, blocks[-i-1]*self.skip_scale**(len(self.up_list)-i)], dim=1) # https://arxiv.org/pdf/2310.13545\n",
        "            # g = -entropies[:,-1:] # certainty\n",
        "            # out = g*out[:,-1:] + (1-g)*out_[:,-1:]\n",
        "            # etp = -etps[-i-1].unsqueeze(-1) # certainty\n",
        "            # # print(etp.shape, out.shape, outs[-i-1].shape)\n",
        "            # x = etp*out + (1-etp)*outs[-i-1]\n",
        "            # x = out + outs[-i-1]\n",
        "            x = .5*(out + outs[-i-1])\n",
        "            # x = out*2**-.5 + outs[-i-1]\n",
        "            x = torch.cat([v[:,:1], x], dim=1)\n",
        "        return x[:,1:]\n",
        "\n",
        "\n",
        "# pos_all, entropy mask;\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "# gen train: mse(x[1:],out[:-1])\n",
        "# gen inference: x cond=x, cond=(x+patch(+*learnt?)), if entropy/bos, pop patch and regen\n",
        "\n",
        "# # fsq: dont realize tokens in layer\n",
        "# layer:\n",
        "# posemb\n",
        "# x less quants patch more quants\n",
        "# quant after linout\n",
        "# # quant before linout\n",
        "\n",
        "\n",
        "d_model=16\n",
        "model = BLT(d_model, num_layers=3).to(device)\n",
        "x = torch.rand(2,13,d_model, device=device)\n",
        "out = model(x)\n",
        "# out = model.generate(x)\n",
        "print(out.shape)\n",
        "# print(out)\n",
        " # [b,t,d], [b,len_cond,d], [b,len_patch,d], [b,t], [b,t,len_patch]"
      ],
      "metadata": {
        "id": "E4BopMdVCKFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd0a825-4952-432e-c4d8-69080f00f4ee",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 13, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ghj"
      ],
      "metadata": {
        "id": "C-bfNToteDix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title HierarchialGPT\n",
        "\n",
        "class HierarchialGPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        # self.tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        self.tok_emb = TTEmbedding([29, 1733], [64,1], rank=min(d_model,256))\n",
        "        # self.pos_enc = RoPE(d_model, base=10000)\n",
        "        self.blt = BLT(d_model, num_layers=num_layers)\n",
        "        # self.out = nn.Linear(d_model, out_dim or in_dim)\n",
        "        # self.out = lambda x: x @ self.tok_emb.weight.T  # weight tying\n",
        "        self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "\n",
        "    #     self.apply(self.init_weights)\n",
        "    # import math\n",
        "    # def init_weights(self, m): # https://openreview.net/pdf?id=lkRjnNW0gb\n",
        "    #     if isinstance(m, nn.Linear):\n",
        "    #         # W ~ N(0, ( 1/ (sqrt(n_in) + sqrt(n_out)) )^2 )\n",
        "    #         # want std = 1/ (sqrt(n_in) + sqrt(n_out))\n",
        "    #         # n_in, n_out = module.weight.shape[0], module.weight.shape[1]\n",
        "    #         n_in, n_out = m.weight.shape\n",
        "    #         torch.nn.init.normal_(m.weight, std=1/(math.sqrt(n_in)+math.sqrt(n_out)))\n",
        "    #         if m.bias is not None:\n",
        "    #             nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tok_emb(x)\n",
        "        # x = self.pos_enc(x)\n",
        "        x = self.blt(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "vocab_size=50\n",
        "vocab_size=train_loader.dataset.vocab_size#50\n",
        "d_model, num_layers = 64, 3\n",
        "model = HierarchialGPT(vocab_size, d_model, num_layers=num_layers).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "x = torch.randint(0,vocab_size, (64,128), device=device)\n",
        "\n",
        "# x, y = x[:,:-1], x[:,1:]\n",
        "# x, y = x.to(device), y.to(device)\n",
        "\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# # loss = F.cross_entropy(out[:,:-1].flatten(0,-2), x[:,1:].flatten())\n",
        "# loss = F.cross_entropy(out.flatten(0,1), y.flatten())\n",
        "# print(loss)\n",
        "# optim.zero_grad()\n",
        "# loss.backward()\n",
        "# optim.step()\n",
        "\n",
        "# test 5.684650421142578\n",
        "# Test Loss: 0.0\n",
        "# this is what cealieqplal to ce ctmedee ss idedo ss tou\n",
        "#  sst N tno-ulosetor t\n",
        "# 18 time: 43.02241659164429\n"
      ],
      "metadata": {
        "id": "9s8hLhpSjJh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de4d226-f32b-4492-9bb6-039fe1bfff19",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "404800\n",
            "torch.Size([64, 128, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GPT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=64, out_dim=None, n_heads=8, n_layers=1, ff_dim=256, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.pos_enc = PositionalEncoder(d_model, dropout=dropout)\n",
        "        # self.pos_enc = LearntPosEnc(d_model, dropout=dropout)\n",
        "        self.pos_enc = RoPE(d_model, base=10000)\n",
        "        # self.tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        self.tok_emb = TTEmbedding([29, 1733], [64,1], rank=min(d_model,256))\n",
        "        # self.encoder = Seq(*[EncoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, n_heads=4, cond_dim=d_model) for _ in range(n_layers)])\n",
        "        # self.out = nn.Linear(d_model, out_dim)\n",
        "        self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,t], [b,t,t]\n",
        "        x = self.pos_enc(self.tok_emb(x))\n",
        "        # print('gpt fwd', x.shape)\n",
        "        # x = self.encoder(x, mask=mask)\n",
        "        x = self.encoder(x, x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "input_size = num_classes = 50\n",
        "# input_size = num_classes = train_data.vocab_size#50\n",
        "# print(train_data.vocab_size)\n",
        "# model = GPT(input_size, d_model=512, out_dim=num_classes, n_layers=6).to(device)\n",
        "model = GPT(input_size, d_model=64, out_dim=num_classes, n_layers=3).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "\n",
        "# batch 64, seqlen 100\n",
        "# dim 64, nlyrs 1: 5.0gb\n",
        "# 512, 1, 6.3, 95sec\n",
        "# dim 512, 6 lyrs, 8.7 gb, 125 sec, 59393105 params\n",
        "# dim 768, 12 lyrs, 12.4 gb, 257 sec, 110318161 params\n",
        "\n",
        "\n",
        "x = torch.randint(0, input_size, (2, 5), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# strain 2.1297216415405273\n",
        "# Test Loss: 5.68384530884879\n",
        "# this is what gl han ast atot t mad indswonshol e nlo refes the the laley emas\n",
        "# 0 time: 10.823431253433228\n",
        "\n",
        "# strain 1.3502650260925293\n",
        "# Test Loss: 8.358739376068115\n",
        "# this is what the signed and commerly the meeticly base finance forformance\n",
        "\n",
        "# 29 time: 10.74680209159851\n",
        "\n",
        "# no norm\n",
        "# strain 2.1280951499938965\n",
        "# Test Loss: 6.583536461421422\n",
        "# this is what the coure <unk> cloh adintt myen llig hroute wanseg hipes sustin\n",
        "# 0 time: 10.436080694198608\n",
        "\n",
        "# strain 1.3436470031738281\n",
        "# Test Loss: 10.137133121490479\n",
        "# this is what americans incentic company 's <unk>\n",
        "#  bear drop in this each hol\n",
        "# 29 time: 11.038460969924927\n",
        "\n",
        "\n",
        "# F.scaled_dot_product_attention\n",
        "# strain 2.116980791091919\n",
        "# Test Loss: 6.470040246418544\n",
        "# this is what inds fougsterined a qouep omal <unk> e lon waby ine <unk> beack>\n",
        "# 0 time: 9.980751752853394\n",
        "# strain 1.373486042022705\n",
        "# Test Loss: 8.955048438480922\n",
        "# this is what legaision can said realizive univerence antinge chairman <unk> t\n",
        "# 29 time: 10.000610589981079 9.92842394510905\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF74TdciO-OD",
        "outputId": "99d00c52-6329-456a-962c-1cee7ed1b4d8",
        "cellView": "form"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "379648\n",
            "torch.Size([2, 5, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/xbeat/Machine-Learning/blob/main/Python%20KV%20Caching%20Efficient%20Data%20Storage%20and%20Retrieval.md"
      ],
      "metadata": {
        "id": "PAPC_l9EbEiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bottom lyrs higher dim\n",
        "# bottom emb mlp satisfiability top layers\n",
        "\n",
        "# current status, job, personality, points of view\n",
        "# dreams, aspirations\n",
        "# expectations, family\n",
        "\n",
        "# scicentre, jap garden\n",
        "# aquarium beach catcafe dinner equestrian  museum/movie nature picnic/pool shopping zoo"
      ],
      "metadata": {
        "id": "AzRz9JTLCi8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optim.param_groups[0]['lr'] = 1e-4#"
      ],
      "metadata": {
        "id": "D6xDI8JOKu4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "f7b48049-ab5e-456a-84ba-b92bf0bf5f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250606_101730-qzm1a9vm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/gpt/runs/qzm1a9vm' target=\"_blank\">usual-sunset-18</a></strong> to <a href='https://wandb.ai/bobdole/gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/gpt' target=\"_blank\">https://wandb.ai/bobdole/gpt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/gpt/runs/qzm1a9vm' target=\"_blank\">https://wandb.ai/bobdole/gpt/runs/qzm1a9vm</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"gpt\", config={\"model\": \"res18\",}) # violet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title perplexity\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# https://www.comet.com/site/blog/perplexity-for-llm-evaluation/\n",
        "\n",
        "def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    perplexity = nll.mean().exp()\n",
        "    return perplexity\n",
        "\n",
        "# logits = torch.randn(2, 4, 10)\n",
        "# target = torch.tensor([[1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "\n",
        "# perplexity = Perplexity(logits, target)\n",
        "# # perplexity = Perplexity(logits[:,:-1], y[:,1:])\n",
        "# print(f'Perplexity: {perplexity}')\n",
        "\n",
        "\n",
        "# @title train test generate\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optimizer, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x, y = x[:,:-1], x[:,1:]\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            # causal_mask = torch.ones(x.size(1), x.size(1), dtype=bool, device=device).tril(diagonal=0).repeat(x.shape[0],1,1) # for F.scaled_dot_product_attention\n",
        "            # # causal_mask = ~torch.ones(x.size(1), x.size(1), dtype=bool, device=device).tril(diagonal=0).repeat(x.shape[0],1,1)\n",
        "            # logits = model(x, mask=causal_mask) #output = [batch size, trg len - 1, output dim]\n",
        "            logits = model(x) #output = [batch size, trg len - 1, output dim]\n",
        "            # logits, _ = model(x) # rnn\n",
        "            loss = F.cross_entropy(logits.flatten(0,1), y.flatten()) # [b*t,d], [b*t]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0: print(\"strain\",loss.item())\n",
        "            # perplexity = Perplexity(logits.detach(), y).item()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "\n",
        "# def test(loader, model):\n",
        "#     model.eval()\n",
        "#     total_loss = 0\n",
        "#     for i, (x, y) in enumerate(loader):\n",
        "#         x, y = x.to(device), y.to(device)\n",
        "#         with torch.no_grad():\n",
        "#             logits = model(x)\n",
        "#             # logits, _ = model(x) # rnn\n",
        "#             perplexity = Perplexity(logits[:,:-1], y[:,1:])\n",
        "#         loss = F.cross_entropy(logits.flatten(0,-2), y.flatten()) # [batch*seq_len, vocab_size], [batch*seq_len]\n",
        "#         total_loss+=loss.item()\n",
        "#         # if i % 100 == 0: print(\"test\",loss.item())\n",
        "#         if i % 100 == 0: print(\"test\",loss.item(),\"ppty\",perplexity.item())\n",
        "#         try: wandb.log({\"test loss\": loss.item()/len(y)})\n",
        "#         except NameError: pass\n",
        "#     return total_loss / len(loader)\n",
        "\n",
        "def generate(model, context, max_steps=64, temperature=1):\n",
        "    x = encode(context)#.to(device)\n",
        "    model.eval()\n",
        "    for n in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            output = model(x)\n",
        "            # output, hidden = model(x, hidden) # rnn\n",
        "        # print('generate', output.shape, hidden.shape)\n",
        "        # hidden = hidden[:, -1, :].unsqueeze(1) # RNN/GRU\n",
        "        output = output[:, -1] # get logit for last character\n",
        "        output = output/temperature\n",
        "        output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    completion = decode(x.squeeze(0))\n",
        "    return completion\n",
        "\n",
        "# import time\n",
        "# start = begin = time.time()\n",
        "for i in range(1):\n",
        "# for i in range(30):\n",
        "    # train_loss = strain(model, train_loader, optim, scheduler=None)\n",
        "    strain(model, train_loader, optim, scheduler=None)\n",
        "    # test_loss = test(test_loader, model)\n",
        "    # print('Test Loss:', test_loss)\n",
        "    # print(generate(model, \"this is what\"))\n",
        "    # print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    # start = time.time()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wDp4m3SRkHg",
        "outputId": "701b9acd-9d8d-4aad-80e0-0b844375a90b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "strain 13.153278350830078\n",
            "strain 8.625616073608398\n",
            "strain 7.532129287719727\n",
            "strain 7.635157585144043\n",
            "strain 7.318344593048096\n",
            "strain 7.13112211227417\n",
            "strain 7.953535556793213\n",
            "strain 7.314005374908447\n",
            "strain 7.358240127563477\n",
            "strain 7.116394519805908\n",
            "strain 7.228402137756348\n",
            "strain 7.046687602996826\n",
            "strain 6.697092533111572\n",
            "strain 7.242772102355957\n",
            "strain 7.065131187438965\n",
            "strain 7.033816337585449\n",
            "strain 7.072269916534424\n",
            "strain 6.835287570953369\n",
            "strain 6.56411075592041\n",
            "strain 6.651105880737305\n",
            "strain 6.974645614624023\n",
            "strain 6.841508388519287\n",
            "strain 7.360500812530518\n",
            "strain 8.260704040527344\n",
            "strain 7.255819797515869\n",
            "strain 7.79181432723999\n",
            "strain 6.3285603523254395\n",
            "strain 6.8741278648376465\n",
            "strain 6.494027614593506\n",
            "strain 7.289155960083008\n",
            "strain 6.912679672241211\n",
            "strain 7.111609935760498\n",
            "strain 6.812265396118164\n",
            "strain 6.852604389190674\n",
            "strain 6.8135504722595215\n",
            "strain 6.717615604400635\n",
            "strain 6.746161937713623\n",
            "strain 6.72939920425415\n",
            "strain 7.120828628540039\n",
            "strain 6.625032901763916\n",
            "strain 6.595771312713623\n",
            "strain 7.110177040100098\n",
            "strain 6.799492835998535\n",
            "strain 7.031942367553711\n",
            "strain 6.8386430740356445\n",
            "strain 6.709420204162598\n",
            "strain 6.759841442108154\n",
            "strain 6.5659661293029785\n",
            "strain 6.7400593757629395\n",
            "strain 6.201128005981445\n",
            "strain 6.713956356048584\n",
            "strain 6.87520694732666\n",
            "strain 6.900241374969482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model, \"this is what\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFHuyJqynBph",
        "outputId": "09c51c1c-a7d2-400c-a9a1-609b2e1414b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is what rapidly on the crime article and CLIImage: Photograph.\n",
            "\n",
            "[governmental as well as well-AlnotWsinkser fell through the DA namely Smith else, of the lowpower drawn from alarm. In the 52]\n",
            "Still, Carops.\n",
            "Recability's infrastructure.\n",
            "\n",
            "\n",
            "9][111:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# # modelsd, optimsd = torch.load(folder+'gpt.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('gpt.pkl', map_location=device).values()\n",
        "# model.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# modelsd = torch.load(folder+'gpt.pkl', map_location=device)['model']#.values()\n",
        "# # print(modelsd)\n",
        "# model.load_state_dict(modelsd, strict=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KWClFaSTjix",
        "outputId": "bd2b2c39-5950-4ff8-acdb-0c1a63483f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': model.state_dict(), 'optimizer': optim.state_dict()}\n",
        "# checkpoint = {'model': model.state_dict()}\n",
        "torch.save(checkpoint, folder+'blt.pkl')\n",
        "# # torch.save(checkpoint, 'gpt.pkl')"
      ],
      "metadata": {
        "id": "7i4NHzB9nFfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "4n3vdnZvBrBT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7ugFOONfWAmx"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import requests\n",
        "# url=\"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\" # https://www.tensorflow.org/text/tutorials/text_generation\n",
        "url=\"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt\" # train test valid # https://pytorch.org/text/stable/datasets.html#penntreebank\n",
        "out=requests.get(url)\n",
        "with open(\"data.txt\", \"wb\") as f:\n",
        "    f.write(out.content)\n",
        "text = open(\"data.txt\", 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt\" # train test valid # https://pytorch.org/text/stable/datasets.html#penntreebank\n",
        "out=requests.get(url)\n",
        "with open(\"data_.txt\", \"wb\") as f: f.write(out.content)\n",
        "test_text = open(\"data_.txt\", 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# print(len(text))\n",
        "# print(text[000:1000])\n",
        "# data = ''.join(text)\n",
        "# chars = sorted(list(set(data)))\n",
        "# print(chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV5HmCFv_ITo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title char dataloader\n",
        "# https://github.com/Sam-Armstrong/tinyGPT/blob/main/Training.py\n",
        "# https://colab.research.google.com/github/karpathy/minGPT/blob/master/play_char.ipynb\n",
        "# https://github.com/karpathy/nanoGPT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, raw_data, seq_len):\n",
        "        data = ''.join(raw_data)\n",
        "        chars = sorted(list(set(data)))\n",
        "        self.vocab_size = len(chars) # 283\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.data = self.data_process(data) # list of int\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def data_process(self, data): # str 10780437\n",
        "        return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data) - self.seq_len\n",
        "        return len(self.data)//(self.seq_len+1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # dix = self.data[idx:idx + self.seq_len + 1]\n",
        "        dix = self.data[idx*(self.seq_len+1) : (idx+1)*(self.seq_len+1)]\n",
        "        x, y = dix[:-1], dix[1:]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "seq_len = 100 # 128\n",
        "train_data = CharDataset(text, seq_len) # one line of poem is roughly 50 characters\n",
        "test_data = CharDataset(test_text, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "test_loader = DataLoader(test_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "\n",
        "def encode(context): return torch.tensor([train_data.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "def decode(x): return ''.join([train_data.itos[int(i)] for i in x])\n",
        "# for x,y in train_loader:\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RNN pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = in_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.rnn = nn.GRU(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "        self.emb = nn.Embedding(in_dim, d_model)\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        x = self.emb(x)\n",
        "        if h is None: h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "        else: h0 = h\n",
        "        # print(x.shape, h0.shape)\n",
        "        out, h = self.rnn(x, h0)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out, h # [b,t,out_dim], [num_layers,b,d_model]\n",
        "\n",
        "\n",
        "hidden_size = 64\n",
        "num_layers = 3\n",
        "input_size = num_classes = 50#train_dataset.vocab_size#65\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_classes, num_layers).to(device)\n",
        "# print(model)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "x = torch.randint(0, 10, (32,25), device=device) # [b,t]\n",
        "out, h = model(x)\n",
        "print(out.shape, h.shape)\n",
        "\n",
        "# this is what said compaticated 's steel a debt rate up taps imposed house in\n",
        "# 27 time: 9.265877962112427 9.091531344822474\n",
        "# Test Loss: 8.320224022519762\n",
        "# this is what indeed sancal quite pralic <unk> over third market that of he an\n",
        "# 28 time: 8.423768997192383 9.068516492843628\n",
        "# Test Loss: 8.308711514956709\n",
        "# this is what banking the mr. $ N ta curres problems\n",
        "#  delose tax come a judge\n",
        "\n",
        "# this is what cents a share\n",
        "#  bargain when a lot out the company 's times <unk\n",
        "# 27 time: 12.4160475730896 12.368791265147072\n",
        "# Test Loss: 8.183734503345214\n",
        "# this is what holders unstock in tokyo-propernatives have sales\n",
        "#  ast worked s\n",
        "# 28 time: 12.395569086074829 12.36972609881697\n",
        "# Test Loss: 8.16899547715118\n",
        "# this is what purchamed of job\n",
        "#  the wrong begy consent of despite of the sept\n",
        "# 29 time: 12.502739667892456 12.374171813329061\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8_EHwVO6eAC",
        "outputId": "3c68a8d6-b35d-470d-ca10-f9407d261dfc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81330\n",
            "torch.Size([32, 25, 50]) torch.Size([3, 32, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        # self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "    @property\n",
        "    def codebook(self): return self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z):\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        # bound = (F.sigmoid(2*z)-1/2) * (self.levels-beta) + offset\n",
        "        bound = (F.tanh(z)/2) * (self.levels-1) + offset\n",
        "        quantized = ste_round(bound)\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1).round().int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "    def codes_to_threshold(self, zhat):\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        # quantized = zhat * self.half_width + offset\n",
        "        bound = zhat * self.half_width# + offset\n",
        "        # bound = ste_round(quantized)\n",
        "        # print(bound)\n",
        "        # z = ((bound-offset) / (self.levels-0) * 2).atanh()\n",
        "        z = (bound / (self.levels-0) * 2).atanh()\n",
        "\n",
        "    def threshold(self, z):\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.tanh(z)/2) * (self.levels-1) + offset\n",
        "        quantized = ste_round(bound)\n",
        "        return ((quantized-offset) / (self.levels-0) * 2).atanh()\n",
        "\n",
        "def soft_clamp_relu(x, lower=None, upper=None):\n",
        "    if lower != None: x = lower+F.relu(x-lower)\n",
        "    if upper != None: x = upper-F.relu(upper-x)\n",
        "    return x\n",
        "\n",
        "    def quant_code(self, z): # round code to nearest code\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = zhat * self.half_width + offset\n",
        "        quantized = ste_round(bound)\n",
        "        zhat = (quantized-offset) / self.half_width # split [-1,1]\n",
        "        return soft_clamp_relu(zhat, -1, 1)\n",
        "\n",
        "\n",
        "fsq = FSQ(levels = [128,4,3,2])\n",
        "# fsq = FSQ(levels = 4*[6])\n",
        "# fsq = FSQ(levels = 16*[6])\n",
        "# print(fsq.codebook)\n",
        "batch_size, seq_len = 2, 4\n",
        "# x = torch.linspace(-5,5,17).repeat(4,1).T # sig need larger variance to reach +-1\n",
        "x = torch.linspace(-3,3,23).repeat(4,1).T\n",
        "# x = torch.linspace(-3,3,17).repeat(16,1).T\n",
        "# x=la\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "# z = fsq.threshold(x)\n",
        "# print(z)\n",
        "# la = fsq.indexes_to_codes(lact)\n",
        "# print(la)\n",
        "# # print(fsq.codebook)\n",
        "# # print(fsq.codebook())\n"
      ],
      "metadata": {
        "id": "EsQ6_7SK-jRN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test codequant\n",
        "# zhat = torch.linspace(-3,3,23).repeat(4,1).T\n",
        "# zhat = torch.linspace(-1,1,970)#.repeat(4,1).T\n",
        "zhat = torch.linspace(-1.2,1.2,970)#.repeat(4,1).T\n",
        "\n",
        "# print((zhat - zhat.detach())==0)\n",
        "# print(soft_clamp_relu(zhat,-1,1)==1)\n",
        "\n",
        "# def soft_clamp_relu(x, lower=None, upper=None):\n",
        "#     if lower != None: x = lower+F.relu(x-lower)\n",
        "#     if upper != None: x = upper-F.relu(upper-x)\n",
        "#     return x\n",
        "\n",
        "# offset = (fsq.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "# bound = zhat * fsq.half_width + offset\n",
        "# quantized = ste_round(bound)\n",
        "# zhat = (quantized-offset) / fsq.half_width # split [-1,1]\n",
        "# zhat = soft_clamp_relu(zhat, -1, 1)\n",
        "# print(zhat)\n",
        "\n",
        "# zhat = (zhat *torch.pi/2).cos()\n",
        "# zhat = zhat *(zhat *torch.pi/2).sin().abs()\n",
        "# print(zhat)\n",
        "\n",
        "# # y=zhat[:,0]\n",
        "# # y=zhat\n",
        "# # y=zhat.acos()/torch.pi*2-1\n",
        "# y=zhat.asin()/torch.pi*2\n",
        "# # print(y)\n",
        "# # y = ((y+1)*torch.pi/2).cos()\n",
        "# y = (y*-torch.pi/2).sin()\n",
        "# # print(y)\n",
        "# x=torch.ones_like(y)\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.hist(y, bins=50, alpha=.5, label='context mask')\n",
        "# # plt.scatter(y.numpy(), x.numpy())\n",
        "# # plt.plot(y)\n",
        "# # plt.plot(new_x, resized_data, label='linear')\n",
        "# plt.legend(loc='best')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "YsaLWLZJNpXM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title left_pad_tensor\n",
        "import torch\n",
        "\n",
        "# def right_to_left_pad(x, mask):\n",
        "#     b,t,d = x.shape\n",
        "#     # Get lengths of valid tokens\n",
        "#     lengths = mask.sum(dim=1)  # (B,)\n",
        "\n",
        "#     # Build index map (B, S)\n",
        "#     idx = torch.arange(t, device=x.device).unsqueeze(0).repeat(b, 1)  # (B, S)\n",
        "#     shift = (t - lengths).unsqueeze(1)                                # (B, 1)\n",
        "\n",
        "#     # Masked index positions: left-shift valid tokens to the right\n",
        "#     new_idx = torch.full_like(idx, t - 1)  # Fill with final index (for padding)\n",
        "#     valid_counts = mask.cumsum(dim=1) - 1  # Position within valid region\n",
        "#     new_pos = shift + valid_counts\n",
        "#     new_idx[mask] = new_pos[mask]          # Only fill valid positions\n",
        "\n",
        "#     # Create scattered result\n",
        "#     out = torch.zeros_like(x)\n",
        "#     out.scatter_(dim=1, index=new_idx.unsqueeze(-1).repeat(1,1,d), src=x)\n",
        "#     return out\n",
        "\n",
        "# def combine_left_padded(x1, mask1, x2, mask2):\n",
        "#     # Convert both to right-padded\n",
        "#     # x1_r = left_to_right_pad(x1, mask1)\n",
        "#     # x2_r = left_to_right_pad(x2, mask2)\n",
        "#     # Combine\n",
        "#     # x_cat = torch.cat([x1_r, x2_r], dim=1)\n",
        "#     x_cat = torch.cat([x1, x2], dim=1)\n",
        "#     mask_cat = torch.cat([mask1.sum(1).unsqueeze(1).repeat(1, x1.size(1)),\n",
        "#                           mask2.sum(1).unsqueeze(1).repeat(1, x2.size(1))], dim=1) > 0\n",
        "#     # Count new lengths\n",
        "#     new_lens = mask_cat.sum(1)\n",
        "#     # Convert back to left-padded\n",
        "#     x_left = right_to_left_pad(x_cat, mask_cat)\n",
        "#     # Build new mask\n",
        "#     B, S_out, D = x_left.shape\n",
        "#     idxs = torch.arange(S_out, device=x_left.device).unsqueeze(0)\n",
        "#     new_mask = idxs >= (S_out - new_lens).unsqueeze(1)\n",
        "#     return x_left, new_mask\n",
        "\n",
        "import torch\n",
        "def left_pad_tensor(x, mask):\n",
        "    sorted_mask, sorted_indices = mask.float().sort(dim=1, descending=True)  # (B, S)\n",
        "    sorted_indices = sorted_indices.unsqueeze(-1).repeat(1,1,x.shape[-1])\n",
        "    crop = sorted_mask.any(0).sum()\n",
        "    x_padded = torch.gather(x, dim=1, index=sorted_indices)[:,:crop]\n",
        "    sorted_mask = sorted_mask.bool()[:,:crop]\n",
        "    return x_padded, sorted_mask\n",
        "\n",
        "\n",
        "# b,t,d = 2,3,4\n",
        "# x=torch.randn(b,t,d)\n",
        "# y=torch.randn(b,t,d)\n",
        "# mask=torch.tensor([[1,1,0],[1,0,0]]).bool()\n",
        "# print(x)\n",
        "# print(y)\n",
        "# # b,t,_ = x.shape\n",
        "# # lengths = mask.sum(dim=1) # (B,)\n",
        "# # print(lengths)\n",
        "# # x_=torch.zeros_like(x)\n",
        "# # x_[torch.arange(b).unsqueeze(-1), :lengths] = x[torch.arange(b).unsqueeze(-1), t-lengths:]\n",
        "# # print(x_)\n",
        "# # out = right_to_left_pad(x, mask)\n",
        "# # out = right_to_left_pad(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "# out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "# print(out)\n",
        "# print(mask_)\n",
        "\n",
        "# x_,mask_ = combine_left_padded(x, mask, y, mask)\n",
        "# print(x_, mask_)\n"
      ],
      "metadata": {
        "id": "yjpbjiJaCUhM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Attention with kvcache\n",
        "class Attention(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, n_heads=None, d_head=8, dropout=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=64, drop=0):\n",
        "        super().__init__()\n",
        "        d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        # self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or query_dim, 2*d_model, bias=False)\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        self.cache = {'x':None, 'xq':None, 'xk':None, 'xv':None}\n",
        "        # self.cache = {'x':torch.empty(0), 'xq':None, 'xk':None, 'xv':None}\n",
        "# gen: xqkv, patch: xkv ~pq, gen: xqkv pvv\n",
        "# so save xqkv\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        # m = torch.rand_like(x)<.00001\n",
        "        # if self.cache['x']!=None and x.shape==self.cache['x'].shape and (x[m]==self.cache['x'][m]).all():\n",
        "        if self.cache['x']!=None and x.shape==self.cache['x'].shape and (x[0,0,0]==self.cache['x'][0,0,0]).all():\n",
        "            # print('cache')\n",
        "            q = self.cache['xq']\n",
        "            if cond==None: k, v = self.cache['xk'], self.cache['xv']\n",
        "            else: k, v = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "        else:\n",
        "            print('no')\n",
        "            q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "            k, v = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "            self.cache = {'x':x, 'xq':q, 'xk':k, 'xv':v}\n",
        "        # q @ k.T # [qlen,d]@[d,klen]=[qlen,klen]\n",
        "        # q-> num_latent # k->seq_len\n",
        "        if mask is not None: mask = mask.unsqueeze(1)\n",
        "        # print('attn fwd',q.shape, k.shape, v.shape)\n",
        "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0) # F->mask # [batch, len_q, len_v]\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "dim = 512\n",
        "model = Attention(dim) # 257 ms\n",
        "x = torch.rand(64,100,dim)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "1j-CclLpmPfQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title sliding window mask me idx kv\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "b,h,t,d = 5,4,10,16\n",
        "# b,h,t,d = 16,8,1000,8\n",
        "# # b,h,t,d = 1,1,10,4\n",
        "# w=3\n",
        "b,h,t,d = 16,8,4096,8\n",
        "w=64\n",
        "\n",
        "q = torch.rand(b,h,t,d, device=device)\n",
        "kv = torch.rand(b,h,t,2*d, device=device)\n",
        "\n",
        "# # midx = torch.arange(w).repeat(t,1) + torch.arange(1-w, t-w+1).unsqueeze(-1)\n",
        "midx = torch.arange(w, dtype=torch.int32).repeat(t,1) + torch.arange(1-w, t-w+1, dtype=torch.int32).unsqueeze(-1)\n",
        "\n",
        "# mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)) * ~torch.tril(torch.ones((t,t), dtype=bool, device=device), diagonal=-w)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "kv = kv[torch.arange(b)[:,None,None,None], torch.arange(h)[None,:,None,None], midx[None,None,...]]\n",
        "mk, mv = kv.chunk(2, dim=-1) # [b,h,t,w,d]\n",
        "\n",
        "# mkv = mask[None,None,...,None]*kv.unsqueeze(2) # [1,1,t,(t),1] @ [b,h,1,t,d] [b,h,t,(t),d]\n",
        "# mkv = torch.cat([torch.zeros((b,h,w-1,2*d), device=device), mkv.flatten(2,3), torch.zeros((b,h,t-w+1,2*d), device=device)], dim=2) # [b,h,(t*(t+1)),d]\n",
        "# mk, mv = mkv.unflatten(2,(t,t+1))[:,:,:,:w].chunk(2, dim=-1) # [b,h,t,w,d]\n",
        "print(time.time()-start)\n",
        "\n",
        "\n",
        "attn = q.unsqueeze(3) @ mk.transpose(-2,-1) # [b,h,t,1,d] @ [b,h,t,d,w] = [b,h,t,1,w]\n",
        "attention = torch.softmax(attn, dim=-1) # [b,h,t,1,w]\n",
        "out = (attention @ mv).squeeze(-2) # [b,h,t,1,w]@[b,h,t,w,d]=[b,h,t,1,d]\n",
        "print(out.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Q6a_UHNxEMpR",
        "outputId": "d72565aa-35e3-4b14-93e6-3139368e7c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5837950706481934\n",
            "torch.Size([16, 8, 4096, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://facebookresearch.github.io/xformers/components/ops.html\n",
        "xformers.ops.memory_efficient_attention(query: Tensor, key: Tensor, value: Tensor, attn_bias: Optional[Union[Tensor, AttentionBias]] = None, p: float = 0.0, scale: Optional[float] = None, *, op: Optional[Tuple[Optional[Type[AttentionFwOpBase]], Optional[Type[AttentionBwOpBase]]]] = None, output_dtype: Optional[dtype] = None) → Tensor\n",
        "# https://github.com/lucidrains/memory-efficient-attention-pytorch/blob/main/memory_efficient_attention_pytorch/memory_efficient_attention.py\n",
        "SELF-ATTENTION DOES NOT NEED O(n^2) MEMORY oct 2022\n",
        "https://arxiv.org/pdf/2112.05682\n"
      ],
      "metadata": {
        "id": "wU3YEcRbj0vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title xformers.ops.memory_efficient_attention\n",
        "!pip install -q xformers # 4min\n",
        "import torch\n",
        "# from xformers.ops import memory_efficient_attention, AttentionOpDispatch, attention_ops\n",
        "from xformers.ops import memory_efficient_attention, attention_ops\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "b=5 # 256\n",
        "d=16 # 1024\n",
        "h=4\n",
        "w=128 # 512\n",
        "t=256 # 7000\n",
        "\n",
        "q = torch.randn(b,t,d, device=device, requires_grad=True)\n",
        "k = torch.randn(b,t,d, device=device, requires_grad=True)\n",
        "v = torch.randn(b,t,d, device=device, requires_grad=True)\n",
        "\n",
        "attn_bias = attention_ops.SlidingWindowBias(w)\n",
        "out = memory_efficient_attention(q, k, v, attn_bias=attn_bias)\n",
        "print(out.shape)  # (B, L, D)\n"
      ],
      "metadata": {
        "id": "MUGgDAo_qqPs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title gemini xformers.ops.memory_efficient_attention\n",
        "import torch\n",
        "import xformers.ops as xops\n",
        "from xformers.ops.fmha.attn_bias import LocalAttentionFromBottomRightMask, LowerTriangularFromBottomRightLocalAttentionMask\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 1024\n",
        "num_heads = 8\n",
        "head_dim = 64\n",
        "\n",
        "# Example Q, K, V tensors\n",
        "query = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
        "key = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
        "value = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import xformers.ops as xops\n",
        "from xformers.ops.fmha.attn_bias import AttentionBias\n",
        "\n",
        "class MyCustomAttentionBias(AttentionBias):\n",
        "    def __init__(self, some_custom_param):\n",
        "        super().__init__()\n",
        "        self.some_custom_param = some_custom_param\n",
        "\n",
        "    def _materialize(self, shape: tuple[int, int, int, int], dtype: torch.dtype):\n",
        "        batch_size, num_heads, query_seq_len, key_seq_len = shape\n",
        "\n",
        "        # Create your custom mask logic here.\n",
        "        # This mask will be ADDED to the QK^T scores before softmax.\n",
        "        # -inf to block attention, 0 for no change.\n",
        "        mask = torch.zeros(query_seq_len, key_seq_len, device=\"cuda\", dtype=dtype)\n",
        "\n",
        "        # Example: A simple causal mask (lower triangular)\n",
        "        # For a truly memory-efficient custom mask, you'd want to avoid\n",
        "        # directly creating a large mask if possible, or ensure it's\n",
        "        # efficiently handled by xformers' kernels.\n",
        "        # For demonstration, let's create a causal mask for a small example.\n",
        "        for i in range(query_seq_len):\n",
        "            mask[i, i + 1:] = float('-inf')\n",
        "\n",
        "        # If you have specific non-standard patterns, implement them here.\n",
        "        # For instance, a very specific block-sparse pattern, or\n",
        "        # attention based on some external graph connectivity.\n",
        "\n",
        "        # Example: Block attention for every 3rd token (conceptual, just for demo)\n",
        "        # for i in range(query_seq_len):\n",
        "        #     for j in range(key_seq_len):\n",
        "        #         if (i % 3 == 0) or (j % 3 == 0):\n",
        "        #             mask[i, j] = float('-inf') # Block if row or col index is a multiple of 3\n",
        "\n",
        "        # You might also want to expand it to (batch_size, num_heads, Q_seq, K_seq)\n",
        "        # if your mask is batch or head dependent.\n",
        "        # For a general sequence-level mask, unsqueeze to add batch and head dims\n",
        "        return mask.unsqueeze(0).unsqueeze(0) # unsqueeze for batch_size and num_heads if not batch/head dependent\n",
        "\n",
        "# More complex example: Attention only within specific 'segments'\n",
        "class SegmentedAttentionBias(AttentionBias):\n",
        "    def __init__(self, segment_ids_q, segment_ids_k):\n",
        "        super().__init__()\n",
        "        # segment_ids_q: (batch_size, query_seq_len) tensor indicating segment for each query token\n",
        "        # segment_ids_k: (batch_size, key_seq_len) tensor indicating segment for each key token\n",
        "        self.segment_ids_q = segment_ids_q\n",
        "        self.segment_ids_k = segment_ids_k\n",
        "\n",
        "    def _materialize(self, shape: tuple[int, int, int, int], dtype: torch.dtype):\n",
        "        batch_size, num_heads, query_seq_len, key_seq_len = shape\n",
        "\n",
        "        # Ensure segment IDs match the sequence lengths\n",
        "        assert self.segment_ids_q.shape[1] == query_seq_len\n",
        "        assert self.segment_ids_k.shape[1] == key_seq_len\n",
        "        assert self.segment_ids_q.shape[0] == batch_size\n",
        "        assert self.segment_ids_k.shape[0] == batch_size\n",
        "\n",
        "        # Create the base mask\n",
        "        mask = torch.empty(batch_size, query_seq_len, key_seq_len, device=self.segment_ids_q.device, dtype=dtype)\n",
        "\n",
        "        # Iterate over batches to apply segment logic\n",
        "        # For true memory efficiency, this would ideally be done with\n",
        "        # clever tensor ops rather than a Python loop if B is large.\n",
        "        for b in range(batch_size):\n",
        "            # Expand segment_ids for broadcasting\n",
        "            seg_q_expanded = self.segment_ids_q[b].unsqueeze(1) # (Q_seq, 1)\n",
        "            seg_k_expanded = self.segment_ids_k[b].unsqueeze(0) # (1, K_seq)\n",
        "\n",
        "            # Attention only allowed if segment IDs match\n",
        "            # This creates a boolean mask (True where segments match)\n",
        "            segment_match = (seg_q_expanded == seg_k_expanded)\n",
        "\n",
        "            # Convert boolean mask to attention bias (0 or -inf)\n",
        "            mask[b] = torch.where(segment_match, 0.0, float('-inf'))\n",
        "\n",
        "        # Expand for num_heads if the mask is independent of heads\n",
        "        return mask.unsqueeze(1) # (B, 1, Q_seq, K_seq) -> will broadcast to all heads\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iuCwlaRCtF75",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title argm\n",
        "\n",
        "def soft_clamp_relu(x, lower=None, upper=None):\n",
        "    if lower != None: x = lower+F.relu(x-lower)\n",
        "    if upper != None: x = upper-F.relu(upper-x)\n",
        "    return x\n",
        "\n",
        "def argm_s(sx, x, h0, zz=None): # [1, d_model], [batch_, T, dim_a], [num_layers, 1, d_model], zz[batch_,T, dim_z] # batch argm z for search\n",
        "    \"\"\"batch optimise worst case z to increase cost\"\"\"\n",
        "    batch_, T, _ = x.shape\n",
        "    batch = 16 # 16\n",
        "    # z = nn.Parameter(torch.zeros((batch*batch_, T, self.dim_z),device=device))\n",
        "    z = nn.Parameter(torch.randn((batch*batch_, T, self.dim_z),device=device)*.02)\n",
        "    # torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # 1.:norm~1 ; 1.\n",
        "    optim_z = torch.optim.SGD([z], lr=1e3, momentum=0.999, maximize=True) # 3e3\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-1, (0.1, 0.5), maximize=True) # 1e-1, (0.1, 0.5)\n",
        "    with torch.no_grad():\n",
        "        z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch*batch_,1,1) # [batch*batch_, seq_len, dim_z]\n",
        "        if zz != None: z[:batch_]=zz #z[::batch]\n",
        "    sx, h0 = sx.detach(), h0.detach() # [1, d_model], [num_layers, 1, d_model]\n",
        "    x = x.detach().repeat(batch,1,1) # [batch, T, dim_a]\n",
        "    # print(\"argm_s\", z[0][0].squeeze())\n",
        "    for i in range(5): # 2 5\n",
        "        loss, lh0 = self.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "        loss.sum().backward() # [batch, T]\n",
        "        optim_z.step()\n",
        "        optim_z.zero_grad()\n",
        "        z /= soft_clamp_relu(torch.norm(z, dim=-1).unsqueeze(-1), lower=1)\n",
        "        # print(i, \"argm_s z\", z[0][0].squeeze().data)\n",
        "        print(i, \"argm_s loss\", loss.sum(-1)[0].data)\n",
        "        # # print(torch.norm(z, dim=-1)) # all 1\n",
        "    idx = torch.argmax(loss.sum(-1).unflatten(0, (batch,batch_)), dim=0) # loss [batch*batch_, T] -> [batch_]\n",
        "    # print(\"argm_s loss[idx]\", loss[idx].sum().item())\n",
        "    return z.unflatten(0, (batch,batch_))[idx, torch.arange(batch_)]\n",
        "\n"
      ],
      "metadata": {
        "id": "pKAESdSGg5Ib",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCN055Zr7Dq4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title gpt train test generate\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optimizer, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            logits = model(x) # [b,t,d]\n",
        "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.flatten()) # [512, 128, 283], [512, 128]\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0: print(\"strain\",loss.item())\n",
        "        total_loss += loss.item()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def test(loader, model):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "        loss = F.cross_entropy(logits.flatten(end_dim=-2), y.flatten()) # [batch*seq_len, vocab_size], [batch*seq_len]\n",
        "        if i % 100 == 0: print(\"test\",loss.item())\n",
        "        try: wandb.log({\"test loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def generate(model, context, max_steps=64, temperature=1): # [b,t]\n",
        "    training = model.training\n",
        "    model.eval()\n",
        "    x = encode(context)\n",
        "    for n in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            out = model(x)\n",
        "        out = out[:,-1] # get logit for last character\n",
        "        out = out/temperature\n",
        "        out = F.softmax(out, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(out, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat([x, ix],1)\n",
        "    completion = decode(x.squeeze(0))\n",
        "    if training: model.train()\n",
        "    return completion\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "for i in range(1):\n",
        "# for i in range(30):\n",
        "    train_loss = strain(model, train_loader, optim, scheduler=None)\n",
        "    test_loss = test(test_loader, model)\n",
        "    print('Test Loss:', test_loss)\n",
        "    print(generate(model, \"this is what \"))\n",
        "    # print(model.generate('this is wh'))\n",
        "    print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/Dao-AILab/flash-attention\n",
        "!pip install -q flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "LWsArYn2dnH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dao-AILab/flash-attention flash_attn_with_kvcache\n",
        "def flash_attn_with_kvcache(\n",
        "    q,\n",
        "    k_cache,\n",
        "    v_cache,\n",
        "    k=None,\n",
        "    v=None,\n",
        "    rotary_cos=None,\n",
        "    rotary_sin=None,\n",
        "    cache_seqlens: Optional[Union[(int, torch.Tensor)]] = None,\n",
        "    cache_batch_idx: Optional[torch.Tensor] = None,\n",
        "    block_table: Optional[torch.Tensor] = None,\n",
        "    softmax_scale=None,\n",
        "    causal=False,\n",
        "    window_size=(-1, -1),  # -1 means infinite context window\n",
        "    rotary_interleaved=True,\n",
        "    alibi_slopes=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    If k and v are not None, k_cache and v_cache will be updated *inplace* with the new values from\n",
        "    k and v. This is useful for incremental decoding: you can pass in the cached keys/values from\n",
        "    the previous step, and update them with the new keys/values from the current step, and do\n",
        "    attention with the updated cache, all in 1 kernel.\n",
        "\n",
        "    If you pass in k / v, you must make sure that the cache is large enough to hold the new values.\n",
        "    For example, the KV cache could be pre-allocated with the max sequence length, and you can use\n",
        "    cache_seqlens to keep track of the current sequence lengths of each sequence in the batch.\n",
        "\n",
        "    Also apply rotary embedding if rotary_cos and rotary_sin are passed in. The key @k will be\n",
        "    rotated by rotary_cos and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.\n",
        "    If causal or local (i.e., window_size != (-1, -1)), the query @q will be rotated by rotary_cos\n",
        "    and rotary_sin at indices cache_seqlens, cache_seqlens + 1, etc.\n",
        "    If not causal and not local, the query @q will be rotated by rotary_cos and rotary_sin at\n",
        "    indices cache_seqlens only (i.e. we consider all tokens in @q to be at position cache_seqlens).\n",
        "\n",
        "    See tests/test_flash_attn.py::test_flash_attn_kvcache for examples of how to use this function.\n",
        "\n",
        "    Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads\n",
        "    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\n",
        "    For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head\n",
        "    0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.\n",
        "\n",
        "    If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.\n",
        "    For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:\n",
        "        1 1 1 1 0\n",
        "        1 1 1 1 1\n",
        "    If seqlen_q = 5 and seqlen_k = 2, the causal mask is:\n",
        "        0 0\n",
        "        0 0\n",
        "        0 0\n",
        "        1 0\n",
        "        1 1\n",
        "    If the row of the mask is all zero, the output will be zero.\n",
        "\n",
        "    If window_size != (-1, -1), implements sliding window local attention. Query at position i\n",
        "    will only attend to keys between\n",
        "    [i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.\n",
        "\n",
        "    Note: Does not support backward pass.\n",
        "\n",
        "    Arguments:\n",
        "        q: (batch_size, seqlen, nheads, headdim)\n",
        "        k_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,\n",
        "            or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)\n",
        "            page_block_size must be a multiple of 256.\n",
        "        v_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,\n",
        "            or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)\n",
        "        k [optional]: (batch_size, seqlen_new, nheads_k, headdim). If not None, we concatenate\n",
        "            k with k_cache, starting at the indices specified by cache_seqlens.\n",
        "        v [optional]: (batch_size, seqlen_new, nheads_k, headdim). Similar to k.\n",
        "        rotary_cos [optional]: (seqlen_ro, rotary_dim / 2). If not None, we apply rotary embedding\n",
        "            to k and q. Only applicable if k and v are passed in. rotary_dim must be divisible by 16.\n",
        "        rotary_sin [optional]: (seqlen_ro, rotary_dim / 2). Similar to rotary_cos.\n",
        "        cache_seqlens: int, or (batch_size,), dtype torch.int32. The sequence lengths of the\n",
        "            KV cache.\n",
        "        block_table [optional]: (batch_size, max_num_blocks_per_seq), dtype torch.int32.\n",
        "        cache_batch_idx: (batch_size,), dtype torch.int32. The indices used to index into the KV cache.\n",
        "            If None, we assume that the batch indices are [0, 1, 2, ..., batch_size - 1].\n",
        "            If the indices are not distinct, and k and v are provided, the values updated in the cache\n",
        "                 might come from any of the duplicate indices.\n",
        "        softmax_scale: float. The scaling of QK^T before applying softmax.\n",
        "            Default to 1 / sqrt(headdim).\n",
        "        causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).\n",
        "        window_size: (left, right). If not (-1, -1), implements sliding window local attention.\n",
        "        rotary_interleaved: bool. Only applicable if rotary_cos and rotary_sin are passed in.\n",
        "            If True, rotary embedding will combine dimensions 0 & 1, 2 & 3, etc. If False,\n",
        "            rotary embedding will combine dimensions 0 & rotary_dim / 2, 1 & rotary_dim / 2 + 1\n",
        "            (i.e. GPT-NeoX style).\n",
        "        alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of\n",
        "            (-alibi_slope * |i + seqlen_k - seqlen_q - j|)\n",
        "            is added to the attention score of query i and key j.\n",
        "\n",
        "    Return:\n",
        "        out: (batch_size, seqlen, nheads, headdim).\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4Do9MJibVCVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title flash_attn_with_kvcache no custom mask\n",
        "import torch\n",
        "from flash_attn import flash_attn_with_kvcache\n",
        "\n",
        "b,t,nheads,dhead = 2,5,3,4\n",
        "# q: (batch_size, seqlen, nheads, headdim)\n",
        "# k_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,\n",
        "#     or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)\n",
        "#     page_block_size must be a multiple of 256.\n",
        "# v_cache: (batch_size_cache, seqlen_cache, nheads_k, headdim) if there's no block_table,\n",
        "#     or (num_blocks, page_block_size, nheads_k, headdim) if there's a block_table (i.e. paged KV cache)\n",
        "# k [optional]: (batch_size, seqlen_new, nheads_k, headdim). If not None, we concatenate\n",
        "#     k with k_cache, starting at the indices specified by cache_seqlens.\n",
        "# v [optional]: (batch_size, seqlen_new, nheads_k, headdim). Similar to k.\n",
        "\n",
        "cache_len = 7\n",
        "q = torch.rand(b,t,nheads,dhead)\n",
        "k = torch.rand(b,t,nheads,dhead)\n",
        "v = torch.rand(b,t,nheads,dhead)\n",
        "k_cache = torch.rand(b,cache_len,nheads,dhead)\n",
        "v_cache = torch.rand(b,cache_len,nheads,dhead)\n",
        "\n",
        "\n",
        "out = flash_attn_with_kvcache(q, k_cache, v_cache, k=None, v=None)\n",
        "out = flash_attn_with_kvcache(q, k_cache, v_cache, k, v)\n",
        "print(out)\n",
        "\n",
        "out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask.unsqueeze(1) if mask != None else None, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "# out = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "# attn = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "# # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "# if mask != None: attn = attn.masked_fill(mask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "# attention = torch.softmax(attn, dim=-1)\n",
        "# out = self.drop(attention) @ v # [batch, n_heads, T, d_head]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3fKu55xKd1rO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## old blt"
      ],
      "metadata": {
        "id": "6e_ZnJPtzrzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLTlayer transformer fsq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class BLTlayer(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=None, quant=6, num_layers=1):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_dim\n",
        "        self.pos_enc = RoPE(d_model, base=100) # 10000\n",
        "        # self.latent = nn.Parameter(torch.randn(1,1,d_model)*.02) # init\n",
        "        # self.perceiver = AttentionBlock(d_model, n_heads=4, cond_dim=d_model)\n",
        "        self.perceiver = Seq(*[AttentionBlock(d_model, n_heads=4, cond_dim=d_model) for _ in range(num_layers)])\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.patch_enc = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.conv_emb = nn.Conv1d(d_model, d_model, 1)\n",
        "        self.vq = codequant(levels = d_model*[quant])\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t,d], [b,t,t]\n",
        "        x = self.pos_enc(x)\n",
        "        b,t,d = x.shape\n",
        "        # if mask==None: mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # F->mask # for F.scaled_dot_product_attention\n",
        "        # print('BLTlayer',x.shape, cond.shape if cond!=None else None, mask.shape if mask!=None else None)\n",
        "        x = self.perceiver(x, cond=cond if cond!=None else x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        out = self.lin(x)\n",
        "        return out # [b,t,d]\n",
        "\n",
        "    def gen_loss(self, y): # [b,t,d]\n",
        "        b,t,d = y.shape\n",
        "        mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1)\n",
        "        y_ = self.forward(y, mask=mask) # [b,t,d], [b,t,t]\n",
        "        print('gen_loss', y_.shape, y.shape)\n",
        "        loss = F.mse_loss(y_[:,:-1], y[:,1:])\n",
        "        return loss, y_\n",
        "\n",
        "    def to_patch(self, x, out=None, entropies=None): # [b,t,d]\n",
        "        if entropies==None: entropies = (out-self.vq(out)).mean(-1) # [b,t] float\n",
        "        mask = patchify(entropies, 1.) # [b,t] # T at start of each patch, F otherwise\n",
        "        patch_mask = padded_block_mask(mask) # [batch ,num_patch, seq_len]\n",
        "        # print('lyr to_patch', x.shape, out.shape, patch_mask.shape)\n",
        "\n",
        "        latent = (x.unsqueeze(1) *patch_mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "        # latent = self.latent.repeat(1,mask.shape[1],1) # [b ,num_patch, d]\n",
        "        latent = latent + self.patch_enc\n",
        "        out = self.conv_emb(x.transpose(-2,-1)).transpose(-2,-1) # instead of hashing\n",
        "        patches = self.forward(latent, cond=out, mask=patch_mask) # [batch, num_patch, latent_dim/d_model]\n",
        "        # patches = self.vqp(patches)\n",
        "        return patches, mask, patch_mask # [b, num_patch, out_dim], [b,t], [b ,num_patch, t]\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "\n",
        "    def ae_loss(self, y, patches, mask, patch_mask): # [b,t,d], [b, n_patch, d], [b,t], [b, n_patch, t]\n",
        "        b,t,d = y.shape\n",
        "        # cond = patch_mask.transpose(-2,-1).to(patches.dtype) @ patches # [b, t, out_dim]\n",
        "        patch_mask = patch_mask.transpose(-2,-1) # [b, t, n_patch]\n",
        "        causal_mask = block_diag(mask) & torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # [b,t,t] # only attend within the patch & is causal\n",
        "        # print('ae_loss', y.shape, patches.shape, patch_mask.shape) # [2, 7, 16], [2, 3, 16], [2, 7, 3]\n",
        "        y_ = self.forward(y, cond=torch.cat([y, patches], dim=1), mask=torch.cat([causal_mask, patch_mask], dim=-1)) # cond: [b, t+n_patch, d]; mask: [batch, len_q, len_v] [b, t, t+n_patch]\n",
        "        print('ae_loss', y_.shape, y.shape)\n",
        "        # loss = F.mse_loss(y[mask], y_[mask])\n",
        "        loss = F.mse_loss(y_[:,:-1], y[:,1:])\n",
        "        return loss\n",
        "\n",
        "# generate one tok unless uncertain ? then patch up?\n",
        "# gen one only bec blt need to tok\n",
        "# or blt feed only one tok? nope, should parallelise\n",
        "# feed all toks bec topatch need out\n",
        "    def generate_one(self, x=None, cond=None, patch=None): # [1,t,d], [1,len_cond,d], [1,1,d]\n",
        "        # print('lyr gen1 x cond patch', x.shape, cond.shape if cond!=None else None, patch.shape if patch!=None else None)\n",
        "        _,t,d = x.shape\n",
        "        if cond==None:\n",
        "            cond = x\n",
        "            mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(1,1,1)\n",
        "        else:\n",
        "            mask = torch.ones((1,t,cond.shape[1]), dtype=bool, device=device) # [1,t,n_cond+n_patch]\n",
        "        if patch!=None:\n",
        "            cond = torch.cat([cond, patch], dim=1)\n",
        "            pmask = torch.zeros((1,t,patch.shape[1]), dtype=bool, device=device)\n",
        "            pmask[:,-1] = True # patch is only for last (aka new) tok\n",
        "            mask = torch.cat([mask, pmask], dim=-1) # [1,t,t+1]\n",
        "            # mask = torch.ones((1,t,cond.shape[1]), dtype=bool, device=device) # [1,t,n_cond+n_patch]\n",
        "        # print('lyr gen1', x.shape, cond.shape, mask.shape)\n",
        "        out = self.forward(x, cond=cond, mask=mask)\n",
        "        qout = self.vq(out)\n",
        "        etp = (out-qout).mean(-1) # [b,1] float\n",
        "        # x = torch.cat([x, qout], dim=1)\n",
        "        # return x, etp\n",
        "        return qout, etp\n",
        "\n",
        "# # generate till uncertain\n",
        "#     def generate_all(self, x=None, cond=None, patch=None): # [1,t,d], [1,1,d]/[1,t,d]?\n",
        "#         out, entropies = model.generate_one(x, patch=patch)\n",
        "#         x = torch.cat([x, out[:,-1:]], dim=1)\n",
        "#         mask = patchify(entropies, 1.)\n",
        "#         while not mask[:,-1]:\n",
        "#             out, etp = model.generate_one(x[:,-1:], cond=x[:,:-1], patch=patch)\n",
        "#             x = torch.cat([x, out], dim=1)\n",
        "#             entropies = torch.cat([entropies, etp], dim=1) # [b,t]\n",
        "#             # if i==0: continue # want gen at least one\n",
        "#             mask = patchify(entropies, 1.)\n",
        "#         return x[:,:-1], entropies\n",
        "\n",
        "# if no patch, generate till uncertain\n",
        "# if got patch list, iter through\n",
        "    def generate_all(self, x=None, pch_lst=[None]): # [1,t,d], [1,1,d]/[1,t,d]?\n",
        "    # def generate_all(self, x=None, pch_lst=[]): # [1,t,d], [1,1,d]/[1,t,d]?\n",
        "        # if pch_lst[0]==None:\n",
        "        # out, entropies = self.generate_one(x, patch=pch_lst.pop())\n",
        "        out, entropies = self.generate_one(x, patch=pch_lst[0])\n",
        "        # x = torch.cat([x, out[:,-1:]], dim=1)\n",
        "        mask = patchify(entropies, 1.)\n",
        "        for patch in pch_lst:\n",
        "            while not mask[:,-1]:\n",
        "                out, etp = self.generate_one(x[:,-1:], cond=x[:,:-1], patch=patch)\n",
        "                x = torch.cat([x, out], dim=1)\n",
        "                entropies = torch.cat([entropies, etp], dim=1) # [b,t]\n",
        "                # if i==0: continue # want gen at least one\n",
        "                mask = patchify(entropies, 1.)\n",
        "            # x, mask, entropies = x[:,:-1], mask[:,:-1], entropies[:,:-1]\n",
        "        # return x[:,:-1], entropies\n",
        "        # print('lyr gen_all', x.shape, entropies.shape)\n",
        "        return x, entropies # [1,t,d], [1,t]\n",
        "\n",
        "# trans: btd->btd, train: shift1\n",
        "# bltlyr: btd->btd+bpd , gentrain: shift1, artrain:btd+bpd->btd\n",
        "\n",
        "# f_ j ; vs f_ t_\n",
        "# full text\n",
        "\n",
        "# is gpt + aetrain?\n",
        "\n",
        "# in: qunatseq\n",
        "# fwd: qunatx -> y_\n",
        "# to_patch: x, y_ -> patches, mask\n",
        "# gen: x, (patch) ->\n",
        "\n",
        "\n",
        "batch=25\n",
        "d_model = 16\n",
        "in_dim=128\n",
        "\n",
        "model = BLTlayer(in_dim, d_model).to(device)\n",
        "# model = BLTlayer(d_model).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # 1e-4 #3e-4\n",
        "\n",
        "\n",
        "x = torch.rand((batch, 7, d_model), device=device)\n",
        "out = model(x)\n",
        "print('out', out.shape)\n",
        "\n",
        "patches, mask, patch_mask = model.to_patch(x, out)\n",
        "print('patches, mask',patches.shape, mask.shape)\n",
        "gloss = model.gen_loss(out)\n",
        "aloss = model.ae_loss(x, patches, mask, patch_mask)\n",
        "# loss = gloss + aloss\n",
        "\n",
        "# patch = torch.rand(batch,1,d_model)\n",
        "# patch = torch.rand(1,1,d_model)\n",
        "# pch_lst = [patch]\n",
        "pch_lst = list(torch.rand(1,3,d_model).split(1, dim=1))\n",
        "# out, mask = model.generate(x)\n",
        "# out, mask = model.generate(x, patch)\n",
        "x = torch.rand((1,7, d_model), device=device)\n",
        "out, _ = model.generate_all(x)\n",
        "# out, _ = model.generate_all(x, patch=patch)\n",
        "# out, _ = model.generate_all(x[:,-1:], cond=x[:,:-1], patch=patch)\n",
        "out, _ = model.generate_all(x, pch_lst=pch_lst)\n",
        "# out, _ = model.generate_all(x[:,-1:], cond=x[:,:-1])\n",
        "\n",
        "# out, _ = model.generate_one(x) # gen all till uncertain\n",
        "# out, _ = model.generate_one(x, patch=patch) # patch cond from last tok onwards\n",
        "# out, _ = model.generate_one(x[:,-1:], cond=x[:,:-1], patch=patch) # got cond no need to cond on all\n",
        "# out, _ = model.generate_one(x[:,-1:], cond=x[:,:-1])\n",
        "print(out.shape)\n",
        "# out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "# print(patches.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9026aa83-5366-49ca-f612-4951ed7ed3ca",
        "cellView": "form",
        "id": "-AfXjApVBWuX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3840\n",
            "out torch.Size([25, 7, 16])\n",
            "patches, mask torch.Size([25, 6, 16]) torch.Size([25, 7])\n",
            "gen_loss torch.Size([25, 7, 16]) torch.Size([25, 7, 16])\n",
            "ae_loss torch.Size([25, 7, 16]) torch.Size([25, 7, 16])\n",
            "torch.Size([1, 9, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLT me fsq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class BLT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "    def __init__(self, d_model, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([BLTlayer(d_model, d_model, quant=6*2**i) for i in range(num_layers)])\n",
        "\n",
        "    # def forward(self, x): # [b,t]\n",
        "    #     x = self.tok_emb(x)\n",
        "    #     for level in self.layer:\n",
        "    #         # out, x = level(x) # [b,t,d]\n",
        "    #         out = level(x) # [b,t,d]\n",
        "    #         patches, mask, patch_mask = level.to_patch(x, out)\n",
        "    #         # outs.append(out)\n",
        "    #         x = level.to_patch(out)\n",
        "    #     x = self.lin(x)\n",
        "    #     return x # [batch, num_patch, out_dim]\n",
        "\n",
        "    def loss(self, x): # [b,t,d]\n",
        "        tt_loss = 0\n",
        "        for i, level in enumerate(self.layer):\n",
        "            b,t,d = x.shape\n",
        "            # mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # F->mask # for F.scaled_dot_product_attention\n",
        "            gloss, out = level.gen_loss(x)\n",
        "            tt_loss += gloss\n",
        "            if i != len(self.layer)-1: # last layer no patching\n",
        "                patches, mask, patch_mask = level.to_patch(x, out)\n",
        "                tt_loss += level.ae_loss(x, patches, mask, patch_mask)\n",
        "                x = patches\n",
        "        return tt_loss\n",
        "\n",
        "\n",
        "\n",
        "#     def generate(self, x):\n",
        "#         patches = []\n",
        "#         for j, patch in enumerate(patches):\n",
        "#             if len(patch)>0:\n",
        "#                 break\n",
        "#         # patch in patches\n",
        "#         for i in reversed(range(j)):\n",
        "#             patch = patches[i].pop(0)\n",
        "#             layer = self.layers[i]\n",
        "#             # = self.perceiver(x, cond=patch)\n",
        "#             out = layer.generate(x if i==0 else None, patch)\n",
        "#             if i!=0: patches[i-1].append(out[:,-1])\n",
        "# # out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "#         # entropies = entropy(out) # [b,t] float\n",
        "#         # mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "#         return out[:,-1]\n",
        "\n",
        "    def generate_all(self, x, src_lst=[], pch_lst=[]): # [1,t,d]\n",
        "    # def generate_all(self, x, src_lst=[]): # [1,t,d]pch_lst=[]\n",
        "        for i, level in enumerate(self.layer):\n",
        "            # out, _ = level.generate_all(x)\n",
        "            out, entropies = level.generate_all(x)\n",
        "            src_lst.append(out)\n",
        "            # pch_lst.append(out[:,x.shape[1]:])\n",
        "            # print('ifyvh blt genall',out[:,x.shape[1]:].shape,list(out[:,x.shape[1]:].split(1, dim=1)))\n",
        "            pch_lst.append(list(out[:,x.shape[1]:].split(1, dim=1)))\n",
        "\n",
        "            if i == len(self.layer)-1: break # top layer dont need patch\n",
        "            # if i != len(self.layer)-1:\n",
        "            # patches, mask, patch_mask = level.to_patch(x, out, entropies=entropies)\n",
        "            patches, mask, _ = level.to_patch(out, entropies=entropies)\n",
        "            # new_pch = mask[:,x.shape[1]:].sum(-1)\n",
        "            # pch_lst.append([patches[:,-new_pch:]])\n",
        "            # pch_lst.append(patches[:,-new_pch:])\n",
        "            x = patches\n",
        "            # src_lst.append(patches)\n",
        "            # src_lst[i+1] = patches\n",
        "\n",
        "            # x = level.to_patch(out)\n",
        "            # src_lst.append(x)\n",
        "        # if self.lin: x = self.lin(x)\n",
        "        # return x # [batch, num_patch, out_dim]\n",
        "        patch = None\n",
        "        # print('5555555555555', pch_lst)\n",
        "        # print('5555555555555', len(pch_lst))\n",
        "        # for i in reversed(range(len(self.layers))):\n",
        "        for i, level in enumerate(reversed(self.layer)):\n",
        "            # layer = self.layers[i]\n",
        "            # x = src_lst.pop()\n",
        "            # x = src_lst[i-1]\n",
        "            if i == len(self.layer)-1:\n",
        "                # out, _ = level.generate_one(x, patch=patch)\n",
        "                out, _ = level.generate_one(x[:,-1:], cond=x[:,:-1])\n",
        "            else:\n",
        "\n",
        "                # out, _ = level.generate_all(x[:,-1:], cond=x[:,:-1], pch_lst=pch_lst)\n",
        "                # out, _ = level.generate_all(x, pch_lst=pch_lst[-i])\n",
        "                # print('blt genall', i, src_lst[-i].shape, [p.shape for p in pch_lst])\n",
        "                # print('blt genall', i, src_lst[-i].shape, pch_lst[-i].shape)\n",
        "                # print('blt genall', i, src_lst[-i].shape, len(pch_lst[-i]))\n",
        "                # print(pch_lst[-i])\n",
        "                # print('blt genall', i, src_lst[-i], [p for p in pch_lst])\n",
        "                # torch.cat([pch_lst[-1-i]])\n",
        "                # out[:,src_lst[-2-i]:]\n",
        "                out, _ = level.generate_all(src_lst[-2-i], pch_lst=pch_lst[-1-i])\n",
        "            # out, mask_ = level.generate(x, patch)\n",
        "            # out = level.generate_all(x, patch=patch)\n",
        "            # out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask_],dim=1))\n",
        "            # = self.perceiver(x, cond=patch)\n",
        "            # patch = x\n",
        "            # torch.cat([pch_lst[-i]], dim=)\n",
        "            # out = layer.generate(x if i==0 else None, patch)\n",
        "            # if i!=0: patches[i-1].append(out[:,-1])\n",
        "\n",
        "        # entropies = entropy(out) # [b,t] float\n",
        "        # mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "\n",
        "        return out#[:,-1]\n",
        "\n",
        "\n",
        "\n",
        "# if not confident, get patch from higher level\n",
        "    def generate_one(self, x, src_lst=None, etp_lst=None): # [b,t]\n",
        "        # x = self.tok_emb(x) # [b,t,d]\n",
        "        b,t,d = x.shape\n",
        "        src_lst = src_lst or len(self.layer)*[None]\n",
        "        etp_lst = etp_lst or len(self.layer)*[torch.empty((b,0), dtype=torch.float, device=device)]\n",
        "\n",
        "        # for i, level in enumerate(self.layer):\n",
        "        for i, (level, entropies) in enumerate(zip(self.layer, etp_lst)):\n",
        "            out, entropies = level.generate_one(x)\n",
        "            # out, qout = level.generate_one(x, cond=src_lst[i])\n",
        "            # print('gen one11', x.shape, out.shape)\n",
        "\n",
        "            mask = patchify(entropies, 1.) # [1,t] # T at start of each patch, F otherwise\n",
        "\n",
        "            # entropies = torch.cat([entropies, etp], dim=1) # [b,t]\n",
        "            # etp_lst[i] = torch.cat([etp_lst[i], etp], dim=1) # [b,t]\n",
        "            # mask = patchify(etp_lst[i], 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "            # print('gen one', mask.shape)\n",
        "            # print('gen one', x.shape, out.shape)\n",
        "            if not mask[:,-1]: break # newly generated tok is confident\n",
        "\n",
        "            if i==len(self.layer)-1: break # last layer no patching\n",
        "            src_lst.append(x)\n",
        "            patches, _, _ = level.to_patch(x, entropies=entropies)\n",
        "            x = patches\n",
        "\n",
        "        # if i==0: return x, src_lst, etp_lst # 1st lvl is confident [1, t+1, d]\n",
        "        patch = None\n",
        "        # for i in reversed(range(len(self.layers))):\n",
        "        for i, v in enumerate(reversed(src_lst)):\n",
        "            # patch, etp = self.layer[len(src_lst)-i].generate_one(v, patch=patch)\n",
        "            patch, etp = self.layer[len(src_lst)-i].generate_one(v[:,:-1], cond=v[:,-1:], patch=patch)\n",
        "            # patch, etp = self.layer[len(src_lst)-i].generate_one(x, patch)\n",
        "\n",
        "        # patch = self.out(patch)\n",
        "        return patch, src_lst, etp_lst\n",
        "# fwd, quant, entropy topatch; save qout, entropy\n",
        "# query new tok, append entropy\n",
        "\n",
        "\n",
        "\n",
        "# pos_all, entropy mask;\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "# gen train: mse(x[1:],out[:-1])\n",
        "# gen inference: x cond=x, cond=(x+patch(+*learnt?)), if entropy/bos, pop patch and regen\n",
        "\n",
        "# # fsq: dont realize tokens in layer\n",
        "# layer:\n",
        "# posemb\n",
        "# x less quants patch more quants\n",
        "# quant after linout\n",
        "# # quant before linout\n",
        "\n",
        "# blt:\n",
        "# tokemb\n",
        "# linout to tok\n",
        "\n",
        "\n",
        "# gen:\n",
        "# gen lower only, until uncertain, then patch, gen top then patch cond bottom\n",
        "# gen 1 tok. if uncertain, to_patch then top gen\n",
        "\n",
        "d_model=16\n",
        "model = BLT(d_model, num_layers=3)\n",
        "x = torch.rand(5,30,d_model)\n",
        "# out = model(x)\n",
        "out = model.loss(x)\n",
        "# print(out)\n",
        "# out = model.generate(x)\n",
        "# out, src_lst, etp_lst = model.generate_one(x)\n",
        "\n",
        "x = torch.rand(1,13,d_model)\n",
        "# pch_lst = list(torch.rand(1,3,d_model).split(1, dim=1))\n",
        "out = model.generate_all(x)\n",
        "# out = model.generate_all(x, pch_lst=pch_lst)\n",
        "print(out.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bdf01a3-64d2-410a-a4c1-b0c87646115f",
        "cellView": "form",
        "id": "n6az9dEddSyA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLTlayer transformer fsq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# transformer\n",
        "class BLTlayer(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model=None, out_dim=None, num_layers=1):\n",
        "    def __init__(self, in_dim, d_model=None, quantxp=(6,12), num_layers=1):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_dim\n",
        "        self.pos_enc = RoPE(d_model, base=100) # 10000\n",
        "        # self.latent = nn.Parameter(torch.randn(1,1,d_model)*.02) # init\n",
        "        # self.perceiver = AttentionBlock(d_model, n_heads=4, cond_dim=d_model)\n",
        "        self.perceiver = Seq(*[AttentionBlock(d_model, n_heads=4, cond_dim=d_model) for _ in range(num_layers)])\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.out = nn.Linear(d_model, out_dim)\n",
        "        self.patch_enc = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.conv_emb = nn.Conv1d(d_model, d_model, 1)\n",
        "        self.vqx = codequant(levels = d_model*[quantxp[0]])\n",
        "        self.vqp = codequant(levels = d_model*[quantxp[1]])\n",
        "        # self.out = nn.Linear(d_model, in_dim)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t,d], [b,t,t]\n",
        "        x = self.pos_enc(x)\n",
        "        b,t,d = x.shape\n",
        "        # if mask==None: mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # F->mask # for F.scaled_dot_product_attention\n",
        "        # if type(mask)!=torch.tensor: mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # F->mask # for F.scaled_dot_product_attention\n",
        "        # if type(mask)==str: mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # F->mask # for F.scaled_dot_product_attention\n",
        "        # print('BLTlayer',x.shape, cond.shape if cond!=None else None, mask.shape if mask!=None else None)\n",
        "        x = self.perceiver(x, cond=cond if cond!=None else x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        out = self.lin(x)\n",
        "        return out # [b,t,d_model]\n",
        "\n",
        "    def gen_loss(self, src): # [b,t,d]\n",
        "        x, y = src[:,:-1], src[:,1:]\n",
        "        b,t,d = x.shape\n",
        "        mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1)\n",
        "        y_ = self.forward(x, mask=mask) # [b,t,d], [b,t,t]\n",
        "        # print('gen_loss', out.shape, y.shape)\n",
        "        loss = F.mse_loss(y_, y)\n",
        "        # loss = F.cross_entropy(out.flatten(0,1), y.flatten()) # [b*t,d], [b*t]\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def to_patch(self, x, out): # [b,t,d]\n",
        "        entropies = (out-self.vqx(out)).mean(-1) # [b,t] float\n",
        "        mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "        patch_mask = padded_block_mask(mask) # [batch ,num_patch, seq_len]\n",
        "        # print('to_patch', x.shape, out.shape, patch_mask.shape)\n",
        "\n",
        "        latent = (x.unsqueeze(1) *patch_mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "        # latent = self.latent.repeat(1,mask.shape[1],1) # [b ,num_patch, d]\n",
        "        latent = latent + self.patch_enc\n",
        "        out = self.conv_emb(x.transpose(-2,-1)).transpose(-2,-1)\n",
        "        patches = self.forward(latent, cond=out, mask=patch_mask) # [batch, num_patch, latent_dim/d_model]\n",
        "        # print('BLTlayer',out.shape)\n",
        "        patches = self.vqp(patches)\n",
        "        return patches, mask, patch_mask # [b, num_patch, out_dim], [b ,num_patch, t]\n",
        "\n",
        "    def ae_loss(self, y, patches, mask, patch_mask): # [b,t,d], [b, n_patch, d], [b,t], [b, n_patch, t]\n",
        "        # print('ae_loss', x.shape, self.tok_emb(x).shape) # [2, 7, 16], [2, 5, 128], [2, 7, 5]\n",
        "        x = self.pos_enc(y)\n",
        "        b,t,d = x.shape\n",
        "        # cond = patch_mask.transpose(-2,-1).to(patches.dtype) @ patches # [b, t, out_dim]\n",
        "        patch_mask = patch_mask.transpose(-2,-1) # [b, t, n_patch]\n",
        "        causal_mask = block_diag(mask) & torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # [b,t,t] # only attend within the patch & is causal\n",
        "        # print('ae_loss', x.shape, patches.shape, patch_mask.shape) # [2, 7, 16], [2, 3, 16], [2, 7, 3]\n",
        "        y_ = self.forward(x, cond=torch.cat([x, patches], dim=1), mask=torch.cat([causal_mask, patch_mask], dim=-1)) # cond: [b, t+n_patch, d]; mask: [batch, len_q, len_v] [b, t, t+n_patch]\n",
        "        loss = F.mse_loss(y[mask], y_[mask])\n",
        "        return loss\n",
        "\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "\n",
        "# generate till uncertain\n",
        "    def generate_all(self, x=None, patch=None): # [b,t,d], [b,t,d]\n",
        "    # def generate(self, x=None, patch=None, mask=None, patch_mask=None): # [b,t], [b,t,d], [b,t], [b,n_patch,t]\n",
        "        b,t,d = x.shape\n",
        "        entropies = torch.empty((b,0), device=device)\n",
        "        # while True:\n",
        "        for i in range(50):\n",
        "            if patch!=None: cond = torch.cat([x[:,:-1], patch],1)\n",
        "            else: cond = x[:,:-1]\n",
        "            out = self.forward(x[:,-1:], cond=cond)\n",
        "            qout = self.vqx(out)\n",
        "            entropy = (out-qout).mean(-1) # [b,1] float\n",
        "            x = torch.cat([x, qout], dim=1)\n",
        "            entropies = torch.cat([entropies, entropy], dim=1) # [b,t]\n",
        "            # if i==0: continue # want gen at least one\n",
        "            mask = patchify(entropies, 1.)\n",
        "            if (mask.sum(1)>=2).all(): break\n",
        "\n",
        "        # print('BLTlayer gen',mask.shape)\n",
        "        mask = mask.cumsum(dim=1)>=2 # [b,t]\n",
        "        out = x[:,t:]*mask.unsqueeze(-1)\n",
        "        # print('BLTlayer gen',x.shape, out.shape, mask.shape)\n",
        "\n",
        "        # return x[1:-1]\n",
        "        # return torch.cat((x, out),1), mask\n",
        "        return torch.cat((x[:,:t], out),1), torch.cat([torch.ones(b,t, dtype=bool, device=device), mask], dim=1)\n",
        "\n",
        "\n",
        "# generate till uncertain\n",
        "    def generate_all(self, x=None, cond=None, patch=None): # [1,t,d], [1,1,d]/[1,t,d]?\n",
        "        # _,t,d = x.shape\n",
        "        # mask=None\n",
        "        # if cond==None:\n",
        "        #     cond = x\n",
        "        #     mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(1,1,1)\n",
        "        # if patch!=None:\n",
        "        #     cond = torch.cat([cond, patch], dim=1)\n",
        "        #     pmask = torch.zeros((1,t,patch.shape[1]), dtype=bool, device=device)\n",
        "        #     pmask[:,-1] = True # patch is only for last tok\n",
        "        #     mask = torch.cat([mask, pmask], dim=-1) # [1,t,t+1]\n",
        "        #     # mask = torch.ones((1,t,cond.shape[1]), dtype=bool, device=device) # [1,t,n_cond+n_patch] # patch\n",
        "\n",
        "        # # print('lyr genall', x.shape, cond.shape, mask.shape)\n",
        "        # out = self.forward(x, cond=cond, mask=mask)\n",
        "        # qout = self.vqx(out)\n",
        "        # entropies = (out-qout).mean(-1)\n",
        "\n",
        "        out, entropies = model.generate_one(x, patch=patch)\n",
        "        x = torch.cat([x, out[:,-1:]], dim=1)\n",
        "\n",
        "\n",
        "        mask = patchify(entropies, 1.)\n",
        "        while not mask[:,-1]:\n",
        "        # for i in range(50):\n",
        "            # if patch!=None: cond = torch.cat([x[:,:-1], patch],1)\n",
        "            # else: cond = x[:,:-1]\n",
        "            # out = self.forward(x[:,-1:], cond=cond)\n",
        "            # qout = self.vqx(out)\n",
        "            # etp = (out-qout).mean(-1) # [b,1] float\n",
        "\n",
        "            out, etp = model.generate_one(x[:,-1:], cond=x[:,:-1], patch=patch)\n",
        "\n",
        "\n",
        "            x = torch.cat([x, out], dim=1)\n",
        "            entropies = torch.cat([entropies, etp], dim=1) # [b,t]\n",
        "            # if i==0: continue # want gen at least one\n",
        "            mask = patchify(entropies, 1.)\n",
        "        return x[:,:-1], entropies\n",
        "\n",
        "\n",
        "\n",
        "    def generate_one(self, x=None, patch=None): # [1,t,d], [1,1,d]\n",
        "    # def generate(self, x=None, patch=None, mask=None, patch_mask=None): # [b,t], [b,t,d], [b,t], [b,n_patch,t]\n",
        "        # t,d = x.shape\n",
        "        _,t,d = x.shape\n",
        "        # if patch!=None: cond = torch.cat([x[:,:-1], patch],1)\n",
        "        # else: cond = x[:,:-1]\n",
        "        # out = self.forward(x[:,-1:], cond=cond)\n",
        "\n",
        "        mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(1,1,1)\n",
        "        if patch!=None:\n",
        "            cond = torch.cat([x, patch],1)\n",
        "            mask = torch.cat([mask, torch.ones((1,t,1), dtype=bool, device=device)], dim=-1)\n",
        "        else: cond = x\n",
        "        # print('lyr gen1', x.shape, cond.shape, mask.shape)\n",
        "        out = self.forward(x, cond=cond, mask=mask)\n",
        "        # print('lyr gen1', x.shape, out.shape)\n",
        "\n",
        "        qout = self.vqx(out)\n",
        "        etp = (out-qout).mean(-1) # [b,1] float\n",
        "        # x = torch.cat([x, qout], dim=1)\n",
        "        # x = torch.cat([x, qout[:,:-1]], dim=1)\n",
        "        # return x, etp\n",
        "        return qout, etp\n",
        "# generate one tok unless uncertain ? then patch up?\n",
        "# gen one only bec blt need to tok\n",
        "\n",
        "\n",
        "\n",
        "# trans: btd->btd, train: shift1\n",
        "# bltlyr: btd->btd+bpd , gentrain: shift1, artrain:btd+bpd->btd\n",
        "\n",
        "# f_ j ; vs f_ t_\n",
        "# full te\n",
        "# full text\n",
        "\n",
        "# is gpt + aetrain?\n",
        "\n",
        "\n",
        "\n",
        "batch=25\n",
        "d_model = 16\n",
        "in_dim=128\n",
        "# x = torch.randint(0, in_dim, (2, 7), device=device)\n",
        "x = torch.rand((batch, 7, d_model), device=device)\n",
        "\n",
        "model = BLTlayer(in_dim, d_model).to(device)\n",
        "# model = BLTlayer(d_model).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # 1e-4 #3e-4\n",
        "\n",
        "# in: qunatseq\n",
        "# fwd: qunatx -> y_\n",
        "# to_patch: x, y_ -> patches, mask\n",
        "# gen: x, (patch) ->\n",
        "\n",
        "\n",
        "# out, patches = model(x)\n",
        "out = model(x)\n",
        "print('out', out.shape)\n",
        "# gloss = model.gen_loss(out)\n",
        "\n",
        "patches, mask, patch_mask = model.to_patch(x, out)\n",
        "print('patches, mask',patches.shape, mask.shape)\n",
        "gloss = model.gen_loss(x)\n",
        "aloss = model.ae_loss(x, patches, mask, patch_mask)\n",
        "# loss = gloss + aloss\n",
        "\n",
        "# patch = torch.rand(batch,1,d_model)\n",
        "patch = torch.rand(1,1,d_model)\n",
        "# out, mask = model.generate(x)\n",
        "# out, mask = model.generate(x, patch)\n",
        "x = torch.rand((1,7, d_model), device=device)\n",
        "out, mask = model.generate_one(x, patch)\n",
        "print(out.shape)\n",
        "# out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "# print(patches.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "31h84axSSwyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLT me fsq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BLT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.pos_enc = RoPE(d_model, base=10000)\n",
        "        self.tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        # self.layer = nn.ModuleList([BLTlayer(d_model, out_dim) for _ in range(num_layers)])\n",
        "        self.layer = nn.ModuleList([BLTlayer(d_model, d_model, quantxp=(6*2**i,12*2**i)) for i in range(num_layers)])\n",
        "        # self.layer = nn.ModuleList([BLTgpt(d_model, d_model, quantxp=(6,12))])\n",
        "        # self.layer.append(BLTlayer(d_model, d_model, quantxp=(6*2**i,12*2**i)) for i in range(num_layers-1))\n",
        "        self.out = nn.Linear(d_model, in_dim)\n",
        "\n",
        "    # def forward(self, x): # [b,t]\n",
        "    #     x = self.tok_emb(x)\n",
        "    #     for level in self.layer:\n",
        "    #         # out, x = level(x) # [b,t,d]\n",
        "    #         out = level(x) # [b,t,d]\n",
        "    #         patches, mask, patch_mask = level.to_patch(x, out)\n",
        "    #         # outs.append(out)\n",
        "    #         x = level.to_patch(out)\n",
        "    #     x = self.lin(x)\n",
        "    #     return x # [batch, num_patch, out_dim]\n",
        "\n",
        "    def loss(self, src): # [b,t]\n",
        "        x = self.tok_emb(src) # [b,t,d]\n",
        "        tt_loss = 0\n",
        "        for i, level in enumerate(self.layer):\n",
        "            # out = level(x, mask=True) # [b,t,d]\n",
        "            b,t = x.shape[:2]\n",
        "            mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # F->mask # for F.scaled_dot_product_attention\n",
        "            # out = level.perceiver(x, cond=x, mask='causal') # [b,t,d], [nlyr,b,d]\n",
        "            out = level.perceiver(x, cond=x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "            out = level.lin(out)\n",
        "            # tt_loss += level.gen_loss(x)\n",
        "            if i==0: tt_loss += F.cross_entropy(self.out(out[:,:-1]).flatten(0,1), src[:,1:].flatten()) # [b*t,d], [b*t]\n",
        "            else: tt_loss += F.mse_loss(out[:,:-1], x[:,1:])\n",
        "\n",
        "            if i != len(self.layer)-1: # last layer no patching\n",
        "                patches, mask, patch_mask = level.to_patch(x, out)\n",
        "                tt_loss += level.ae_loss(x, patches, mask, patch_mask)\n",
        "                x = patches\n",
        "        return tt_loss\n",
        "\n",
        "\n",
        "\n",
        "#     def generate(self, x):\n",
        "#         patches = []\n",
        "#         for j, patch in enumerate(patches):\n",
        "#             if len(patch)>0:\n",
        "#                 break\n",
        "#         # patch in patches\n",
        "#         for i in reversed(range(j)):\n",
        "#             patch = patches[i].pop(0)\n",
        "#             layer = self.layers[i]\n",
        "#             # = self.perceiver(x, cond=patch)\n",
        "#             out = layer.generate(x if i==0 else None, patch)\n",
        "#             if i!=0: patches[i-1].append(out[:,-1])\n",
        "# # out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "#         # entropies = entropy(out) # [b,t] float\n",
        "#         # mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "#         return out[:,-1]\n",
        "\n",
        "    def generate_all(self, x): # [b,t]\n",
        "        x = self.tok_emb(x) # [b,t,d]\n",
        "        # src_lst = [x]\n",
        "        # src_lst = [len(self.layer)*[]]\n",
        "        # src_lst = [len(self.layer)*None]\n",
        "        src_lst = len(self.layer)*[None]\n",
        "        for i, level in enumerate(self.layer):\n",
        "            # out, x = level(x) # [b,t,d]\n",
        "            # out = level(x) # [b,t,d]\n",
        "\n",
        "            out, _ = level.generate(x)\n",
        "            # src_lst[i].append(out)\n",
        "            src_lst[i] = out\n",
        "\n",
        "            # outs.append(out)\n",
        "            if i != len(self.layer)-1:\n",
        "                patches, mask, patch_mask = level.to_patch(x, out)\n",
        "                # src_lst[i+1].append(patches)\n",
        "                src_lst[i+1] = patches\n",
        "\n",
        "                # x = level.to_patch(out)\n",
        "                # src_lst.append(x)\n",
        "        # if self.lin: x = self.lin(x)\n",
        "        # return x # [batch, num_patch, out_dim]\n",
        "        patch = None\n",
        "        # for i in reversed(range(len(self.layers))):\n",
        "        for i, level in enumerate(reversed(self.layer)):\n",
        "            # layer = self.layers[i]\n",
        "            # x = src_lst.pop()\n",
        "            x = src_lst[i-1]\n",
        "            out, mask_ = level.generate(x, patch)\n",
        "            # out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask_],dim=1))\n",
        "            # = self.perceiver(x, cond=patch)\n",
        "            patch = x\n",
        "            # out = layer.generate(x if i==0 else None, patch)\n",
        "            # if i!=0: patches[i-1].append(out[:,-1])\n",
        "\n",
        "        # entropies = entropy(out) # [b,t] float\n",
        "        # mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "\n",
        "\n",
        "        output = output[:, -1] # get logit for last character\n",
        "        output = output/temperature\n",
        "        output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_data.itos[int(i)] for i in x.flatten()])\n",
        "\n",
        "\n",
        "        return out[:,-1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate_one(self, x, src_lst=None, etp_lst=None): # [b,t]\n",
        "        x = self.tok_emb(x) # [b,t,d]\n",
        "        b,t,d = x.shape\n",
        "        src_lst = src_lst or [] #len(self.layer)*[None]\n",
        "        etp_lst = etp_lst or [torch.empty((b,0), dtype=torch.float, device=device)]*len(self.layer)\n",
        "\n",
        "        # for i, level in enumerate(self.layer):\n",
        "        for i, (level, entropies) in enumerate(zip(self.layer, etp_lst)):\n",
        "            out, entropies = level.generate_one(x)\n",
        "            # print('gen one11', x.shape, out.shape)\n",
        "\n",
        "            # mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1)\n",
        "            # out = level(x, mask=mask)\n",
        "            # entropies = (out-level.vqx(out)).mean(-1) # [b,t] float\n",
        "            mask = patchify(entropies, 1.) # [1,t] # T at start of each patch, F otherwise\n",
        "\n",
        "            # entropies = torch.cat([entropies, etp], dim=1) # [b,t]\n",
        "            # etp_lst[i] = torch.cat([etp_lst[i], etp], dim=1) # [b,t]\n",
        "            # mask = patchify(etp_lst[i], 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "            # print('gen one', mask.shape)\n",
        "            # if mask[:,-1]: # newly generated tok is uncertain\n",
        "            # print('gen one', x.shape, out.shape)\n",
        "            patches, _, _ = level.to_patch(x, out)\n",
        "            x = patches\n",
        "            # else: break\n",
        "            src_lst.append(out)\n",
        "\n",
        "        if i==0: return x, src_lst, etp_lst # 1st lvl is confident [1, t+1, d]\n",
        "        patch = None\n",
        "        # for i in reversed(range(len(self.layers))):\n",
        "        for i, v in enumerate(reversed(src_lst)):\n",
        "            patch, etp = self.layer[len(src_lst)-i].generate_one(v, patch)\n",
        "            # patch, etp = self.layer[len(src_lst)-i].generate_one(x, patch)\n",
        "\n",
        "        patch = self.out(patch)\n",
        "        return patch, src_lst, etp_lst\n",
        "\n",
        "\n",
        "    def generate_all(self, context, max_steps=64, temperature=1): # [b,t]\n",
        "        training = self.training\n",
        "        self.eval()\n",
        "        x = torch.tensor([train_data.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "        for n in range(max_steps):\n",
        "            with torch.no_grad():\n",
        "                out, _, _ = self.generate_one(x)\n",
        "            out = out[:,-1] # get logit for last character\n",
        "            out = out/temperature\n",
        "            out = F.softmax(out, dim=-1) # vocab_size to char\n",
        "            ix = torch.multinomial(out, num_samples=1) # rand sample by output distribution\n",
        "            print(x.shape, ix.shape)\n",
        "            x = torch.cat((x, ix.unsqueeze(1)),1)\n",
        "        completion = ''.join([train_data.itos[int(i)] for i in x.flatten()])\n",
        "        if training: self.train()\n",
        "        return completion\n",
        "\n",
        "\n",
        "# pos_all, entropy mask;\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "# gen train: mse(x[1:],out[:-1])\n",
        "# gen inference: x cond=x, cond=(x+patch(+*learnt?)), if entropy/bos, pop patch and regen\n",
        "\n",
        "# # fsq: dont realize tokens in layer\n",
        "# layer:\n",
        "# posemb\n",
        "# x less quants patch more quants\n",
        "# quant after linout\n",
        "# # quant before linout\n",
        "\n",
        "# blt:\n",
        "# tokemb\n",
        "# linout to tok\n",
        "\n",
        "\n",
        "# gen:\n",
        "# gen lower only, until uncertain, then patch, gen top then patch cond bottom\n",
        "# gen 1 tok. if uncertain, to_patch then top gen\n",
        "\n",
        "vocab_size=50\n",
        "in_dim, d_model, out_dim, num_layers = 4,16,vocab_size,1\n",
        "# x = torch.rand(2,5,in_dim)\n",
        "x = torch.rand(1,5,d_model)\n",
        "model = BLT(vocab_size, d_model, out_dim, num_layers)\n",
        "# model = BLTlayer(d_model, d_model)\n",
        "# out = model(x)\n",
        "# out = model.loss(x)\n",
        "# print(out)\n",
        "# out = model.generate(x)\n",
        "x = torch.randint(0,vocab_size, (2,500))\n",
        "out, src_lst, etp_lst = model.generate_one(x)\n",
        "# out = model.generate_all(x)\n",
        "print(out.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ecf836-647d-4ddc-f20c-28b99f786e79",
        "cellView": "form",
        "id": "nX9w4z23J8DZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 260, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLTlayer transformer gpt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# transformer\n",
        "class BLTgpt(BLTlayer):\n",
        "    # def __init__(self, in_dim, d_model=None, out_dim=None, num_layers=1):\n",
        "    def __init__(self, in_dim, d_model=None, quantxp=(6,12), num_layers=1):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_dim\n",
        "        self.tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        self.pos_enc = RoPE(d_model, base=100) # 10000\n",
        "        # self.perceiver = Seq(*[AttentionBlock(d_model, n_heads=4, cond_dim=d_model) for _ in range(num_layers)])\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.out = nn.Linear(d_model, in_dim)\n",
        "\n",
        "    # def forward(self, x, cond=None, mask=None): # [b,t,d], [b,t,t]\n",
        "    #     x = self.pos_enc(x)\n",
        "    #     b,t,d = x.shape\n",
        "    #     # if mask==None: mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # F->mask # for F.scaled_dot_product_attention\n",
        "    #     # print('BLTlayer',x.shape, causal_mask.shape)\n",
        "    #     x = self.perceiver(x, cond=cond if cond!=None else x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "    #     out = self.lin(x)\n",
        "    #     return out # [b,t,d_model]\n",
        "\n",
        "    def gen_loss(self, src): # [b,t]\n",
        "        x, y = src[:,:-1], src[:,1:]\n",
        "        # x = self.pos_enc(x)\n",
        "        x = self.pos_enc(self.tok_emb(x))\n",
        "        b,t,d = x.shape\n",
        "        mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1)\n",
        "        # print('gpt gen_loss', out.shape, y.shape)\n",
        "        x = self.perceiver(x, cond=x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        out = self.out(x)\n",
        "        loss = F.cross_entropy(out.flatten(0,1), y.flatten()) # [b*t,d], [b*t]\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "    # def to_patch(self, x, out): # [b,t,d]\n",
        "    #     entropies = (x-self.vqx(x)).mean(-1) # [b,t] float\n",
        "    #     mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "    #     patch_mask = padded_block_mask(mask) # [batch ,num_patch, seq_len]\n",
        "    #     # print('to_patch', x.shape,mask.shape)\n",
        "\n",
        "    #     latent = (x.unsqueeze(1) *patch_mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "    #     # latent = self.latent.repeat(1,mask.shape[1],1) # [b ,num_patch, d]\n",
        "    #     latent = latent + self.patch_enc\n",
        "    #     out = self.conv_emb(x.transpose(-2,-1)).transpose(-2,-1)\n",
        "    #     patches = self.forward(latent, cond=out, mask=patch_mask) # [batch, num_patch, latent_dim/d_model]\n",
        "    #     # print('BLTlayer',out.shape)\n",
        "    #     patches = self.vqp(patches)\n",
        "    #     return patches, mask, patch_mask # [b, num_patch, out_dim], [b ,num_patch, t]\n",
        "\n",
        "    # def ae_loss(self, y, patches, mask, patch_mask): # [b,t,d], [b, n_patch, d], [b,t], [b, n_patch, t]\n",
        "    #     # print('ae_loss', x.shape, self.tok_emb(x).shape) # [2, 7, 16], [2, 5, 128], [2, 7, 5]\n",
        "    #     x = self.pos_enc(y)\n",
        "    #     b,t,d = x.shape\n",
        "    #     # cond = patch_mask.transpose(-2,-1).to(patches.dtype) @ patches # [b, t, out_dim]\n",
        "    #     patch_mask = patch_mask.transpose(-2,-1) # [b, t, n_patch]\n",
        "    #     causal_mask = block_diag(mask) & torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # [b,t,t] # only attend within the patch & is causal\n",
        "    #     # print('ae_loss', x.shape, patches.shape, patch_mask.shape) # [2, 7, 16], [2, 3, 16], [2, 7, 3]\n",
        "    #     y_ = self.forward(x, cond=torch.cat([x, patches], dim=1), mask=torch.cat([causal_mask, patch_mask], dim=-1)) # cond: [b, t+n_patch, d]; mask: [batch, len_q, len_v] [b, t, t+n_patch]\n",
        "    #     loss = F.mse_loss(y[mask], y_[mask])\n",
        "    #     return loss\n",
        "\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "\n",
        "    def generate(self, x=None, patch=None): # [b,t,d], [b,t,d]\n",
        "    # def generate(self, x=None, patch=None, mask=None, patch_mask=None): # [b,t], [b,t,d], [b,t], [b,n_patch,t]\n",
        "        b,t,d = x.shape\n",
        "        # entropies = torch.empty((b,0), device=device)\n",
        "        # while True:\n",
        "        for i in range(50):\n",
        "            if patch!=None: cond = torch.cat([x[:,:-1], patch],1)\n",
        "            else: cond = x[:,:-1]\n",
        "            x = self.pos_enc(self.tok_emb(x))\n",
        "            out = self.perceiver(x[:,-1:], cond=cond)\n",
        "            out = self.out(out)\n",
        "\n",
        "            # entropy = (out-qout).mean(-1) # [b,1] float\n",
        "            entropi = entropy(out) # [b,1] float\n",
        "\n",
        "            temperature = 1.\n",
        "            out = out/temperature\n",
        "            out = F.softmax(out, dim=-1) # vocab_size to char\n",
        "            ix = torch.multinomial(out, num_samples=1) # rand sample by output distribution\n",
        "\n",
        "            x = torch.cat([x, ix], dim=1)\n",
        "            entropies = torch.cat([entropies, entropi], dim=1) # [b,t]\n",
        "            # if i==0: continue # want gen at least one\n",
        "            mask = patchify(entropies, 1.)\n",
        "            if (mask.sum(1)>=2).all(): break\n",
        "\n",
        "        # print('BLTlayer gen',mask.shape)\n",
        "        mask = mask.cumsum(dim=1)>=2 # [b,t]\n",
        "        out = x[:,t:]*mask.unsqueeze(-1)\n",
        "        print('BLTlayer gen',x.shape, out.shape, mask.shape)\n",
        "\n",
        "        # return x[1:-1]\n",
        "        # return torch.cat((x, out),1), mask\n",
        "        return torch.cat((x[:,:t], out),1), torch.cat([torch.ones(b,t, dtype=bool, device=device), mask], dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch=25\n",
        "d_model = 16\n",
        "in_dim=128\n",
        "x = torch.randint(0, in_dim, (2, 7), device=device)\n",
        "# x = torch.rand((batch, 7, d_model), device=device)\n",
        "\n",
        "model = BLTlayer(in_dim, d_model).to(device)\n",
        "# model = BLTlayer(d_model).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # 1e-4 #3e-4\n",
        "\n",
        "# in: qunatseq\n",
        "# fwd: qunatx -> y_\n",
        "# to_patch: x, y_ -> patches, mask\n",
        "# gen: x, (patch) ->\n",
        "\n",
        "\n",
        "# BLTgpt\n",
        "\n",
        "# out, patches = model(x)\n",
        "out = model(x)\n",
        "print('out', out.shape)\n",
        "# gloss = model.gen_loss(out)\n",
        "\n",
        "patches, mask, patch_mask = model.to_patch(x, out)\n",
        "print('patches, mask',patches.shape, mask.shape)\n",
        "gloss = model.gen_loss(x)\n",
        "aloss = model.ae_loss(x, patches, mask, patch_mask)\n",
        "# loss = gloss + aloss\n",
        "\n",
        "patch = torch.rand(batch,1,d_model)\n",
        "# out, mask = model.generate(x)\n",
        "out, mask = model.generate(x, patch)\n",
        "print(out.shape)\n",
        "# out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "# print(patches.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-ekc9eL1PNb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLTlayer transformer tok\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# transformer\n",
        "class BLTlayer(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=None, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_dim\n",
        "        self.tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        self.pos_enc = RoPE(d_model, base=100) # 10000\n",
        "        # self.latent = nn.Parameter(torch.randn(1,1,d_model)*.02) # init\n",
        "        # self.perceiver = AttentionBlock(d_model, n_heads=4, cond_dim=d_model)\n",
        "        self.perceiver = Seq(*[AttentionBlock(d_model, n_heads=4, cond_dim=d_model) for _ in range(num_layers)])\n",
        "        self.lin = nn.Linear(d_model, in_dim)\n",
        "        # self.out = nn.Linear(d_model, out_dim)\n",
        "        self.patch_enc = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.conv_emb = nn.Conv1d(d_model, d_model, 1)\n",
        "\n",
        "    # def forward(self, x, mask=None): # [b,t], [b,t,t]\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t], [b,t,t]\n",
        "        x = self.pos_enc(self.tok_emb(x))\n",
        "        b,t,d = x.shape\n",
        "        # if mask==None:\n",
        "        #     mask = torch.ones((t,t), dtype=bool, device=device).tril().repeat(b,1,1) # for F.scaled_dot_product_attention\n",
        "        #     # mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # F->mask\n",
        "        # print('BLTlayer',x.shape, causal_mask.shape)\n",
        "        x = self.perceiver(x, cond=cond or x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        out = self.lin(x)\n",
        "        return out # [b,t,d_model]\n",
        "\n",
        "    def gen_loss(self, src): # [b,t,d]\n",
        "        x, y = src[:,:-1], src[:,1:]\n",
        "        b,t = x.shape\n",
        "        mask = torch.tril(torch.ones((t,t), device=device)).bool().unsqueeze(0).repeat(b,1,1)\n",
        "        out = self.forward(x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        # print('gen_loss', out.shape, y.shape)\n",
        "        loss = F.cross_entropy(out.flatten(0,1), y.flatten()) # [b*t,d], [b*t]\n",
        "        return loss\n",
        "\n",
        "    def to_patch(self, x, out): # [b,t]\n",
        "        entropies = entropy(out) # [b,t] float\n",
        "        mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "        patch_mask = padded_block_mask(mask) # [batch ,num_patch, seq_len]\n",
        "        # print('to_patch', x.shape,mask.shape)\n",
        "        x = self.tok_emb(x)\n",
        "\n",
        "        latent = (x.unsqueeze(1) *patch_mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "        # latent = self.latent.repeat(1,mask.shape[1],1) # [b ,num_patch, d]\n",
        "        latent = latent + self.patch_enc\n",
        "        out = self.conv_emb(x.transpose(-2,-1)).transpose(-2,-1)\n",
        "        patches = self.perceiver(latent, cond=out, mask=patch_mask) # [batch, num_patch, latent_dim/d_model]\n",
        "        # patches = self.lin(patches)\n",
        "        # patches = self.out(patches)\n",
        "        # patches = self.forward(latent, cond=out, mask=mask) # [batch, num_patch, latent_dim/d_model]\n",
        "        # print('BLTlayer',out.shape)\n",
        "        # if self.lin: patches = self.lin(patches)\n",
        "        return patches, mask, patch_mask # [b, num_patch, out_dim], [b ,num_patch, t]\n",
        "\n",
        "    def ae_loss(self, y, patches, mask, patch_mask): # [b,t,d], [b, n_patch, d], [b, n_patch, t]\n",
        "        # print('ae_loss', x.shape, self.tok_emb(x).shape) # [2, 7, 16], [2, 5, 128], [2, 7, 5]\n",
        "        x = self.pos_enc(self.tok_emb(y))\n",
        "        b,t,d = x.shape\n",
        "        # cond = patch_mask.transpose(-2,-1).to(patches.dtype) @ patches # [b, t, out_dim]\n",
        "        patch_mask = patch_mask.transpose(-2,-1) # [b, t, n_patch]\n",
        "\n",
        "        causal_mask = block_diag(mask) & torch.tril(torch.ones((t,t), device=device)).bool().unsqueeze(0).repeat(b,1,1) # [b,t,t]\n",
        "\n",
        "        # x_ = self.perceiver(x, cond=cond, mask=mask)\n",
        "        # print('ae_loss', x.shape, patches.shape, patch_mask.shape) # [2, 7, 16], [2, 3, 16], [2, 7, 3]\n",
        "        out = self.perceiver(x, cond=torch.cat([x, patches], dim=1), mask=torch.cat([causal_mask, patch_mask], dim=-1)) # cond: [b, t+n_patch, d]; mask: [batch, len_q, len_v] [b, t, t+n_patch]\n",
        "        out = self.lin(out)\n",
        "        # x_ = self.out(x_)\n",
        "        # x_ = self.forward(x, cond=cond, mask=mask)\n",
        "        # loss = F.mse_loss(x, x_)\n",
        "        print('ae_loss', x.shape, out.shape, mask.shape)\n",
        "        # loss = F.mse_loss(y[mask], x_[mask])\n",
        "        loss = F.cross_entropy(out.flatten(0,1), y.flatten()) # [b*t,d], [b*t]\n",
        "        return out\n",
        "\n",
        "    # def loss(self, x, out): # [b,t,d]\n",
        "    #     out = model(x)\n",
        "    #     gloss = model.gen_loss(out)\n",
        "    #     patches, mask = model.to_patch(x, out)\n",
        "    #     aloss = model.ae_loss(x, patches, mask)\n",
        "    #     # loss = gloss + aloss\n",
        "\n",
        "# pos_all, entropy mask;\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "# gen train: mse(x[1:],out[:-1])\n",
        "# gen inference: x cond=x, cond=(x+patch(+*learnt?)), if entropy/bos, pop patch and regen\n",
        "\n",
        "\n",
        "    def generate(self, x=None, patch=None): # [b,t], [b,t,d]\n",
        "    def generate(self, x=None, patch=None, mask=None, patch_mask=None): # [b,t], [b,t,d], [b,t], [b,n_patch,t]\n",
        "        # if patch==None: patch=x\n",
        "        x = self.pos_enc(self.tok_emb(x))\n",
        "        b,t,d = x.shape\n",
        "        out = torch.empty((b,0,d), device=device)\n",
        "        while True:\n",
        "        # for i in range(5):\n",
        "            print('generate x',x.shape, x[:,-1:].shape)\n",
        "            # src = self.pos_enc(torch.cat((x, out),1))\n",
        "            if patch!=None: cond = torch.cat((x[:,:-1], patch),1)\n",
        "            else: cond = x[:,:-1]\n",
        "\n",
        "            # # output = self.perceiver(src, cond=patch)\n",
        "            # # output = self.perceiver(src, cond=torch.cat((src, patch),1))\n",
        "            # output = self.perceiver(src, cond=torch.cat((src[:,-1:], patch),1))\n",
        "            # output = self.forward(x[:,-1:], cond=torch.cat((x, patch),1))\n",
        "            output = self.forward(x[:,-1:], cond=cond)\n",
        "            # # print('generate x',x.shape)\n",
        "            # # print('generate output',output)\n",
        "\n",
        "            # out = torch.cat([out, output[:,-1:]], dim=1)\n",
        "            out = torch.cat([out, output[:,:0]], dim=1)\n",
        "            # out = torch.cat([out, output], dim=1)\n",
        "            entropies = entropy(out) # [b,t] float\n",
        "\n",
        "            # print('entropies',entropies)\n",
        "            mask = patchify(entropies, 1.)\n",
        "            # print('generate',out.shape, mask.shape)\n",
        "            # print(mask)\n",
        "            # print((mask.sum(1)>=2))\n",
        "            if (mask.sum(1)>=2).all():\n",
        "            # if (mask.sum(1)>=2 or (EOS_IDX == mask).any(1)).all():\n",
        "                break\n",
        "\n",
        "            temperature=1.\n",
        "            # output = output[:, -1] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "\n",
        "\n",
        "        # print('BLTlayer gen',mask.shape)\n",
        "        print(mask)\n",
        "        # mask = padded_block_mask(mask)[:,1:]\n",
        "        mask = mask.cumsum(dim=1)>=2 # [b,t]\n",
        "        print(mask)\n",
        "        out.unsqueeze(1) *mask.unsqueeze(-1)\n",
        "\n",
        "        # return x[1:-1]\n",
        "        # return torch.cat((x, out),1), mask\n",
        "        return torch.cat((x, out),1), mask\n",
        "\n",
        "\n",
        "\n",
        "# tok in/out\n",
        "# typical causal generation\n",
        "# stop gen when trans entroy high\n",
        "# typical autoregressive\n",
        "\n",
        "# ae train\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def generate(model, context, max_steps=64, temperature=1):\n",
        "#     x = torch.tensor([train_data.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "#     model.eval()\n",
        "#     for n in range(max_steps):\n",
        "#         with torch.no_grad():\n",
        "#             output = model(x)\n",
        "#         # print('generate', output.shape, hidden.shape)\n",
        "#         output = output[:, -1] # get logit for last character\n",
        "#         output = output/temperature\n",
        "#         output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "#         ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "#         x = torch.cat((x, ix),1)\n",
        "#     completion = ''.join([train_data.itos[int(i)] for i in x.flatten()])\n",
        "#     return completion\n",
        "\n",
        "d_model = 16\n",
        "in_dim=128\n",
        "x = torch.randint(0, in_dim, (2, 7), device=device)\n",
        "\n",
        "model = BLTlayer(in_dim, d_model).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # 1e-4 #3e-4\n",
        "\n",
        "# # out, patches = model(x)\n",
        "# out = model(x)\n",
        "# print('out', out.shape)\n",
        "# # gloss = model.gen_loss(out)\n",
        "# gloss = model.gen_loss(x)\n",
        "\n",
        "# # patch = torch.rand(2,1,d_model)\n",
        "# patches, mask, patch_mask = model.to_patch(x, out)\n",
        "# print('patches, mask',patches.shape, mask.shape)\n",
        "# aloss = model.ae_loss(x, patches, mask, patch_mask)\n",
        "# # loss = gloss + aloss\n",
        "\n",
        "out, mask = model.generate(x)\n",
        "# out, mask = model.generate(x, patch)\n",
        "# print(out.shape)\n",
        "# out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "# print(patches.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3HpsXGalYjcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLT me tok\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BLT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        # self.layer = nn.ModuleList([BLTlayer(d_model, out_dim) for _ in range(num_layers)])\n",
        "        self.layer = nn.ModuleList([BLTlayer(d_model, d_model) for _ in range(num_layers)])\n",
        "        # self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x): # [b,t]\n",
        "        for level in self.layer:\n",
        "            # out, x = level(x) # [b,t,d]\n",
        "            out = level(x) # [b,t,d]\n",
        "            # outs.append(out)\n",
        "            x = level.to_patch(out)\n",
        "        # if self.lin: x = self.lin(x)\n",
        "        return x # [batch, num_patch, out_dim]\n",
        "\n",
        "\n",
        "#     def generate(self, x):\n",
        "#         patches = []\n",
        "#         for j, patch in enumerate(patches):\n",
        "#             if len(patch)>0:\n",
        "#                 break\n",
        "#         # patch in patches\n",
        "#         for i in reversed(range(j)):\n",
        "#             patch = patches[i].pop(0)\n",
        "#             layer = self.layers[i]\n",
        "#             # = self.perceiver(x, cond=patch)\n",
        "#             out = layer.generate(x if i==0 else None, patch)\n",
        "#             if i!=0: patches[i-1].append(out[:,-1])\n",
        "# # out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "#         # entropies = entropy(out) # [b,t] float\n",
        "#         # mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "#         return out[:,-1]\n",
        "\n",
        "    def generate(self, x):\n",
        "        # batch, seq_len = x.shape\n",
        "        # x = self.pos_enc(self.tok_emb(x)) # [batch_size, seq_len, d_model]\n",
        "        src_lst = [x]\n",
        "        msk_lst = []\n",
        "        for level in self.layer:\n",
        "            # out, x = level(x) # [b,t,d]\n",
        "            out = level(x) # [b,t,d]\n",
        "            # outs.append(out)\n",
        "            if i != len(self.levels)-1\n",
        "                x, mask = model.to_patch(x, out)\n",
        "                # x = level.to_patch(out)\n",
        "                src_lst.append(x)\n",
        "        # if self.lin: x = self.lin(x)\n",
        "        # return x # [batch, num_patch, out_dim]\n",
        "        patch = None\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            layer = self.layers[i]\n",
        "            x = src_lst.pop()\n",
        "            mask = msk_lst.pop()\n",
        "            out, mask_ = model.generate(x, patch)\n",
        "            out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask_],dim=1))\n",
        "            # = self.perceiver(x, cond=patch)\n",
        "            patch = x\n",
        "            # out = layer.generate(x if i==0 else None, patch)\n",
        "            if i!=0: patches[i-1].append(out[:,-1])\n",
        "\n",
        "        # entropies = entropy(out) # [b,t] float\n",
        "        # mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "        return out[:,-1]\n",
        "\n",
        "# out, patches = model(x)\n",
        "out = model(x)\n",
        "print('out', out.shape)\n",
        "patches, mask, patch_mask = model.to_patch(x, out)\n",
        "print('patches, mask',patches.shape, mask.shape)\n",
        "gloss = model.gen_loss(x)\n",
        "aloss = model.ae_loss(x, patches, mask, patch_mask)\n",
        "# loss = gloss + aloss\n",
        "\n",
        "\n",
        "    def loss(self, x):\n",
        "        # batch, seq_len = x.shape\n",
        "        x = self.tok_emb(x) #[batch_size, seq_len, d_model]\n",
        "        tt_loss = 0\n",
        "        for level in self.layer:\n",
        "            # out, x = level(x) # [b,t,d]\n",
        "            # out = level(x) # [b,t,d]\n",
        "            out = level(x) # [b,t,d]\n",
        "            # outs.append(out)\n",
        "            # loss = F.mse_loss(x[:,1:], out[:,:-1])\n",
        "            # tt_loss += loss\n",
        "            # x, mask = level.to_patch(out)\n",
        "\n",
        "\n",
        "            tt_loss += model.gen_loss(out)\n",
        "            if i != len(self.levels)-1\n",
        "                patches, mask = model.to_patch(x, out)\n",
        "                tt_loss += model.ae_loss(x, patches, mask)\n",
        "\n",
        "            # # output = self.perceiver(src, cond=torch.cat((src, patch),1))\n",
        "            # output = self.perceiver(pos(torch.cumsum()), cond=patch, mask=mask)\n",
        "\n",
        "        # if self.lin: x = self.lin(x)\n",
        "        # return x # [batch, num_patch, out_dim]\n",
        "        return tt_loss#, out\n",
        "\n",
        "\n",
        "# pos_all, entropy mask;\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "# gen train: mse(x[1:],out[:-1])\n",
        "# gen inference: x cond=x, cond=(x+patch(+*learnt?)), if entropy/bos, pop patch and regen\n",
        "\n",
        "\n",
        "vocab_size=50\n",
        "in_dim, d_model, out_dim, num_layers = 4,16,vocab_size,1\n",
        "# x = torch.rand(2,5,in_dim)\n",
        "x = torch.randint(0,vocab_size, (2,500))\n",
        "model = BLT(d_model, out_dim, num_layers)\n",
        "# model = BLTlayer(d_model, d_model)\n",
        "# out = model(x)\n",
        "# out = model.loss(x)\n",
        "out = model.generate(x)\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "upJBm6hSYbw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLTlayer transformer old\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# transformer\n",
        "class BLTlayer(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model=None, out_dim=None, num_layers=1):\n",
        "    def __init__(self, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        # d_model = d_model or in_dim\n",
        "        self.pos_enc = RoPE(d_model, base=100) # 10000\n",
        "        # self.latent = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.perceiver = AttentionBlock(d_model, n_heads=4, cond_dim=d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        self.bos = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.eos = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.patch_enc = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.conv_emb = nn.Conv1d(d_model, d_model, 1)\n",
        "\n",
        "    # def forward(self, x): # [b,t,d]\n",
        "    def forward(self, x, mask=None): # [b,t,d]\n",
        "        x = self.pos_enc(x)\n",
        "\n",
        "        b,t,d = x.shape\n",
        "        mask = torch.ones((t,t), dtype=bool, device=device).tril().repeat(b,1,1) # for F.scaled_dot_product_attention\n",
        "        mask = torch.tril(torch.ones((t,t), dtype=bool, device=device)).repeat(b,1,1) # F->mask\n",
        "        # with torch.no_grad():\n",
        "        # print('BLTlayer',x.shape, causal_mask.shape)\n",
        "        out = self.perceiver(x, cond=x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "\n",
        "        return out # [b,t,d_model]\n",
        "\n",
        "    def gen_loss(self, src): # [b,t,d]\n",
        "        x, y = src[:,:-1], src[:,1:]\n",
        "        b,t,d = x.shape\n",
        "        mask = torch.tril(torch.ones((t,t), device=device)).bool().unsqueeze(0).repeat(b,1,1)\n",
        "        out = self.perceiver(x, cond=x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        loss = F.mse_loss(out, y)\n",
        "        return loss\n",
        "\n",
        "    def to_patch(self, x, out): # [b,t,d]\n",
        "        entropies = entropy(out) # [b,t] float\n",
        "        # entropies = out @ self.eos.transpose(-2,-1)\n",
        "        mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "        mask = padded_block_mask(mask) # [batch ,num_patch, seq_len]\n",
        "        # latent = (out.unsqueeze(1) *mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "        latent = (x.unsqueeze(1) *mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "        latent = latent + self.patch_enc\n",
        "        # latent = self.latent.repeat(1,mask.shape[1],1) # [b ,num_patch, d]\n",
        "        out = self.conv_emb(x.transpose(-2,-1)).transpose(-2,-1)\n",
        "        patches = self.perceiver(latent, cond=out, mask=mask) # [batch, num_patch, latent_dim/d_model]\n",
        "        # print('BLTlayer',out.shape)\n",
        "\n",
        "        if self.lin: patches = self.lin(patches)\n",
        "        return patches, mask # [batch, num_patch, out_dim], [batch ,num_patch, seq_len]\n",
        "\n",
        "# pos_all, entropy mask;\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "# gen train: mse(x[1:],out[:-1])\n",
        "# gen inference: x cond=x, cond=(x+patch(+*learnt?)), if entropy/bos, pop patch and regen\n",
        "\n",
        "    def ae_loss(self, x, patches, mask): # [b,t,d], [b, n_patch, latent_dim/d_model], [b, n_patch, t]\n",
        "        b,t,d = x.shape\n",
        "        cond = mask.transpose(-2,-1).to(patches.dtype) @ patches # [b, t, latent_dim/d_model]\n",
        "        mask = torch.tril(torch.ones((t,t), device=device)).bool().unsqueeze(0).repeat(b,1,1)\n",
        "        x_ = self.perceiver(x, cond=cond, mask=mask)\n",
        "        loss = F.mse_loss(x, x_)\n",
        "        return out\n",
        "\n",
        "    def generate(self, x=None, patch=None):\n",
        "        # x = x or self.bos.repeat(patch.shape[0],1,1)\n",
        "        if patch==None: patch=x\n",
        "        # x=patch=self.bos\n",
        "        b,t,d = x.shape\n",
        "        out = torch.empty((b,0,d), device=device)\n",
        "        while True:\n",
        "        # for i in range(5):\n",
        "            # print('generate x',x.shape)\n",
        "            src = self.pos_enc(torch.cat((x, out),1))\n",
        "\n",
        "            # output = self.perceiver(src, cond=patch)\n",
        "            # output = self.perceiver(src, cond=torch.cat((src, patch),1))\n",
        "            output = self.perceiver(src, cond=torch.cat((src[:,-1:], patch),1))\n",
        "            # print('generate x',x.shape)\n",
        "            # print('generate output',output)\n",
        "\n",
        "            # out = torch.cat([out, output[:,-1:]], dim=1)\n",
        "            out = torch.cat([out, output[:,:0]], dim=1)\n",
        "\n",
        "            # entropies = entropy(out) # [b,t] float\n",
        "            entropies = (out @ self.eos.transpose(-2,-1)).squeeze(-1).cumsum(1) # [b,t]\n",
        "            # print('entropies',entropies)\n",
        "            mask = patchify(entropies, 1.)\n",
        "            # print('generate',out.shape, mask.shape)\n",
        "            # print(mask)\n",
        "            # print((mask.sum(1)>=2))\n",
        "            if (mask.sum(1)>=2).all():\n",
        "            # if (mask.sum(1)>=2 or (EOS_IDX == mask).any(1)).all():\n",
        "                break\n",
        "        # print('BLTlayer gen',mask.shape)\n",
        "        print(mask)\n",
        "        # mask = padded_block_mask(mask)[:,1:]\n",
        "        mask = mask.cumsum(dim=1)>=2 # [b,t]\n",
        "        print(mask)\n",
        "        out.unsqueeze(1) *mask.unsqueeze(-1)\n",
        "\n",
        "        # return x[1:-1]\n",
        "        return torch.cat((x, out),1), mask\n",
        "# BOS_IDX, EOS_IDX\n",
        "\n",
        "\n",
        "# def generate(model, context, max_steps=64, temperature=1):\n",
        "#     x = torch.tensor([train_data.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "#     model.eval()\n",
        "#     for n in range(max_steps):\n",
        "#         with torch.no_grad():\n",
        "#             output = model(x)\n",
        "#         # print('generate', output.shape, hidden.shape)\n",
        "#         output = output[:, -1] # get logit for last character\n",
        "#         output = output/temperature\n",
        "#         output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "#         ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "#         x = torch.cat((x, ix),1)\n",
        "#     completion = ''.join([train_data.itos[int(i)] for i in x.flatten()])\n",
        "#     return completion\n",
        "\n",
        "d_model = 4\n",
        "x = torch.rand(2,3,d_model)\n",
        "patch = torch.rand(2,1,d_model)\n",
        "model = BLTlayer(d_model, d_model).to(device)\n",
        "# print(model)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # 1e-4 #3e-4\n",
        "\n",
        "# out, patches = model(x)\n",
        "out = model(x)\n",
        "gloss = model.gen_loss(out)\n",
        "patches, mask = model.to_patch(x, out)\n",
        "aloss = model.ae_loss(x, patches, mask)\n",
        "# loss = gloss + aloss\n",
        "\n",
        "# out = model.loss(x)\n",
        "# out, mask = model.generate(x)\n",
        "# out, mask = model.generate(x, patch)\n",
        "# print(out.shape)\n",
        "# out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "# print(patches.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e02a53f-3969-4f25-dcee-4aee29e98bf0",
        "cellView": "form",
        "id": "f93Pd8CMVgai"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLT me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BLT(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model, out_dim, num_layers):\n",
        "    def __init__(self, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(50, d_model)\n",
        "        # self.pos_enc = RoPE(d_model, base=10000)\n",
        "        # self.layer = nn.ModuleList([BLTlayer(d_model, out_dim) for _ in range(num_layers)])\n",
        "        self.layer = nn.ModuleList([BLTlayer(d_model, d_model) for _ in range(num_layers)])\n",
        "        # self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch, seq_len = x.shape\n",
        "        # x = self.pos_enc(self.tok_emb(x)) # [batch_size, seq_len, d_model]\n",
        "        for level in self.layer:\n",
        "            # out, x = level(x) # [b,t,d]\n",
        "            out = level(x) # [b,t,d]\n",
        "            # outs.append(out)\n",
        "            x = level.to_patch(out)\n",
        "        # if self.lin: x = self.lin(x)\n",
        "        return x # [batch, num_patch, out_dim]\n",
        "\n",
        "\n",
        "#     def generate(self, x):\n",
        "#         patches = []\n",
        "#         for j, patch in enumerate(patches):\n",
        "#             if len(patch)>0:\n",
        "#                 break\n",
        "#         # patch in patches\n",
        "#         for i in reversed(range(j)):\n",
        "#             patch = patches[i].pop(0)\n",
        "#             layer = self.layers[i]\n",
        "#             # = self.perceiver(x, cond=patch)\n",
        "#             out = layer.generate(x if i==0 else None, patch)\n",
        "#             if i!=0: patches[i-1].append(out[:,-1])\n",
        "# # out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "#         # entropies = entropy(out) # [b,t] float\n",
        "#         # mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "#         return out[:,-1]\n",
        "\n",
        "    def generate(self, x):\n",
        "        # batch, seq_len = x.shape\n",
        "        # x = self.pos_enc(self.tok_emb(x)) # [batch_size, seq_len, d_model]\n",
        "        src_lst = [x]\n",
        "        msk_lst = []\n",
        "        for level in self.layer:\n",
        "            # out, x = level(x) # [b,t,d]\n",
        "            out = level(x) # [b,t,d]\n",
        "            # outs.append(out)\n",
        "            if i != len(self.levels)-1\n",
        "                x, mask = model.to_patch(x, out)\n",
        "                # x = level.to_patch(out)\n",
        "                src_lst.append(x)\n",
        "        # if self.lin: x = self.lin(x)\n",
        "        # return x # [batch, num_patch, out_dim]\n",
        "        patch = None\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            layer = self.layers[i]\n",
        "            x = src_lst.pop()\n",
        "            mask = msk_lst.pop()\n",
        "            out, mask_ = model.generate(x, patch)\n",
        "            out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask_],dim=1))\n",
        "            # = self.perceiver(x, cond=patch)\n",
        "            patch = x\n",
        "            # out = layer.generate(x if i==0 else None, patch)\n",
        "            if i!=0: patches[i-1].append(out[:,-1])\n",
        "\n",
        "        # entropies = entropy(out) # [b,t] float\n",
        "        # mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "        return out[:,-1]\n",
        "\n",
        "# out, patches = model(x)\n",
        "out = model(x)\n",
        "print('out', out.shape)\n",
        "patches, mask, patch_mask = model.to_patch(x, out)\n",
        "print('patches, mask',patches.shape, mask.shape)\n",
        "gloss = model.gen_loss(x)\n",
        "aloss = model.ae_loss(x, patches, mask, patch_mask)\n",
        "# loss = gloss + aloss\n",
        "\n",
        "# out = model.loss(x)\n",
        "# out, mask = model.generate(x)\n",
        "# out, mask = model.generate(x, patch)\n",
        "# out, mask_ = left_pad_tensor(torch.cat([x,y],dim=1), torch.cat([mask, mask],dim=1))\n",
        "\n",
        "\n",
        "    def loss(self, x):\n",
        "        # batch, seq_len = x.shape\n",
        "        x = self.tok_emb(x) #[batch_size, seq_len, d_model]\n",
        "        tt_loss = 0\n",
        "        for level in self.layer:\n",
        "            # out, x = level(x) # [b,t,d]\n",
        "            # out = level(x) # [b,t,d]\n",
        "            out = level(x) # [b,t,d]\n",
        "            # outs.append(out)\n",
        "            # loss = F.mse_loss(x[:,1:], out[:,:-1])\n",
        "            # tt_loss += loss\n",
        "            # x, mask = level.to_patch(out)\n",
        "\n",
        "\n",
        "            tt_loss += model.gen_loss(out)\n",
        "            if i != len(self.levels)-1\n",
        "                patches, mask = model.to_patch(x, out)\n",
        "                tt_loss += model.ae_loss(x, patches, mask)\n",
        "\n",
        "            # # output = self.perceiver(src, cond=torch.cat((src, patch),1))\n",
        "            # output = self.perceiver(pos(torch.cumsum()), cond=patch, mask=mask)\n",
        "\n",
        "        # if self.lin: x = self.lin(x)\n",
        "        # return x # [batch, num_patch, out_dim]\n",
        "        return tt_loss#, out\n",
        "\n",
        "\n",
        "# pos_all, entropy mask;\n",
        "# AE: mask->pos_patch?/conv1d? + mask+ latent(mean+*leant) -> patch; pos_patch?+ mask.T + patch(+*learnt?)-> og\n",
        "# gen train: mse(x[1:],out[:-1])\n",
        "# gen inference: x cond=x, cond=(x+patch(+*learnt?)), if entropy/bos, pop patch and regen\n",
        "\n",
        "\n",
        "vocab_size=50\n",
        "in_dim, d_model, out_dim, num_layers = 4,16,vocab_size,1\n",
        "# x = torch.rand(2,5,in_dim)\n",
        "x = torch.randint(0,vocab_size, (2,500))\n",
        "model = BLT(d_model, out_dim, num_layers)\n",
        "# model = BLTlayer(d_model, d_model)\n",
        "# out = model(x)\n",
        "# out = model.loss(x)\n",
        "out = model.generate(x)\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XN1AYY3b2OzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLTlayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# first\n",
        "class BLTlayer(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model=None, out_dim=None, num_layers=1):\n",
        "    def __init__(self, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        # d_model = d_model or in_dim\n",
        "        # self.rnn = nn.GRU(in_dim, d_model, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(d_model, d_model, num_layers, batch_first=True)\n",
        "        # self.latent = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.perceiver = AttentionBlock(d_model, n_heads=4, cond_dim=d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "\n",
        "    def forward(self, x, h0=None): # [batch_size, seq_len, d_model]\n",
        "    # def forward(self, x): # [batch_size, seq_len, d_model]\n",
        "        out, h0 = self.rnn(x, h0) # [b,t,d], [nlyr,b,d]\n",
        "        # out = self.perceiver(x) # [b,t,d], [nlyr,b,d]\n",
        "        entropies = entropy(out) # [b,t] float\n",
        "        mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "        mask = padded_block_mask(mask) # [batch ,num_patch, seq_len]\n",
        "        latent = (x.unsqueeze(1) *mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "        # latent = self.latent.repeat(1,mask.shape[1],1) # [b ,num_patch, d]\n",
        "        out = self.perceiver(latent, cond=x, mask=mask) # [batch, num_patch, latent_dim/d_model]\n",
        "        # print('BLTlayer',out.shape)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [batch, num_patch, out_dim]\n",
        "\n",
        "\n",
        "# rnn\n",
        "class BLTlayer(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model=None, out_dim=None, num_layers=1):\n",
        "    def __init__(self, d_model, out_dim=None, cond_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        # d_model = d_model or in_dim\n",
        "        # self.rnn = nn.GRU(in_dim, d_model, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(d_model, d_model, num_layers, batch_first=True)\n",
        "        # # self.latent = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        # self.cond_dim = d_model\n",
        "\n",
        "    def forward(self, x, h0=None): # [batch_size, seq_len, d_model]\n",
        "        # torch.cat([x, cond])\n",
        "        out, h0 = self.rnn(x, h0) # [b,t,d], [nlyr,b,d]\n",
        "        entropies = entropy(out) # [b,t] float\n",
        "        mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "        mask = padded_block_mask(mask) # [batch ,num_patch, seq_len]\n",
        "        latent = (x.unsqueeze(1) *mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "\n",
        "        out, h0 = self.rnn(torch.cat([x, latent]), h0) # [b,t,d], [nlyr,b,d]\n",
        "\n",
        "        # print('BLTlayer',out.shape)\n",
        "        if self.lin: out = self.lin(out)\n",
        "        return out # [batch, num_patch, out_dim]\n",
        "\n",
        "# transformer\n",
        "class BLTlayer(nn.Module):\n",
        "    # def __init__(self, in_dim, d_model=None, out_dim=None, num_layers=1):\n",
        "    def __init__(self, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        # d_model = d_model or in_dim\n",
        "        # self.latent = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.perceiver = AttentionBlock(d_model, n_heads=4, cond_dim=d_model)\n",
        "        self.lin = nn.Linear(d_model, out_dim) if out_dim and out_dim != d_model else None\n",
        "        self.bos = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "\n",
        "    def forward(self, x): # [batch_size, seq_len, d_model]\n",
        "        b,t,d = x.shape\n",
        "        causal_mask = torch.tril(torch.ones((t,t), device=device)).bool().unsqueeze(0).repeat(b,1,1)\n",
        "        # with torch.no_grad():\n",
        "        # print('BLTlayer',x.shape, causal_mask.shape)\n",
        "        out = self.perceiver(x, cond=x, mask=causal_mask) # [b,t,d], [nlyr,b,d]\n",
        "        entropies = entropy(out) # [b,t] float\n",
        "        mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "        mask = padded_block_mask(mask) # [batch ,num_patch, seq_len]\n",
        "        latent = (x.unsqueeze(1) *mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "        # latent = self.latent.repeat(1,mask.shape[1],1) # [b ,num_patch, d]\n",
        "        patches = self.perceiver(latent, cond=x, mask=mask) # [batch, num_patch, latent_dim/d_model]\n",
        "        # print('BLTlayer',out.shape)\n",
        "        if self.lin: patches = self.lin(patches)\n",
        "        return out, patches # [batch, num_patch, out_dim]\n",
        "\n",
        "    def generate(self, x, patch=None):\n",
        "        x = self.bos\n",
        "        if patch==None: patch=x\n",
        "        while True:\n",
        "            x = self.perceiver(x, cond=patch)\n",
        "            entropies = entropy(out) # [b,t] float\n",
        "            mask = patchify(entropies, 1.)\n",
        "            if (mask.sum(1)>=2).all():\n",
        "            # if (mask.sum(1)>=2 or (EOS_IDX == mask).any(1)).all():\n",
        "                mask = padded_block_mask(mask)[:,1:]\n",
        "                x.unsqueeze(1) *mask.unsqueeze(-1)\n",
        "                return x[1:-1]\n",
        "# BOS_IDX, EOS_IDX\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fUxGck00vkrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BLT me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BLT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.tok_emb = nn.Embedding(50, d_model)\n",
        "\n",
        "        self.rnn = nn.GRU(d_model, d_model, num_layers, batch_first=True)\n",
        "        # self.latent = nn.Parameter(torch.randn(1,1,d_model)*.02)\n",
        "        self.perceiver = AttentionBlock(d_model, n_heads=4, cond_dim=d_model)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        # batch, seq_len = x.shape\n",
        "        x = self.tok_emb(x) #[batch_size, seq_len, d_model]\n",
        "        # if h0==None: h0 = torch.zeros(1, batch, self.d_model, device=device) # [nlyr,b,d]\n",
        "\n",
        "        # for level in self.rnn:\n",
        "        # out, h0 = level(x, h0)\n",
        "        out, h0 = self.rnn(x, h0) # [b,t,d], [nlyr,b,d]\n",
        "        entropies = entropy(out) # [b,t] float\n",
        "\n",
        "        mask = patchify(entropies, 1.) # [b,t,t] # T at start of each patch, F otherwise\n",
        "        mask = padded_block_mask(mask) # [batch ,num_patch, seq_len]\n",
        "        latent = (x.unsqueeze(1) *mask.unsqueeze(-1)).mean(2) # [b ,num_patch, t, d] -> [b ,num_patch, d]\n",
        "        # latent = self.latent.repeat(1,mask.shape[1],1) # [b ,num_patch, d]\n",
        "        out = self.perceiver(latent, cond=x, mask=mask) # [batch, num_patch, latent_dim/d_model]\n",
        "        print('out',out.shape)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        return out # [batch, num_patch, out_dim]\n",
        "\n",
        "\n",
        "vocab_size=50\n",
        "in_dim, d_model, out_dim, num_layers = 4,16,vocab_size,1\n",
        "# x = torch.rand(2,5,in_dim)\n",
        "x = torch.randint(0,vocab_size, (2,5))\n",
        "model = BLT(in_dim, d_model, out_dim, num_layers)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afd81bb-d0de-494f-dd48-f59c696bb205",
        "cellView": "form",
        "id": "VFWTBtt3zxIP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out torch.Size([2, 3, 16])\n",
            "torch.Size([2, 3, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title block_diag\n",
        "\n",
        "# patch_start_ids = find_entropy_patch_start_ids(\n",
        "#     scores,\n",
        "#     self.patch_size,\n",
        "#     include_next_token=include_next_token,\n",
        "#     threshold=threshold if threshold is not None else self.threshold,\n",
        "#     threshold_add=self.threshold_add,\n",
        "#     monotonicity=self.monotonicity,\n",
        "# )\n",
        "# patch_lengths = patch_lengths_from_start_ids(\n",
        "#     patch_start_ids, seq_len_next_tok\n",
        "# )\n",
        "\n",
        "\n",
        "def block_diag(mask): # [b,t]\n",
        "    \"\"\"Convert a 1D bool mask to a block diagonal matrix, where each `True` starts a new block.\n",
        "    mask = [1,0,1,1] ->\n",
        "    [[1,1,0,0],\n",
        "     [1,1,0,0],\n",
        "     [0,0,1,0],\n",
        "     [0,0,0,1]]\"\"\"\n",
        "    block_ids = mask.cumsum(1) # Assign a unique group index to each block e.g., [0, 0, 1, 2]\n",
        "    row_ids = block_ids.unsqueeze(2)  # [b,t,1]\n",
        "    col_ids = block_ids.unsqueeze(1)  # [b,1,t]\n",
        "    out = row_ids == col_ids # Compare row and col IDs — same block → True\n",
        "    return out # [b,t,t]\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jZHrlMw5kAKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title blockwise_mean\n",
        "import torch\n",
        "\n",
        "def blockwise_mean(x, mask): # [b,t]\n",
        "    \"\"\"\n",
        "    Given a 1D tensor x and a boolean mask, compute block-wise means,\n",
        "    where each True in mask starts a new block.\n",
        "\n",
        "    Example:\n",
        "    x = [1,2,3,4], mask = [T,F,T,T] → [ (1+2)/2, 3, 4 ]\n",
        "    \"\"\"\n",
        "    batch = x.shape[0]\n",
        "    # Assign a block ID to each element\n",
        "    block_ids = mask.cumsum(1) - 1  # e.g., [0,0,1,2]\n",
        "\n",
        "    # Get number of blocks\n",
        "    # num_blocks = int(block_ids[-1]) + 1\n",
        "    num_blocks = block_ids[:,-1] + 1\n",
        "    print(block_ids)\n",
        "    # Sum values and count per block\n",
        "    # sums = torch.zeros(num_blocks, dtype=x.dtype, device=x.device).index_add(0, block_ids, x)\n",
        "    # counts = torch.zeros(num_blocks, dtype=x.dtype, device=x.device).index_add(0, block_ids, torch.ones_like(x))\n",
        "    sums = torch.zeros(batch, num_blocks, dtype=x.dtype, device=x.device).index_add(0, block_ids, x)\n",
        "    counts = torch.zeros(num_blocks, dtype=x.dtype, device=x.device).index_add(0, block_ids, torch.ones_like(x))\n",
        "    return sums / counts\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def blockwise_mean(x: torch.Tensor, mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    x:     (B, N) - input values\n",
        "    mask:  (B, N) - boolean mask (True starts new block)\n",
        "\n",
        "    Returns:\n",
        "    - mean_values:  (total_blocks,) — concatenated mean values across all blocks\n",
        "    - block_lens:   (B,) — number of blocks per sequence (can be used to split result)\n",
        "    \"\"\"\n",
        "    B, N = x.shape\n",
        "\n",
        "    # Compute block IDs for each sequence\n",
        "    block_ids = mask.cumsum(dim=1) - 1  # (B, N)\n",
        "    block_lens = mask.sum(dim=1)        # (B,)\n",
        "    total_blocks = block_lens.sum()     # scalar\n",
        "\n",
        "    # Offset block IDs to make them unique across the batch\n",
        "    block_offsets = torch.cumsum(torch.cat([torch.zeros(1, device=x.device, dtype=block_lens.dtype), block_lens[:-1]]), dim=0)\n",
        "    block_ids += block_offsets.unsqueeze(1)  # shape: (B, N)\n",
        "\n",
        "    # Flatten\n",
        "    flat_ids = block_ids.reshape(-1)           # (B*N,)\n",
        "    flat_x = x.reshape(-1)                     # (B*N,)\n",
        "    ones = torch.ones_like(flat_x)\n",
        "\n",
        "    # Compute sums and counts using index_add\n",
        "    mean_sums = torch.zeros(total_blocks, device=x.device, dtype=x.dtype).index_add(0, flat_ids, flat_x)\n",
        "    mean_counts = torch.zeros(total_blocks, device=x.device, dtype=x.dtype).index_add(0, flat_ids, ones)\n",
        "\n",
        "    # Return mean and optional info to reshape if needed\n",
        "    return mean_sums / mean_counts, block_lens\n",
        "\n",
        "\n",
        "# x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "x = torch.rand(2,4)\n",
        "print(x)\n",
        "mask = torch.tensor([1, 0, 1, 1], dtype=torch.bool).repeat(2,1)\n",
        "print(blockwise_mean(x, mask))  # Output: tensor([1.5, 3.0, 4.0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD0yW3yHHTbL",
        "outputId": "8d9b3cf1-5f92-47cc-8002-f65d1187b184",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4062, 0.2538, 0.5809, 0.0500],\n",
            "        [0.7076, 0.0494, 0.7538, 0.4290]])\n",
            "(tensor([0.3300, 0.5809, 0.0500, 0.3785, 0.7538, 0.4290]), tensor([3, 3]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swJu5gX4ScMg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch bytelatent/model/blt.py\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/model/blt.py\n",
        "from enum import Enum, auto\n",
        "from typing import Any, Optional\n",
        "\n",
        "import torch\n",
        "from pydantic import ConfigDict, model_validator\n",
        "from torch import nn\n",
        "from torch.nn.attention.flex_attention import create_block_mask\n",
        "from typing_extensions import Self\n",
        "\n",
        "from bytelatent.base_transformer import (BaseTransformerArgs, InitStdFactor, SequenceModelWithOutput, TransformerBlock)\n",
        "from bytelatent.data.patcher import Patcher, PatcherArgs\n",
        "from bytelatent.model.latent_transformer import GlobalTransformer\n",
        "from bytelatent.model.local_models import LocalDecoder, LocalEncoder, LocalModelArgs\n",
        "from bytelatent.model.utils import downsample\n",
        "from bytelatent.tokenizers.constants import BOE_ID, BOS_ID, EOS_ID, OFFSET, PAD_ID\n",
        "\n",
        "def causal_mask(b, h, q_idx, kv_idx):\n",
        "    return q_idx >= kv_idx\n",
        "\n",
        "def setattrs(_self, **kwargs):\n",
        "    for k, v in kwargs.items():\n",
        "        setattr(_self, k, v)\n",
        "\n",
        "def get_encoder_dim_token_emb(args):\n",
        "    if args.dim_token is not None:\n",
        "        dim_token_emb = args.dim_token\n",
        "    elif args.use_local_encoder_transformer:\n",
        "        dim_token_emb = args.dim_local_encoder\n",
        "    else:\n",
        "        dim_token_emb = args.dim_global // args.patch_size\n",
        "    return dim_token_emb\n",
        "\n",
        "\n",
        "def get_encoder_dim_patch_emb(args):\n",
        "    dim_patch_emb = None\n",
        "    if args.cross_attn_encoder:\n",
        "        if args.cross_attn_init_by_pooling:\n",
        "            dim_patch_emb = args.dim_local_encoder\n",
        "        else:\n",
        "            dim_patch_emb = args.dim_global\n",
        "    return dim_patch_emb\n",
        "\n",
        "\n",
        "def parse_ngram_to_size(ngram_to_size_str: str | None) -> dict[int, int]:\n",
        "    if ngram_to_size_str is None:\n",
        "        return None\n",
        "    ngram_to_size = {}\n",
        "    for entry in ngram_to_size_str.split(\",\"):\n",
        "        ngram, size = entry.split(\":\")\n",
        "        ngram = int(ngram)\n",
        "        size = int(size)\n",
        "        ngram_to_size[ngram] = size\n",
        "    return ngram_to_size\n",
        "\n",
        "\n",
        "def fill_tokens(tokens, patch_size, fill_id):\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    if seq_len % patch_size == 0:\n",
        "        return tokens\n",
        "    else:\n",
        "        remaining = patch_size - seq_len % patch_size\n",
        "        final_padding = tokens.new(batch_size, remaining).fill_(fill_id)\n",
        "        return torch.cat((tokens, final_padding), dim=1)\n",
        "\n",
        "def decoder_patch_ids_from_lengths(patch_lengths, nb_boe, seq_len):\n",
        "    first_patch_length = patch_lengths[0, 0]\n",
        "    assert torch.all(first_patch_length == patch_lengths[:, 0]), \"first patch should always be the same size (1 for dynamic, patch_size for static).\"\n",
        "    assert (first_patch_length - nb_boe == 1), f\"First patch (patch length: {first_patch_length}) should have one non-boe token (boe toks: {nb_boe})\"\n",
        "    # Remove first patch from patch_ids for local decoder inputs and shift the last patch.\n",
        "    # decoder_patch_lengths = patch_lengths[:, 1:].clone()\n",
        "    # decoder_patch_lengths = add_to_last_nonzero_patch(decoder_patch_lengths, 1)\n",
        "    decoder_patch_lengths = patch_lengths[:, 1:]\n",
        "    assert (decoder_patch_lengths.sum() + (nb_boe + 1) * patch_lengths.shape[0] == patch_lengths.sum()), f\"{decoder_patch_lengths.sum() + (nb_boe + 1) * patch_lengths.shape[0]} != {patch_lengths.sum()}\"\n",
        "    assert torch.all(decoder_patch_lengths >= 0), f\"{decoder_patch_lengths}\"\n",
        "    decoder_patch_ids = patch_ids_from_lengths(patch_lengths=decoder_patch_lengths, seq_len=seq_len)\n",
        "    return decoder_patch_ids\n",
        "\n",
        "def create_patch_mask_from_ids(patch_ids, num_patches, window=None, patches_as_queries=False):\n",
        "    \"\"\"\n",
        "    Creates a tensor of shape [bs, seq_len, num_patches] where each element at position (i, j, k)\n",
        "    is True if the patch id at position (i, j) is less than or equal to k.\n",
        "    Args:\n",
        "        patch_ids (torch.Tensor): Tensor of shape [bs, seq_len] containing patch ids.\n",
        "        num_patches (int): Total number of patches.\n",
        "        window (int): If not None, only considers patches within a window of size window.\n",
        "        patches_as_queries (bool): If True, the patches are used as queries\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of shape [bs, q_len, kv_len] with the desired mask.\n",
        "    \"\"\"\n",
        "    bs, seq_len = patch_ids.shape\n",
        "    if not patches_as_queries:\n",
        "        q_ids = patch_ids.unsqueeze(-1).expand(bs, seq_len, num_patches)\n",
        "        kv_ids = (torch.arange(num_patches, device=patch_ids.device)[None,None,...].expand(bs, seq_len, num_patches))\n",
        "    else:\n",
        "        kv_ids = patch_ids.unsqueeze(1).expand(bs, num_patches, seq_len)\n",
        "        q_ids = (torch.arange(num_patches, device=patch_ids.device)[None,...,None].expand(bs, num_patches, seq_len))\n",
        "    if window is None:\n",
        "        mask = q_ids == kv_ids\n",
        "    else:\n",
        "        mask = (kv_ids <= q_ids) & (q_ids < kv_ids + window)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def cross_attn_mask(patch_ids, patch_lengths, N, patches_as_queries=False, cross_attn_k=1, window=None, block_mask=True):\n",
        "    bs = patch_ids.shape[0]\n",
        "    with torch.no_grad():\n",
        "        # Create the patch mask\n",
        "        cross_mask = create_patch_mask_from_ids(patch_ids, patch_lengths.shape[1], window=window, patches_as_queries=patches_as_queries).repeat_interleave(cross_attn_k, dim=1 if patches_as_queries else -1)\n",
        "        q_len = patch_lengths.shape[1] * cross_attn_k if patches_as_queries else N\n",
        "        kv_len = N if patches_as_queries else patch_lengths.shape[1] * cross_attn_k\n",
        "        assert cross_mask.shape == (bs, q_len, kv_len), f\"{cross_mask.shape} != {(bs, q_len, kv_len)}\"\n",
        "        if block_mask:\n",
        "            def patch_mask(b, h, q_idx, kv_idx):\n",
        "                return cross_mask[b, q_idx, kv_idx]\n",
        "            block_mask = create_block_mask(patch_mask, B=bs, H=None, Q_LEN=q_len, KV_LEN=kv_len, _compile=True,)\n",
        "            return block_mask\n",
        "        else:\n",
        "            return torch.where(cross_mask, torch.tensor(0.0), torch.tensor(float(\"-inf\"))).unsqueeze(1)  # [bs, 1, q_len, kv_len]\n",
        "\n",
        "\n",
        "def get_blt_input(tokens: torch.Tensor, enforce_patch_size_multiple: bool, nb_boe: torch.Tensor, patch_size: int, boe_id: int):\n",
        "    \"\"\"\n",
        "        This function returns X_et, X_gt and X_dt, the encoder, global, and decoder\n",
        "    tokens respectively.\n",
        "\n",
        "    Consider the input and target sequences:\n",
        "    X=[3,4,5,6,7,eos,bos,8,9,10,eos,bos,11,12,13]\n",
        "    Y=[4,5,6,7,eos,bos,8,9,10,eos,bos,11,12,13,14]\n",
        "    with patch_size=4\n",
        "\n",
        "    Note 1: that there will be no special tokens introduced at the patch level.\n",
        "    Note 2: X_e needs to be trimmed to be passed to Global\n",
        "\n",
        "    Current without boe:\n",
        "    X_et = [[boe,boe,boe,boe] [3,4,5,6],      [7,eos,bos,8],    [9,10,eos,bos] [11,12,13, pad]]\n",
        "    X_g =  [[boe,boe,boe,boe] [3,4,5,6],      [7,eos,bos,8],    [9,10,eos,bos] [11,12,13, pad]] # remove last glob patch\n",
        "    X_dt = [[3,4,5,6]         [7,eos,bos,8],  [9,10,eos,bos],   [11,12,13]]\n",
        "    Y =    [[4,5,6,7]         [eos,bos,8,9],  [10,eos,bos,11],  [12,13,14]]\n",
        "\n",
        "    Note 1: that there will be no special tokens introduced at the patch level.\n",
        "    Note 2: X_e needs to be trimmed to be passed to Global\n",
        "    \"\"\"\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    local_encoder_tokens = tokens\n",
        "    local_decoder_tokens = tokens\n",
        "\n",
        "    if nb_boe > 0:\n",
        "        padded_patch = tokens.new(batch_size, nb_boe).fill_(boe_id)\n",
        "        local_encoder_tokens = torch.cat((padded_patch, local_encoder_tokens), dim=1)\n",
        "    # global_tokens = tokens.new(batch_size, ((seq_len-1) // patch_size)+1).fill_(boe_id)\n",
        "\n",
        "    # create global tokens, contains boe tokens and eos\n",
        "    # padded_local_encoder_tokens = fill_tokens(local_encoder_tokens, patch_size, boe_id)\n",
        "    # patches = padded_local_encoder_tokens.view(batch_size, -1, patch_size)\n",
        "    # global_tokens = (patches.eq(eos_id).any(dim=2).int() * eos_id)[:, 1:]\n",
        "    # global_tokens += global_tokens.eq(0).int() * boe_id\n",
        "    # TODO: fix this when we want to use block causal in the global.\n",
        "    if enforce_patch_size_multiple and local_encoder_tokens.shape[-1] % patch_size != 0:\n",
        "        local_encoder_tokens = fill_tokens(local_encoder_tokens, patch_size, boe_id)\n",
        "    return local_encoder_tokens, None, local_decoder_tokens\n",
        "\n",
        "\n",
        "def patch_ids_from_lengths(patch_lengths, seq_len):\n",
        "    bs, num_patches = patch_lengths.shape\n",
        "    # Create a tensor of cumulative sums of the patch lengths\n",
        "    cum_d = torch.cat([torch.zeros(bs, 1, dtype=patch_lengths.dtype, device=patch_lengths.device), patch_lengths.cumsum(dim=-1)], dim=-1)\n",
        "    patch_ids = (cum_d.unsqueeze(-1) <= torch.arange(seq_len, device=cum_d.device)).sum(dim=-2) - 1\n",
        "    assert not (torch.max(patch_ids) > patch_lengths.shape[-1] or torch.min(patch_ids) < 0), f\"{torch.max(patch_ids)} > {patch_lengths.shape[-1]} or {torch.min(patch_ids)} < 0\"\n",
        "    return patch_ids\n",
        "\n",
        "\n",
        "class ByteLatentTransformer(nn.Module, SequenceModelWithOutput):\n",
        "    \"\"\"\n",
        "    The ByteLatentTransformer (BLT) is a byte-level language model architecture that processes byte sequences\n",
        "    by dynamically segmenting them into patches. It uses a combination of local encoders, global transformers,\n",
        "    and local decoders to efficiently encode and decode byte sequences, leveraging patch-based processing for\n",
        "    improved performance and inference efficiency.\n",
        "    \"\"\"\n",
        "    def __init__(self, args: ByteLatentTransformerArgs):\n",
        "        super().__init__()\n",
        "\n",
        "        # General configuration\n",
        "        self.weight_tying = args.weight_tying\n",
        "        self.patch_size = args.patch_size\n",
        "        self.patching_mode = args.patching_mode\n",
        "\n",
        "        self.downsampling_by_pooling = args.downsampling_by_pooling\n",
        "        self.patching_threshold = args.patching_threshold\n",
        "        self.dim = args.dim\n",
        "        self.init_base_std = args.init_base_std\n",
        "        self.init_std_factor = InitStdFactor(args.init_std_factor)\n",
        "        self.max_seqlen = args.max_seqlen\n",
        "\n",
        "        # Cross attention configuration\n",
        "        self.cross_attn_encoder = args.cross_attn_encoder\n",
        "        self.cross_attn_decoder = args.cross_attn_decoder\n",
        "        self.cross_attn_k = args.cross_attn_k\n",
        "        self.cross_attn_window_encoder = args.cross_attn_window_encoder\n",
        "        self.cross_attn_window_decoder = args.cross_attn_window_decoder\n",
        "        self.cross_attn_use_flex_attention = args.cross_attn_use_flex_attention\n",
        "\n",
        "        # Encoder hash configuration\n",
        "        self.encoder_hash_byte_group_size = args.encoder_hash_byte_group_size\n",
        "        self.encoder_hash_byte_group_vocab = args.encoder_hash_byte_group_vocab\n",
        "        self.encoder_hash_byte_group_nb_functions = (\n",
        "            args.encoder_hash_byte_group_nb_functions\n",
        "        )\n",
        "\n",
        "        # ByteLatent modules\n",
        "        self.local_encoder = create_local_encoder(args)\n",
        "        self.global_transformer = create_global_transformer(args)\n",
        "        self.local_decoder = create_local_decoder(args)\n",
        "\n",
        "        # Patcher module\n",
        "        if args.patch_in_forward:\n",
        "            self.patcher = Patcher()\n",
        "\n",
        "    def get_output_seq_len(self):\n",
        "        return self.max_seqlen\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor,\n",
        "        patch_lengths: Optional[torch.Tensor] = None,\n",
        "        ngram_ids: Optional[torch.Tensor] = None):\n",
        "        # Ensure ngram_ids is either a tensor or None\n",
        "        assert (isinstance(ngram_ids, torch.Tensor) or ngram_ids is None), f\"ngram_ids must be a tensor or None, but was: {type(ngram_ids)}\"\n",
        "\n",
        "        bs, N = tokens.shape  # Batch size and sequence length\n",
        "\n",
        "        # Get megabyte inputs\n",
        "        nb_boe = int(0 if self.patching_mode != \"\" else self.patch_size - 1)\n",
        "        local_encoder_tokens, _, local_decoder_tokens = get_blt_input(tokens=tokens, enforce_patch_size_multiple=False, nb_boe=nb_boe, patch_size=self.patch_size, boe_id=self.boe_id)\n",
        "\n",
        "        # Patching\n",
        "        if patch_lengths is None:\n",
        "            assert (getattr(self, \"patcher\", None) is not None), \"Patcher not defined and no patch_lengths passed.\"\n",
        "            patch_lengths, tok_scores = self.patcher.patch(local_encoder_tokens, include_next_token=True, threshold=self.patcher.threshold)\n",
        "        else:\n",
        "            if nb_boe > 0:\n",
        "                patch_lengths[:, 0] += nb_boe\n",
        "\n",
        "        assert torch.min(patch_lengths) >= 0\n",
        "\n",
        "        # Generate patch IDs from patch_lengths\n",
        "        patch_ids = patch_ids_from_lengths(patch_lengths, local_encoder_tokens.shape[-1])\n",
        "        assert torch.max(patch_ids) + 1 <= torch.max((patch_lengths != 0).sum(dim=-1)), f\"{torch.max(patch_ids) + 1} > {torch.max((patch_lengths != 0).sum(dim=-1))}\"\n",
        "\n",
        "        cross_attn_mask_enc = None\n",
        "        # Cross-attention encoder\n",
        "        if self.cross_attn_encoder:\n",
        "            cross_attn_mask_enc = cross_attn_mask(patch_ids, patch_lengths, N, patches_as_queries=True,\n",
        "                cross_attn_k=self.cross_attn_k, window=self.cross_attn_window_encoder, block_mask=self.cross_attn_use_flex_attention)\n",
        "\n",
        "        # Hashing and embedding\n",
        "        local_encoder_embeds = compute_hash_embeddings(local_encoder_tokens=local_encoder_tokens, local_encoder=self.local_encoder, encoder_hash_tok_embedding=self.encoder_hash_tok_embedding,\n",
        "            encoder_hash_byte_group_nb_functions=self.encoder_hash_byte_group_nb_functions, encoder_hash_byte_group_size=self.encoder_hash_byte_group_size, encoder_hash_byte_group_vocab=self.encoder_hash_byte_group_vocab)\n",
        "\n",
        "        # N-gram table embeddings\n",
        "        if self.encoder_ngram_embedding is not None:\n",
        "            assert ngram_ids is not None, \"ngram_ids must be provided\"\n",
        "            if local_encoder_embeds is None:\n",
        "                local_encoder_embeds = self.local_encoder.tok_embeddings(local_encoder_tokens)\n",
        "            assert len(ngram_ids) == len(self.encoder_ngram_embedding), f\"ngram_ids.shape[0]={ngram_ids.shape[0]} versus len(encoder_ngram_embedding)={len(self.encoder_ngram_embedding)}, ngram_ids.shape={ngram_ids.shape}\"\n",
        "            for i in range(ngram_ids.shape[0]):\n",
        "                ngram_embedding = self.encoder_ngram_embedding[i]\n",
        "                ngram_embeds = ngram_embedding(ngram_ids[i])\n",
        "                assert (local_encoder_embeds.shape == ngram_embeds.shape), f\"Shape mismatch: {local_encoder_embeds.shape} vs {ngram_embeds.shape}, ngram_ids.shape={ngram_ids.shape}\"\n",
        "                local_encoder_embeds = local_encoder_embeds + ngram_embeds\n",
        "\n",
        "        # Local encoder\n",
        "        (h_encoder, h_cross), cache_encoder = self.local_encoder(tokens=local_encoder_tokens, embeds=local_encoder_embeds,\n",
        "            patch_embeds=None, cross_mask=cross_attn_mask_enc, num_patches=patch_lengths.shape[1], patch_ids=patch_ids)\n",
        "\n",
        "        # Downsampling\n",
        "        if not self.cross_attn_encoder:\n",
        "            assert (patch_ids.shape[1] == h_encoder.shape[1]), f\"{patch_ids.shape[1]} != {h_encoder.shape[1]}\"\n",
        "            h = downsample(h_encoder, patch_lengths.shape[1], patch_lengths,\n",
        "                patch_ids, downsampling_by_pooling=self.downsampling_by_pooling, patch_size=self.patch_size)\n",
        "        else:\n",
        "            # Reshape h_cross\n",
        "            h = h_cross.view(bs, patch_lengths.shape[1], -1)\n",
        "\n",
        "        # Global transformer\n",
        "        global_tokens = tokens.new(h.shape[0], h.shape[1]).fill_(self.boe_id)\n",
        "        rows, cols = torch.where(local_encoder_tokens == self.eos_id)\n",
        "        eos_patch_ids = patch_ids[rows, cols]\n",
        "        global_tokens[rows, eos_patch_ids] = self.eos_id\n",
        "\n",
        "        h, _ = self.global_transformer(embeds=h, tokens=global_tokens)\n",
        "\n",
        "        # Unpatching\n",
        "        dec_embeds = h_encoder[:, nb_boe : nb_boe + N, :]\n",
        "\n",
        "        # Generate decoder patch IDs\n",
        "        decoder_patch_ids = decoder_patch_ids_from_lengths(patch_lengths, nb_boe, local_decoder_tokens.shape[-1])\n",
        "        assert (torch.max(decoder_patch_ids) + 1 <= h.shape[1]), f\"{torch.max(decoder_patch_ids) + 1} > {h.shape[1]}\"\n",
        "        assert (decoder_patch_ids.shape[1] == dec_embeds.shape[1]), f\"{decoder_patch_ids.shape[1]} != {dec_embeds.shape[1]}\"\n",
        "\n",
        "        # Cross-attention decoder\n",
        "        if not self.cross_attn_decoder:\n",
        "            h = torch.gather(h, 1, decoder_patch_ids.unsqueeze(-1).expand(-1, -1, h.shape[-1]))\n",
        "            cross_attn_mask_dec = None\n",
        "            assert local_decoder_tokens.shape == h.shape[:-1]\n",
        "        else:\n",
        "            cross_attn_mask_dec = cross_attn_mask(decoder_patch_ids, patch_lengths, N, patches_as_queries=False, cross_attn_k=self.cross_attn_k, window=self.cross_attn_window_decoder, block_mask=self.cross_attn_use_flex_attention)\n",
        "\n",
        "        # Local decoder\n",
        "        output, _ = self.local_decoder(embeds=dec_embeds, patch_embeds=h, tokens=local_decoder_tokens, cross_mask=cross_attn_mask_dec)\n",
        "        return output\n",
        "\n",
        "\n",
        "local_encoder_tokens, _, local_decoder_tokens = get_blt_input(tokens=tokens, enforce_patch_size_multiple=False, nb_boe=nb_boe, patch_size=self.patch_size, boe_id=self.boe_id)\n",
        "patch_lengths, tok_scores = self.patcher.patch(local_encoder_tokens, include_next_token=True, threshold=self.patcher.threshold)\n",
        "patch_ids = patch_ids_from_lengths(patch_lengths, local_encoder_tokens.shape[-1])\n",
        "cross_attn_mask_enc = cross_attn_mask(patch_ids, patch_lengths, N, patches_as_queries=True,\n",
        "local_encoder_embeds = local_encoder_embeds + ngram_embeds\n",
        "(h_encoder, h_cross), cache_encoder = self.local_encoder(tokens=local_encoder_tokens, embeds=local_encoder_embeds,\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## other blt"
      ],
      "metadata": {
        "id": "SqVRFiA3j9gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title anine09 ByteLatentTransformer.py\n",
        "# https://github.com/anine09/EyesNet/blob/master/eyesnet/reranker/xrd_encoder/ByteLatentTransformer.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"Custom Layer Normalization.\"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias=True):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer Block with LayerNorm, Attention, and MLP.\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(n_embd)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim=n_embd, num_heads=n_head, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.ln2 = LayerNorm(n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(self.ln1(x), self.ln1(x), self.ln1(x))\n",
        "        x = x + attn_out\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class CrossAttentionLayer(nn.Module):\n",
        "    \"\"\"Cross Attention Layer for Encoder and Decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, query_dim, key_dim, n_head, dropout):\n",
        "        super().__init__()\n",
        "        self.ln_q = LayerNorm(query_dim)\n",
        "        self.ln_kv = LayerNorm(key_dim)\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim=query_dim, num_heads=n_head, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.proj = nn.Linear(query_dim, query_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        query = self.ln_q(query)\n",
        "        key = self.ln_kv(key)\n",
        "        value = self.ln_kv(value)\n",
        "        attn_out, _ = self.attn(query, key, value)\n",
        "        attn_out = self.proj(attn_out)\n",
        "        attn_out = self.dropout(attn_out)\n",
        "        return query + attn_out\n",
        "\n",
        "\n",
        "class HashNGramEmbedding(nn.Module):\n",
        "    \"\"\"Hash n-gram Embeddings.\"\"\"\n",
        "    def __init__(self, hash_sizes, hash_table_size, n_embd):\n",
        "        super().__init__()\n",
        "        self.hash_sizes = hash_sizes\n",
        "        self.hash_table_size = hash_table_size\n",
        "        self.n_embd = n_embd\n",
        "        self.hash_embeddings = nn.ModuleDict({f\"hash_{n}\": nn.Embedding(hash_table_size, n_embd) for n in hash_sizes})\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        embeddings = torch.zeros(B, T, self.n_embd, device=x.device)\n",
        "        for n in self.hash_sizes:\n",
        "            if T < n:\n",
        "                continue\n",
        "            # Extract n-grams\n",
        "            ngrams = x.unfold(1, n, 1)  # [B, T - n +1, n]\n",
        "            # Compute hash\n",
        "            hashes = self.roll_poly_hash(ngrams)\n",
        "            hashes = hashes % self.hash_table_size\n",
        "            # Lookup embeddings\n",
        "            hash_emb = self.hash_embeddings[f\"hash_{n}\"](\n",
        "                hashes\n",
        "            )  # [B, T - n +1, n_embd]\n",
        "            # Scatter add\n",
        "            embeddings[:, n - 1 : T, :] += hash_emb\n",
        "        # Normalize\n",
        "        embeddings = embeddings / len(self.hash_sizes)\n",
        "        return embeddings  # [B, T, n_embd]\n",
        "\n",
        "    def roll_poly_hash(self, ngrams):\n",
        "        \"\"\"Simple polynomial rolling hash.\"\"\"\n",
        "        base = 257\n",
        "        hash_val = torch.zeros(\n",
        "            ngrams.size(0), ngrams.size(1), device=ngrams.device, dtype=torch.long\n",
        "        )\n",
        "        for i in range(ngrams.size(2)):\n",
        "            hash_val = (hash_val * base + ngrams[:, :, i].long()) % (2**32)\n",
        "        return hash_val\n",
        "\n",
        "\n",
        "class LocalEncoder(nn.Module):\n",
        "    \"\"\"Local Encoder that encodes input bytes into patch representations.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        n_embd,\n",
        "        patch_size,\n",
        "        hash_sizes,\n",
        "        hash_table_size,\n",
        "        n_head,\n",
        "        dropout,\n",
        "        lE,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.n_embd = n_embd\n",
        "        self.byte_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.hash_ngram = HashNGramEmbedding(hash_sizes, hash_table_size, n_embd)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [Block(n_embd, n_head, dropout) for _ in range(lE)]\n",
        "        )\n",
        "        self.cross_attn = CrossAttentionLayer(n_embd, n_embd, n_head, dropout)\n",
        "        self.ln = LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        # Byte Embedding\n",
        "        x_emb = self.byte_embedding(x)  # [B, T, C]\n",
        "        # Hash n-gram Embedding\n",
        "        hash_emb = self.hash_ngram(x)  # [B, T, C]\n",
        "        x_emb = x_emb + hash_emb  # [B, T, C]\n",
        "        # Transformer Layers\n",
        "        for block in self.transformer_blocks:\n",
        "            x_emb = block(x_emb)\n",
        "        # Cross-Attention to form patches\n",
        "        # Assume patches are non-overlapping\n",
        "        # Pad if necessary\n",
        "        if T % self.patch_size != 0:\n",
        "            pad_len = self.patch_size - (T % self.patch_size)\n",
        "            pad = torch.zeros((B, pad_len), dtype=x.dtype, device=x.device).long()\n",
        "            pad_emb = self.byte_embedding(pad)  # [B, pad_len, C]\n",
        "            pad_emb += self.hash_ngram(pad)  # Incorporate hash embeddings\n",
        "            x_emb = torch.cat([x_emb, pad_emb], dim=1)  # [B, T + pad_len, C]\n",
        "            T += pad_len\n",
        "        # Reshape and pool to create patch representations\n",
        "        patches = x_emb.view(\n",
        "            B, T // self.patch_size, self.patch_size, self.n_embd\n",
        "        ).mean(\n",
        "            dim=2\n",
        "        )  # [B, N_patches, C]\n",
        "        patches = self.cross_attn(patches, x_emb, x_emb)  # [B, N_patches, C]\n",
        "        patches = self.ln(patches)\n",
        "        return patches  # [B, N_patches, C]\n",
        "\n",
        "\n",
        "class LatentTransformer(nn.Module):\n",
        "    \"\"\"Latent Transformer over patch representations.\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [Block(n_embd, n_head, dropout) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.ln_f = LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return self.ln_f(x)\n",
        "\n",
        "\n",
        "class ByteLatentTransformer(nn.Module):\n",
        "    \"\"\"Byte Latent Transformer combining encoder, transformer, and decoder.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        n_embd,\n",
        "        n_head,\n",
        "        n_layers_encoder,\n",
        "        n_layers_latent,\n",
        "        dropout,\n",
        "        patch_size,\n",
        "        hash_sizes,\n",
        "        hash_table_size,\n",
        "        block_size,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.local_encoder = LocalEncoder(\n",
        "            vocab_size,\n",
        "            n_embd,\n",
        "            patch_size,\n",
        "            hash_sizes,\n",
        "            hash_table_size,\n",
        "            n_head,\n",
        "            dropout,\n",
        "            n_layers_encoder,\n",
        "        )\n",
        "        self.latent_transformer = LatentTransformer(\n",
        "            n_embd, n_head, n_layers_latent, dropout\n",
        "        )\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.zeros(1, math.ceil(block_size / patch_size), n_embd)\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode bytes to patches\n",
        "        patches = self.local_encoder(x)  # [B, N_patches, C]\n",
        "        # Add positional embeddings\n",
        "        patches = (patches + self.pos_embedding[:, : patches.size(1), :]) # [B, N_patches, C]\n",
        "        # Transform patches\n",
        "        transformed_patches = self.latent_transformer(patches)  # [B, N_patches, C]\n",
        "        return transformed_patches\n",
        "\n",
        "ByteLatentTransformer(vocab_size,\n",
        "        n_embd,\n",
        "        n_head,\n",
        "        n_layers_encoder,\n",
        "        n_layers_latent,\n",
        "        dropout,\n",
        "        patch_size,\n",
        "        hash_sizes,\n",
        "        hash_table_size,\n",
        "        block_size,\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mM3g-2rbPDoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Kitsunp/Prueba-de-modelo-de-ByteLatentTransformer blt_model.py\n",
        "# https://github.com/Kitsunp/Prueba-de-modelo-de-ByteLatentTransformer/blob/main/blt_model.py\n",
        "\n",
        "import torch\n",
        "torch.backends.cuda.enable_flash_sdp(enabled=True)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, List\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Capa de encoder que combina self-attention, cross-attention (opcional)\n",
        "    y feed-forward.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(config)\n",
        "        self.cross_attn = CrossAttention(config)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.dropout = nn.Dropout(config.resid_dropout)\n",
        "\n",
        "    def forward(self, x, cross_context=None, self_mask=None, cross_mask=None, positions=None):\n",
        "        # print(\"\\n[EncoderLayer] - Input shape:\", x.shape)\n",
        "        h = x + self.self_attn(x, mask=self_mask, positions=positions, is_causal=False)\n",
        "        # print(\"[EncoderLayer] - After Self-Attn shape:\", h.shape)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        if cross_context is not None:\n",
        "            h = h + self.cross_attn(h, cross_context, cross_mask)\n",
        "            # print(\"[EncoderLayer] - After Cross-Attn shape:\", h.shape)\n",
        "            h = self.dropout(h)\n",
        "\n",
        "        out = h + self.feed_forward(h)\n",
        "        # print(\"[EncoderLayer] - After FeedForward shape:\", out.shape)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Capa de decoder con cross-attention, self-attention con enmascaramiento causal\n",
        "    y feed-forward.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.cross_attn = CrossAttention(config)\n",
        "        self.self_attn = MultiHeadAttention(config)\n",
        "        self.feed_forward = FeedForward(config)\n",
        "        self.dropout = nn.Dropout(config.resid_dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, self_mask=None, cross_mask=None, positions=None):\n",
        "        # print(\"\\n[DecoderLayer] - Input shape:\", x.shape)\n",
        "        h = x + self.cross_attn(x, encoder_output, cross_mask)\n",
        "        # print(\"[DecoderLayer] - After Cross-Attn shape:\", h.shape)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        h = h + self.self_attn(h, self_mask, positions, is_causal=True)\n",
        "        # print(\"[DecoderLayer] - After Self-Attn shape:\", h.shape)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        out = h + self.feed_forward(h)\n",
        "        # print(\"[DecoderLayer] - After FeedForward shape:\", out.shape)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                          EMBEDDINGS A NIVEL DE BYTE\n",
        "# =============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ByteEmbedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Inicializa la capa ByteEmbedding.\n",
        "\n",
        "        Args:\n",
        "            config (Namespace): Objeto de configuración que debe contener:\n",
        "                - hidden_size (int): Dimensión de los embeddings\n",
        "                - ngram_vocab_size (int): Tamaño del vocabulario para cada n-grama\n",
        "                - resid_dropout (float): Tasa de dropout para embeddings y gating\n",
        "                - noise_std (float): Desviación estándar del ruido Gaussiano\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding para bytes individuales (256 posibles valores)\n",
        "        self.byte_embeddings = nn.Embedding(256, config.hidden_size)\n",
        "\n",
        "        # Lista de embeddings para n-gramas de tamaño 3 a 8\n",
        "        self.ngram_hash_embeddings = nn.ModuleList([nn.Embedding(config.ngram_vocab_size, config.hidden_size) for _ in range(6)])\n",
        "\n",
        "        # Proyecciones lineales para después de la concatenación\n",
        "        self.projections = nn.ModuleList([nn.Linear(config.hidden_size * 2, config.hidden_size) for _ in range(6)])\n",
        "\n",
        "        # Normalización por capa aplicada al final\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
        "\n",
        "        self.noise_scale = nn.Parameter(torch.tensor(0.09))\n",
        "\n",
        "        # Parámetros aprendibles de gating para cada tamaño de n-grama (3 a 8)\n",
        "        self.ngram_gates = nn.Parameter(torch.ones(6))\n",
        "\n",
        "        # Gates residuales aprendibles\n",
        "        self.residual_gates = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(config.hidden_size * 2, config.hidden_size),\n",
        "                nn.LayerNorm(config.hidden_size), nn.GELU(), nn.Linear(config.hidden_size, 1),\n",
        "                nn.Sigmoid()\n",
        "            ) for _ in range(6)])\n",
        "\n",
        "        # Dropouts\n",
        "        self.dropout = nn.Dropout(config.resid_dropout)  # Dropout en embeddings base\n",
        "        self.gate_dropout = nn.Dropout(config.resid_dropout)  # Dropout en gates estocásticos\n",
        "        self.residual_dropout = nn.Dropout(config.resid_dropout)  # Dropout en embeddings expandidos\n",
        "        self.residual_gate_dropout = nn.Dropout(config.resid_dropout)  # Dropout en gates residuales\n",
        "        self.projection_dropout = nn.Dropout(config.resid_dropout)  # Dropout post-proyección\n",
        "\n",
        "    def compute_ngram_hash(self, bytes_sequence, n):\n",
        "        \"\"\"\n",
        "        Calcula índices de hash para cada n-grama en la secuencia.\n",
        "\n",
        "        Implementa un esquema de hashing que:\n",
        "        1. Extrae n-gramas consecutivos\n",
        "        2. Calcula hashes usando pesos exponenciales\n",
        "        3. Aplica factores de escala adaptativos\n",
        "        4. Utiliza offsets primos para mejor distribución\n",
        "\n",
        "        Args:\n",
        "            bytes_sequence (torch.Tensor): Tensor de bytes [batch_size, seq_length]\n",
        "            n (int): Tamaño del n-grama (3-8)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Índices de hash [batch_size, seq_length - n + 1]\n",
        "        \"\"\"\n",
        "        device = bytes_sequence.device\n",
        "        batch_size, seq_length = bytes_sequence.shape\n",
        "\n",
        "        if seq_length < n:\n",
        "            return torch.empty((batch_size, 0), dtype=torch.long, device=device)\n",
        "\n",
        "        # Extraer n-gramas: [B, (seq_length - n + 1), n]\n",
        "        ngrams = bytes_sequence.unfold(dimension=1, size=n, step=1)\n",
        "\n",
        "        # Factores adaptativos basados en n\n",
        "        scale_factor = 1.0 - (n - 3) * 0.1  # Decrece con n\n",
        "        offset = ((n - 3) * 37) % 256  # Offset primo\n",
        "\n",
        "        # Pesos exponenciales para el hash\n",
        "        exponents = torch.arange(n, device=device).float()\n",
        "        weights = (256 ** exponents).unsqueeze(0).unsqueeze(0)  # [1,1,n]\n",
        "\n",
        "        # Cálculo del hash\n",
        "        hash_values = (ngrams.float() * weights).sum(dim=-1)  # [B, seq_length-n+1]\n",
        "        hash_values = hash_values * scale_factor + offset\n",
        "        hash_values = hash_values.long()\n",
        "\n",
        "        # Ajustar al tamaño del vocabulario\n",
        "        vocab_size = self.ngram_hash_embeddings[n-3].num_embeddings\n",
        "        return hash_values % vocab_size\n",
        "\n",
        "    def forward(self, bytes_input):\n",
        "        \"\"\"\n",
        "        Procesa la secuencia de bytes para generar embeddings enriquecidos.\n",
        "\n",
        "        El proceso incluye:\n",
        "        1. Generación de embeddings base\n",
        "        2. Procesamiento de n-gramas\n",
        "        3. Aplicación de gates estocásticos y residuales\n",
        "        4. Concatenación y proyección de embeddings\n",
        "        5. Normalización final\n",
        "\n",
        "        Args:\n",
        "            bytes_input (torch.Tensor): Tensor de bytes [batch_size, seq_length]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Embeddings procesados [batch_size, seq_length, hidden_size]\n",
        "        \"\"\"\n",
        "        device = bytes_input.device\n",
        "        batch_size, seq_length = bytes_input.shape\n",
        "\n",
        "        # Embeddings base\n",
        "        embeds = self.byte_embeddings(bytes_input).float()\n",
        "        embeds = self.dropout(embeds)\n",
        "\n",
        "        # Escala de ruido adaptativa\n",
        "        current_noise_scale = torch.sigmoid(self.noise_scale)\n",
        "\n",
        "        # Procesamiento de n-gramas\n",
        "        for i, n in enumerate(range(3, 9)):\n",
        "            if seq_length < n:\n",
        "                continue\n",
        "\n",
        "            # Generar embeddings de n-gramas\n",
        "            ngram_hashes = self.compute_ngram_hash(bytes_input, n)\n",
        "            ngram_embeds = self.ngram_hash_embeddings[i](ngram_hashes)\n",
        "\n",
        "            # Gate estocástico con ruido\n",
        "            noise = torch.randn_like(self.ngram_gates[i]) * current_noise_scale\n",
        "            alpha_n = self.ngram_gates[i] + noise\n",
        "            alpha_n = self.gate_dropout(alpha_n)\n",
        "            alpha_n = torch.relu(alpha_n)\n",
        "\n",
        "            # Aplicar gate y normalización\n",
        "            gated_embeds = (ngram_embeds / n) * alpha_n\n",
        "\n",
        "            # Gate residual\n",
        "            gate_input = torch.cat([embeds[:, :seq_length - n + 1], gated_embeds], dim=-1)\n",
        "            gate_input = self.residual_gate_dropout(gate_input)\n",
        "            residual_gate = self.residual_gates[i](gate_input)\n",
        "            residual_gate = self.residual_gate_dropout(residual_gate)\n",
        "\n",
        "            # Concatenar y proyectar\n",
        "            concatenated = torch.cat([\n",
        "                embeds[:, :seq_length - n + 1],\n",
        "                gated_embeds * residual_gate\n",
        "            ], dim=-1)\n",
        "\n",
        "            projected = self.projections[i](concatenated)\n",
        "            embeds[:, :seq_length - n + 1, :] = projected\n",
        "        # Normalización final\n",
        "        embeds = self.layer_norm(embeds)\n",
        "\n",
        "        return embeds\n",
        "\n",
        "# =============================================================================\n",
        "#                        MODELOS DE ENCODER Y DECODER\n",
        "# =============================================================================\n",
        "\n",
        "class LocalEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder local que procesa los bytes de forma detallada.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.byte_embeddings = ByteEmbedding(config)\n",
        "        self.embedding_dropout = nn.Dropout(config.resid_dropout)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.dropout = nn.Dropout(config.resid_dropout)\n",
        "\n",
        "    def forward(self, bytes_input, patch_boundaries=None):\n",
        "        # print(\"\\n[LocalEncoder] - Input shape:\", bytes_input.shape)\n",
        "        h = self.byte_embeddings(bytes_input)\n",
        "        # print(\"[LocalEncoder] - After ByteEmbedding shape:\", h.shape)\n",
        "        h = self.embedding_dropout(h)\n",
        "\n",
        "        positions = torch.arange(bytes_input.size(1), device=bytes_input.device)\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            # print(f\"[LocalEncoder] - Passing through EncoderLayer {idx}\")\n",
        "            h = layer(h, positions=positions)\n",
        "            # print(f\"[LocalEncoder] - EncoderLayer {idx} output shape:\", h.shape)\n",
        "            h = self.dropout(h)\n",
        "        # print(\"[LocalEncoder] - Final output shape:\", h.shape)\n",
        "        return h\n",
        "\n",
        "class GlobalTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Procesa la información a nivel de parches con atención global.\n",
        "    Optimizado internamente para reducir redundancias y uso de VRAM,\n",
        "    SIN añadir nuevas caches ni eliminar la lógica principal.\n",
        "    Conserva la misma interfaz y atributos para compatibilidad.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.expansion_rate = getattr(config, 'expansion_rate', 2)\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        #  Submódulos principales: (se mantienen igual para compatibilidad)\n",
        "        # ------------------------------------------------------------\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(config) for _ in range(config.global_layers)\n",
        "        ])\n",
        "\n",
        "        # Sistemas de Dropout\n",
        "        self.dropout = nn.Dropout(config.resid_dropout)\n",
        "        self.adaptive_dropout = nn.Dropout(0.0855)\n",
        "        self.gate_dropout = nn.Dropout(0.0855)\n",
        "        self.mem_dropout = nn.Dropout(0.0855)\n",
        "        self.skip_dropout = nn.Dropout(0.0855)\n",
        "\n",
        "        # Normalizaciones\n",
        "        self.layer_norms = nn.ModuleList([\n",
        "            RMSNorm(config.hidden_size, eps=1e-6)\n",
        "            for _ in range(config.global_layers)\n",
        "        ])\n",
        "        self.input_norm = RMSNorm(config.hidden_size, eps=1e-6)\n",
        "        self.output_norm = RMSNorm(config.hidden_size, eps=1e-6)\n",
        "\n",
        "        # Normalizaciones específicas\n",
        "        self.pre_width_norm = RMSNorm(config.hidden_size, eps=1e-6)\n",
        "        self.post_width_norm = (\n",
        "            RMSNorm(config.n_states, eps=1e-6) if hasattr(config, 'n_states') else None\n",
        "        )\n",
        "        self.post_alpha_norm = (\n",
        "            RMSNorm(config.n_states, eps=1e-6) if hasattr(config, 'n_states') else None\n",
        "        )\n",
        "        self.pre_gate_norm = RMSNorm(config.hidden_size, eps=1e-6)\n",
        "        self.post_laurel_norm = RMSNorm(config.hidden_size, eps=1e-6)\n",
        "        self.pre_memory_norm = RMSNorm(config.hidden_size, eps=1e-6)\n",
        "        self.post_memory_norm = RMSNorm(config.hidden_size, eps=1e-6)\n",
        "        self.post_combined_norm = RMSNorm(config.hidden_size, eps=1e-6)\n",
        "\n",
        "        # Skip Gates\n",
        "        self.skip_gates = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(config.hidden_size * 2, config.hidden_size),\n",
        "                nn.Dropout(0.0855),\n",
        "                nn.Sigmoid()\n",
        "            ) for _ in range(config.global_layers)\n",
        "        ])\n",
        "\n",
        "        # LAUREL\n",
        "        self.laurel_alphas = nn.Parameter(torch.ones(config.global_layers))\n",
        "        self.laurel_g = nn.ModuleList([\n",
        "            nn.Linear(config.hidden_size, config.hidden_size)\n",
        "            for _ in range(config.global_layers)\n",
        "        ])\n",
        "\n",
        "        # Pesos Adaptativos\n",
        "        self.adaptive_weights = nn.Parameter(torch.ones(config.global_layers))\n",
        "\n",
        "        # Hyper-Connections\n",
        "        self.hyper_static_beta = nn.Parameter(torch.ones(self.expansion_rate))\n",
        "\n",
        "        init_alpha0 = torch.zeros((config.global_layers, self.expansion_rate, 1))\n",
        "        for i in range(config.global_layers):\n",
        "            init_alpha0[i, i % self.expansion_rate, 0] = 1.0\n",
        "\n",
        "        self.hyper_static_alpha = nn.Parameter(\n",
        "            torch.cat([\n",
        "                init_alpha0,\n",
        "                torch.eye(self.expansion_rate).unsqueeze(0).repeat(config.global_layers, 1, 1)\n",
        "            ], dim=2)\n",
        "        )\n",
        "\n",
        "        hidden_size = config.hidden_size\n",
        "        self.hyper_dynamic_alpha_fn = nn.Parameter(\n",
        "            torch.zeros((config.global_layers, hidden_size, self.expansion_rate + 1))\n",
        "        )\n",
        "        self.hyper_dynamic_alpha_scale = nn.Parameter(\n",
        "            torch.ones(config.global_layers) * 0.01\n",
        "        )\n",
        "        self.hyper_dynamic_beta_fn = nn.Parameter(\n",
        "            torch.zeros((config.global_layers, hidden_size))\n",
        "        )\n",
        "        self.hyper_dynamic_beta_scale = nn.Parameter(\n",
        "            torch.ones(config.global_layers) * 0.01\n",
        "        )\n",
        "\n",
        "        # Memoria Jerárquica\n",
        "        self.hierarchical_mem = nn.Parameter(\n",
        "            torch.zeros(config.global_layers, config.hidden_size) + 1e-6\n",
        "        )\n",
        "        self.mem_gate = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size * 2, config.hidden_size),\n",
        "            nn.Dropout(0.085),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # Métodos internos para skip+laurel, hyper-connections y memoria\n",
        "    # Se reorganizan para optimizar la implementación, sin añadir caches.\n",
        "    # ----------------------------------------------------------------\n",
        "\n",
        "    def _apply_skip_and_laurel(self, x, residual, layer_output, layer_idx):\n",
        "        \"\"\"\n",
        "        Mezcla la salida de la capa (layer_output) con la entrada (x, residual)\n",
        "        mediante gates adaptativos (skip_gates) y el mecanismo LAUREL.\n",
        "        \"\"\"\n",
        "        # Normalizaciones fusionadas\n",
        "        norm_x = self.layer_norms[layer_idx](x) * 0.1\n",
        "        norm_r = self.layer_norms[layer_idx](residual) * 0.1\n",
        "\n",
        "        gx = self.pre_gate_norm(norm_x) * 0.1\n",
        "        gr = self.pre_gate_norm(norm_r) * 0.1\n",
        "\n",
        "        # Gate input\n",
        "        gate_input = torch.cat([gx, gr], dim=-1)  # (B, S, 2D)\n",
        "        gate_val = self.skip_gates[layer_idx](gate_input)  # Sigmoid\n",
        "        gate_val = self.gate_dropout(gate_val)\n",
        "\n",
        "        # Pesos adaptativos\n",
        "        aw = torch.sigmoid(self.adaptive_weights[layer_idx]) * 0.1\n",
        "        aw = aw.view(1, 1, 1)\n",
        "        weighted_r = aw * gate_val * norm_r\n",
        "        weighted_r = self.skip_dropout(weighted_r)\n",
        "\n",
        "        # LAUREL\n",
        "        alpha = torch.sigmoid(self.laurel_alphas[layer_idx]) * 0.1\n",
        "        alpha = self.adaptive_dropout(alpha)\n",
        "\n",
        "        g_x = self.laurel_g[layer_idx](x) * 0.1\n",
        "        g_x = self.skip_dropout(g_x)\n",
        "\n",
        "        laurel_out = layer_output * alpha + g_x\n",
        "        laurel_out = self.post_laurel_norm(laurel_out) * 0.1\n",
        "\n",
        "        combined_output = laurel_out + weighted_r\n",
        "        return combined_output\n",
        "\n",
        "    def _apply_hyper_connections(self, x, hyper_h, layer_idx):\n",
        "        \"\"\"\n",
        "        Mezcla la señal x con el tensor hyper_h a través de matrices alpha y beta,\n",
        "        manteniendo la lógica original y sin añadir cache externo.\n",
        "        \"\"\"\n",
        "        # Normalizar hyper_h\n",
        "        norm_h = self.layer_norms[layer_idx](hyper_h)\n",
        "        B, S, N, D = norm_h.shape  # N = n_states (expansion_rate)\n",
        "\n",
        "        # Flatten + pre_width_norm\n",
        "        h_flat = norm_h.reshape(B * S * N, D)\n",
        "        h_flat = self.pre_width_norm(h_flat)\n",
        "\n",
        "        # dynamic alpha\n",
        "        alpha_fn = self.hyper_dynamic_alpha_fn[layer_idx][:, :N]  # (D, N)\n",
        "        wc_weight = torch.matmul(h_flat, alpha_fn)\n",
        "\n",
        "        if self.post_width_norm is not None:\n",
        "            wc_weight = self.post_width_norm(wc_weight.reshape(-1, N))\n",
        "        wc_weight = wc_weight.reshape(B, S, N, N)\n",
        "        wc_weight = torch.tanh(wc_weight)\n",
        "\n",
        "        alpha_scale = self.hyper_dynamic_alpha_scale[layer_idx].view(1, 1, 1, 1)\n",
        "        dynamic_alpha = wc_weight * alpha_scale\n",
        "\n",
        "        static_alpha = self.hyper_static_alpha[layer_idx][:N, :N]\n",
        "        static_alpha = static_alpha.view(1, 1, N, N).expand(B, S, -1, -1)\n",
        "        alpha = dynamic_alpha + static_alpha\n",
        "\n",
        "        if self.post_alpha_norm is not None:\n",
        "            alpha_view = alpha.reshape(-1, N)\n",
        "            alpha_view = self.post_alpha_norm(alpha_view)\n",
        "            alpha = alpha_view.reshape(B, S, N, N)\n",
        "\n",
        "        # dynamic beta\n",
        "        beta_fn = self.hyper_dynamic_beta_fn[layer_idx]  # (D,)\n",
        "        dc_weight = torch.matmul(norm_h, beta_fn.view(-1, 1)).squeeze(-1)\n",
        "        dc_weight = torch.tanh(dc_weight)\n",
        "\n",
        "        beta_scale = self.hyper_dynamic_beta_scale[layer_idx].view(1, 1, 1)\n",
        "        dynamic_beta = dc_weight * beta_scale\n",
        "\n",
        "        static_beta = self.hyper_static_beta[:N].view(1, 1, -1)\n",
        "        beta = dynamic_beta + static_beta\n",
        "\n",
        "        # Mezcla final\n",
        "        # alpha: (B,S,N,N), hyper_h: (B,S,N,D)\n",
        "        mix_h = torch.matmul(alpha, hyper_h)\n",
        "\n",
        "        x_expanded = x.unsqueeze(2).expand(-1, -1, N, -1)\n",
        "        depth_conn = x_expanded * beta.unsqueeze(-1)\n",
        "\n",
        "        return mix_h + depth_conn\n",
        "\n",
        "    def _apply_hierarchical_memory(self, x, layer_idx, batch_size):\n",
        "        \"\"\"\n",
        "        Integra la memoria jerárquica de la capa layer_idx,\n",
        "        manteniendo la lógica original sin añadir cachés.\n",
        "        \"\"\"\n",
        "        # Sumar offset y reescalar\n",
        "        x = (x + 1e-6) * 0.1\n",
        "        x = self.pre_memory_norm(x) + 1e-6\n",
        "\n",
        "        # Extraer y expandir la memoria para esta capa\n",
        "        mem = self.hierarchical_mem[layer_idx:layer_idx+1] + 1e-6\n",
        "        mem = mem.unsqueeze(0).expand(batch_size, x.size(1), -1)\n",
        "        mem = self.mem_dropout(mem)\n",
        "\n",
        "        # Calcular gate\n",
        "        mem_input = torch.cat([x, mem], dim=-1)\n",
        "        mem_gate_val = self.mem_gate(mem_input) * 0.1 + 1e-6\n",
        "\n",
        "        memory_output = mem_gate_val * mem + 1e-6\n",
        "        memory_output = self.post_memory_norm(memory_output) * 0.1 + 1e-6\n",
        "        memory_output = self.mem_dropout(memory_output) * 0.1\n",
        "\n",
        "        result = x + memory_output\n",
        "        if torch.isnan(result).any():\n",
        "            result = torch.nan_to_num(result, nan=1e-6)\n",
        "        return result\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    #  Forward principal: igual firma y pasos, sin añadir caches extras\n",
        "    # ----------------------------------------------------------------\n",
        "\n",
        "    def forward(self,\n",
        "                patch_embeddings: torch.Tensor,\n",
        "                attention_mask: Optional[torch.Tensor] = None\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Mismo forward y firma, sin cache adicional.\n",
        "        \"\"\"\n",
        "        batch_size = patch_embeddings.size(0)\n",
        "\n",
        "        # Normalización de entrada + dropout\n",
        "        h = self.input_norm(patch_embeddings)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        # Expansión \"virtual\" para hyper-connections\n",
        "        hyper_h = h.unsqueeze(2).expand(-1, -1, self.expansion_rate, -1)\n",
        "        positions = torch.arange(patch_embeddings.size(1), device=patch_embeddings.device)\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            # Paso por la capa\n",
        "            prev_h = self.dropout(h)\n",
        "            layer_out = layer(h, self_mask=attention_mask, positions=positions)\n",
        "\n",
        "            # Combinación Skip & Laurel\n",
        "            combined_out = self._apply_skip_and_laurel(prev_h, prev_h, layer_out, idx)\n",
        "\n",
        "            # Hyper-connections\n",
        "            hyper_h = self._apply_hyper_connections(combined_out, hyper_h, idx)\n",
        "\n",
        "            # Suma con la media de hyper_h\n",
        "            combined_features = combined_out + hyper_h.mean(dim=2)\n",
        "            combined_features = self.post_combined_norm(combined_features)\n",
        "\n",
        "            # Memoria jerárquica\n",
        "            h = self._apply_hierarchical_memory(combined_features, idx, batch_size)\n",
        "            h = self.dropout(h)\n",
        "\n",
        "        # Normalización de salida + dropout\n",
        "        h = self.output_norm(h)\n",
        "        h = self.dropout(h)\n",
        "        return h\n",
        "class LocalDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder local que reconvierte las representaciones latentes en logits de bytes.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])\n",
        "        self.byte_predictor = nn.Linear(config.hidden_size, 256)\n",
        "        self.dropout = nn.Dropout(config.resid_dropout)\n",
        "\n",
        "    def forward(self, encoded_bytes, global_output, byte_mask=None, cross_mask=None):\n",
        "        # print(\"\\n[LocalDecoder] - Input encoded_bytes shape:\", encoded_bytes.shape)\n",
        "        h = encoded_bytes\n",
        "        positions = torch.arange(encoded_bytes.size(1), device=encoded_bytes.device)\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            # print(f\"[LocalDecoder] - Passing through DecoderLayer {idx}\")\n",
        "            h = layer(h, global_output, self_mask=byte_mask, cross_mask=cross_mask, positions=positions)\n",
        "            # print(f\"[LocalDecoder] - DecoderLayer {idx} output shape:\", h.shape)\n",
        "            h = self.dropout(h)\n",
        "        logits = self.byte_predictor(h)\n",
        "        # print(\"[LocalDecoder] - Final logits shape:\", logits.shape)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                    MODELO DE ENTROPÍA (SIN USO DE CACHÉ)\n",
        "# =============================================================================\n",
        "class EntropyLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo de lenguaje basado en entropía dual con análisis local y global.\n",
        "\n",
        "    Este modelo implementa un sistema de entropía dual que:\n",
        "    1. Analiza patrones locales en ventanas pequeñas.\n",
        "    2. Captura contexto global en una dimensión reducida.\n",
        "    3. Combina ambas medidas de forma adaptativa mediante pesos aprendidos.\n",
        "\n",
        "    El modelo utiliza skip connections y gates aprendibles para mejorar\n",
        "    la eficiencia y el flujo de información, sin necesidad de cache adicional.\n",
        "    \"\"\"\n",
        "\n",
        "    # ========================================================\n",
        "    #                SUBMÓDULOS INTERNOS\n",
        "    # ========================================================\n",
        "    class AdaptiveWaveletLayer(nn.Module):\n",
        "        \"\"\"\n",
        "        Capa optimizada de análisis global usando wavelets neuronales adaptativos.\n",
        "        Implementa procesamiento por lotes vectorizado sin uso de caché.\n",
        "        \"\"\"\n",
        "        def __init__(self, hidden_size, global_size, num_wavelets=8, dropout=0.1, chunk_size=1024):\n",
        "            super().__init__()\n",
        "            self.hidden_size = hidden_size\n",
        "            self.global_size = global_size\n",
        "            self.num_wavelets = num_wavelets\n",
        "            self.chunk_size = chunk_size\n",
        "\n",
        "            # Wavelets optimizados para procesamiento por lotes\n",
        "            self.mother_wavelets = nn.Parameter(\n",
        "                torch.randn(1, num_wavelets, hidden_size, 1) * 0.02\n",
        "            )\n",
        "\n",
        "            # Escalas con broadcasting optimizado\n",
        "            self.scales = nn.Parameter(torch.ones(1, num_wavelets, 1, 1))\n",
        "\n",
        "            # Mixer optimizado con menos parámetros y mejor regularización\n",
        "            self.coeff_mixer = nn.Sequential(\n",
        "                nn.Linear(num_wavelets, hidden_size // 2),\n",
        "                nn.LayerNorm(hidden_size // 2), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden_size // 2, hidden_size)\n",
        "            )\n",
        "\n",
        "            # Proyección con skip connection residual\n",
        "            self.output_proj = nn.Sequential(\n",
        "                nn.LayerNorm(hidden_size), nn.Linear(hidden_size, global_size), nn.Dropout(dropout)\n",
        "            )\n",
        "\n",
        "            # MultiheadAttention optimizada\n",
        "            self.num_heads = 4\n",
        "            self.head_dim = hidden_size // self.num_heads\n",
        "            self.scale = self.head_dim ** -0.5\n",
        "\n",
        "            # Proyecciones para Q, K, V\n",
        "            self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "            self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "            self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "            self.out_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "            self.attention_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        def _scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "            \"\"\"Implementación optimizada de atención sin caché.\"\"\"\n",
        "            attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "            if mask is not None:\n",
        "                attn_weights = attn_weights.masked_fill(mask, float('-inf'))\n",
        "\n",
        "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "            attn_weights = self.attention_dropout(attn_weights)\n",
        "\n",
        "            return torch.matmul(attn_weights, v)\n",
        "\n",
        "        def _attention_forward(self, x):\n",
        "            \"\"\"Forward pass de atención optimizado sin caché.\"\"\"\n",
        "            batch_size, seq_len, _ = x.shape\n",
        "\n",
        "            # Proyectar Q, K, V y asegurar que sean contiguos\n",
        "            q = self.q_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).contiguous()\n",
        "            k = self.k_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).contiguous()\n",
        "            v = self.v_proj(x).reshape(batch_size, seq_len, self.num_heads, self.head_dim).contiguous()\n",
        "\n",
        "            # Transponer después de asegurar que son contiguos\n",
        "            q = q.transpose(1, 2)\n",
        "            k = k.transpose(1, 2)\n",
        "            v = v.transpose(1, 2)\n",
        "\n",
        "            # Generar máscara causal on-the-fly\n",
        "            if self.training:\n",
        "                mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device, dtype=torch.bool), diagonal=1).unsqueeze(0).unsqueeze(0)\n",
        "            else:\n",
        "                mask = None\n",
        "\n",
        "            # Calcular atención\n",
        "            attn_output = self._scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "            # Reorganizar y proyectar salida\n",
        "            attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "            return self.out_proj(attn_output)\n",
        "\n",
        "        def _compute_wavelet_coeffs_batched(self, x):\n",
        "            \"\"\"Calcula coeficientes wavelet de forma vectorizada para todo el batch.\"\"\"\n",
        "            batch_size, seq_len, channels = x.shape\n",
        "\n",
        "            # Normalizar wavelets on-the-fly\n",
        "            normalized_wavelets = F.normalize(self.mother_wavelets, dim=2)\n",
        "            scaled_wavelets = normalized_wavelets * torch.sigmoid(self.scales)\n",
        "\n",
        "            # Reorganizar para la convolución y asegurar que sea contiguo\n",
        "            x_reshaped = x.reshape(batch_size * seq_len, 1, channels).contiguous()\n",
        "\n",
        "            # Preparar wavelets para convolución\n",
        "            wavelet_kernels = scaled_wavelets.squeeze(0).transpose(1, 2).contiguous()\n",
        "\n",
        "            # Convolución vectorizada para todos los wavelets simultáneamente\n",
        "            coeffs = F.conv1d(x_reshaped, wavelet_kernels, groups=1)\n",
        "\n",
        "            return coeffs.reshape(batch_size, seq_len, self.num_wavelets)\n",
        "\n",
        "        def _process_chunk(self, x_chunk):\n",
        "            \"\"\"Procesa un chunk de la secuencia.\"\"\"\n",
        "            # Calcular coeficientes wavelet\n",
        "            coeffs = self._compute_wavelet_coeffs_batched(x_chunk)\n",
        "\n",
        "            # Mezclar coeficientes\n",
        "            mixed = self.coeff_mixer(coeffs)\n",
        "\n",
        "            # Aplicar atención\n",
        "            attn_output = self._attention_forward(mixed)\n",
        "\n",
        "            return self.output_proj(attn_output + mixed)\n",
        "\n",
        "        def forward(self, x):\n",
        "            \"\"\"Forward pass optimizado con procesamiento por chunks.\"\"\"\n",
        "            batch_size, seq_len, _ = x.shape\n",
        "\n",
        "            # Para secuencias cortas, procesar directamente\n",
        "            if seq_len <= self.chunk_size:\n",
        "                return self._process_chunk(x)\n",
        "\n",
        "            # Para secuencias largas, procesar por chunks\n",
        "            outputs = []\n",
        "            for start in range(0, seq_len, self.chunk_size):\n",
        "                end = min(start + self.chunk_size, seq_len)\n",
        "                chunk = x[:, start:end, :]\n",
        "                chunk_output = self._process_chunk(chunk)\n",
        "                outputs.append(chunk_output)\n",
        "\n",
        "            # Concatenar resultados\n",
        "            return torch.cat(outputs, dim=1)\n",
        "\n",
        "    class CrossAttention(nn.Module):\n",
        "        \"\"\"\n",
        "        Atención cruzada optimizada para interacción bidireccional entre características\n",
        "        locales y globales. Mantiene la precisión en la detección de patrones mientras\n",
        "        mejora la eficiencia computacional.\n",
        "        \"\"\"\n",
        "        def __init__(self, hidden_size, global_size, num_heads, dropout):\n",
        "            super().__init__()\n",
        "            self.hidden_size = hidden_size\n",
        "            self.global_size = global_size\n",
        "            self.num_heads = num_heads\n",
        "            self.dropout = dropout\n",
        "\n",
        "            # Escalado para estabilidad numérica\n",
        "            self.local_scale = (global_size // (num_heads // 2 if num_heads > 1 else 1)) ** -0.5\n",
        "            self.global_scale = (hidden_size // num_heads) ** -0.5\n",
        "\n",
        "            # Proyecciones optimizadas sin bias para reducir parámetros\n",
        "            # pero manteniendo capacidad representativa\n",
        "            self.local2global = nn.Linear(hidden_size, global_size, bias=True)\n",
        "            self.global2local = nn.Linear(global_size, hidden_size, bias=True)\n",
        "\n",
        "            # Proyecciones Q,K,V para atención local->global\n",
        "            self.local_to_global_qkv = nn.ModuleDict({\n",
        "                'q': nn.Linear(global_size, global_size, bias=True),\n",
        "                'k': nn.Linear(global_size, global_size, bias=True),\n",
        "                'v': nn.Linear(global_size, global_size, bias=True)\n",
        "            })\n",
        "\n",
        "            # Proyecciones Q,K,V para atención global->local\n",
        "            self.global_to_local_qkv = nn.ModuleDict({\n",
        "                'q': nn.Linear(hidden_size, hidden_size, bias=True),\n",
        "                'k': nn.Linear(hidden_size, hidden_size, bias=True),\n",
        "                'v': nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "            })\n",
        "\n",
        "            # Proyecciones de salida\n",
        "            self.local_out = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "            self.global_out = nn.Linear(global_size, global_size, bias=False)\n",
        "\n",
        "            # Dropouts estratégicos\n",
        "            self.attn_dropout = nn.Dropout(dropout)\n",
        "            self.proj_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        def _scaled_dot_product_attention(self, q, k, v, mask=None, scale=None):\n",
        "            \"\"\"\n",
        "            Implementación optimizada de atención que usa flash attention cuando está disponible.\n",
        "            \"\"\"\n",
        "            if hasattr(F, 'scaled_dot_product_attention'):\n",
        "                # Usar Flash Attention si está disponible\n",
        "                attn_output = F.scaled_dot_product_attention(\n",
        "                    q, k, v,\n",
        "                    attn_mask=mask,\n",
        "                    dropout_p=self.dropout if self.training else 0.0,\n",
        "                    is_causal=False\n",
        "                )\n",
        "            else:\n",
        "                # Implementación estándar optimizada\n",
        "                scores = torch.matmul(q, k.transpose(-2, -1)) * (scale or 1.0)\n",
        "\n",
        "                if mask is not None:\n",
        "                    scores = scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "                attn_weights = F.softmax(scores, dim=-1)\n",
        "                attn_weights = self.attn_dropout(attn_weights)\n",
        "                attn_output = torch.matmul(attn_weights, v)\n",
        "\n",
        "            return attn_output\n",
        "\n",
        "        def _process_attention(self, q, k, v, num_heads, head_dim, mask=None, scale=None):\n",
        "            \"\"\"\n",
        "            Procesa la atención manteniendo la precisión necesaria para detectar patrones.\n",
        "            \"\"\"\n",
        "            batch_size, seq_len, _ = q.shape\n",
        "\n",
        "            # Reshape preservando la información\n",
        "            q = q.reshape(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "            k = k.reshape(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "            v = v.reshape(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "            # Calcular atención\n",
        "            attn_output = self._scaled_dot_product_attention(q, k, v, mask, scale)\n",
        "\n",
        "            # Reshape de vuelta\n",
        "            return attn_output.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "\n",
        "        def forward(self, local_feats, global_feats, local_mask=None, global_mask=None):\n",
        "            \"\"\"\n",
        "            Forward pass optimizado que mantiene la precisión en la detección de patrones.\n",
        "\n",
        "            Args:\n",
        "                local_feats: [batch_size, seq_len, hidden_size]\n",
        "                global_feats: [batch_size, seq_len, global_size]\n",
        "                local_mask/global_mask: Máscaras opcionales para atención\n",
        "            \"\"\"\n",
        "            batch_size, seq_len, _ = local_feats.shape\n",
        "\n",
        "            # Proyección local->global\n",
        "            local_as_global = self.local2global(local_feats)\n",
        "\n",
        "            # Atención local->global\n",
        "            q_global = self.local_to_global_qkv['q'](global_feats)\n",
        "            k_global = self.local_to_global_qkv['k'](local_as_global)\n",
        "            v_global = self.local_to_global_qkv['v'](local_as_global)\n",
        "\n",
        "            global_head_dim = self.global_size // (self.num_heads // 2 if self.num_heads > 1 else 1)\n",
        "            global_attn = self._process_attention(\n",
        "                q_global, k_global, v_global,\n",
        "                num_heads=self.num_heads // 2 if self.num_heads > 1 else 1,\n",
        "                head_dim=global_head_dim,\n",
        "                mask=local_mask,\n",
        "                scale=self.local_scale\n",
        "            )\n",
        "            global_attn = self.global_out(global_attn)\n",
        "            global_attn = self.proj_dropout(global_attn)\n",
        "\n",
        "            # Proyección global->local\n",
        "            global_as_local = self.global2local(global_feats)\n",
        "\n",
        "            # Atención global->local\n",
        "            q_local = self.global_to_local_qkv['q'](local_feats)\n",
        "            k_local = self.global_to_local_qkv['k'](global_as_local)\n",
        "            v_local = self.global_to_local_qkv['v'](global_as_local)\n",
        "\n",
        "            local_head_dim = self.hidden_size // self.num_heads\n",
        "            local_attn = self._process_attention(\n",
        "                q_local, k_local, v_local,\n",
        "                num_heads=self.num_heads,\n",
        "                head_dim=local_head_dim,\n",
        "                mask=global_mask,\n",
        "                scale=self.global_scale\n",
        "            )\n",
        "            local_attn = self.local_out(local_attn)\n",
        "            local_attn = self.proj_dropout(local_attn)\n",
        "\n",
        "            # Actualización residual con dropout estratégico\n",
        "            local_updated = local_feats + self.proj_dropout(local_attn)\n",
        "            global_updated = global_feats + self.proj_dropout(global_attn)\n",
        "\n",
        "            return local_updated, global_updated\n",
        "\n",
        "    # ========================================================\n",
        "    #                INICIALIZACIÓN DE LA CLASE\n",
        "    # ========================================================\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size: int = 512,\n",
        "        global_size: int = 128,\n",
        "        num_layers: int = 2,\n",
        "        num_heads: int = 8,\n",
        "        context_size: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        learnable_dropout: float = 0.12,\n",
        "        max_seq_length: int = 2048,\n",
        "        window_size: int = 128,\n",
        "        vocab_size: int = 256  # Añadido para mayor generalidad\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.context_size = context_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.global_size = global_size\n",
        "        self.window_size = window_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # ========== EMBEDDING ================\n",
        "        self.byte_embedding = nn.Embedding(256, hidden_size)\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # ========== ANALISIS LOCAL =============\n",
        "        local_encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.local_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer=local_encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "            norm=nn.LayerNorm(hidden_size),\n",
        "            enable_nested_tensor=True\n",
        "        )\n",
        "\n",
        "        # ========== REDUCCION A GLOBAL (MEJORADA) ============\n",
        "        self.global_reduction = self.AdaptiveWaveletLayer(\n",
        "            hidden_size=hidden_size,\n",
        "            global_size=global_size,\n",
        "            num_wavelets=8,  # Ajustable según necesidades\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.global_reduction_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # ========== ANALISIS GLOBAL =============\n",
        "        global_encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=global_size,\n",
        "            nhead=num_heads // 2 if num_heads > 1 else 1,\n",
        "            dim_feedforward=global_size * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.global_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer=global_encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "            norm=nn.LayerNorm(global_size),\n",
        "            enable_nested_tensor=True\n",
        "        )\n",
        "\n",
        "        # ========== SKIP CONNECTIONS LOCALES =============\n",
        "        self.local_skip_weights = nn.ParameterList([\n",
        "            nn.Parameter(torch.ones(1, 1, hidden_size))\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.local_gates = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_size * 2, hidden_size), nn.Dropout(learnable_dropout), nn.Sigmoid()\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # ========== SKIP CONNECTIONS GLOBALES =============\n",
        "        self.global_skip_weights = nn.ParameterList([\n",
        "            nn.Parameter(torch.ones(1, 1, global_size))\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.global_gates = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(global_size * 2, global_size), nn.Dropout(learnable_dropout), nn.Sigmoid()\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # ========== DROPOUT PARA SKIP CONNECTIONS =============\n",
        "        self.skip_dropout = nn.Dropout(learnable_dropout)\n",
        "\n",
        "        # ========== ATENCION CRUZADA =============\n",
        "        self.cross_attention = self.CrossAttention(\n",
        "            hidden_size=hidden_size,\n",
        "            global_size=global_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # ========== PONDERACION ADAPTATIVA =============\n",
        "        self.weight_network = nn.Sequential(\n",
        "            nn.Linear(hidden_size + global_size, 1), nn.Dropout(dropout), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # ========== GATE FINAL Y SALIDA ============\n",
        "        self.output_gate = nn.Sequential(\n",
        "            nn.Linear((hidden_size + global_size) * 2, hidden_size + global_size), nn.Dropout(learnable_dropout), nn.Sigmoid()\n",
        "        )\n",
        "        self.output = nn.Linear(hidden_size + global_size, vocab_size)\n",
        "        self.output_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # ========== NORMALIZACIONES GENERALES =============\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.learnable_dropout = nn.Dropout(learnable_dropout)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        # ========== MASCARA CAUSAL PRE-GENERADA =============\n",
        "        self._initialize_mask_cache(max_seq_length)\n",
        "        self.base_smoothing = 0.1\n",
        "        self.smoothing_scale = nn.Parameter(torch.tensor(0.5))\n",
        "    # ========================================================\n",
        "    #              METODOS INTERNOS\n",
        "    # ========================================================\n",
        "    def _compute_adaptive_smoothing(self, logits: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calcula un factor de suavizado adaptativo basado en la confianza del modelo\n",
        "        y la temperatura dinámica del batch.\n",
        "\n",
        "        Este método:\n",
        "        1. Calcula un factor de suavizado base usando la probabilidad máxima\n",
        "        2. Ajusta la temperatura según la entropía del batch\n",
        "        3. Combina ambos efectos para suavizar adaptativamente las probabilidades\n",
        "\n",
        "        Args:\n",
        "            logits (torch.Tensor): Tensor de logits sin normalizar con shape [B, S, V]\n",
        "                donde B es batch_size, S es sequence_length, y V es vocab_size.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Factor de suavizado por posición con shape [B, S].\n",
        "                        Los valores están entre [0, base_smoothing].\n",
        "                        - Valores más altos indican más suavizado (alta confianza)\n",
        "                        - Valores más bajos indican menos suavizado (baja confianza)\n",
        "\n",
        "        Ejemplo:\n",
        "            Para una secuencia donde el modelo está:\n",
        "            - Muy seguro (p=0.9) → factor ≈ base_smoothing\n",
        "            - Moderadamente seguro (p=0.6) → factor ≈ 0.5 * base_smoothing\n",
        "            - Inseguro (p=0.3) → factor ≈ 0.1 * base_smoothing\n",
        "        \"\"\"\n",
        "        # 1. Obtener distribución de probabilidad base\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # 2. Calcular máxima probabilidad por posición [B, S]\n",
        "        max_probs = probs.max(dim=-1)[0]\n",
        "\n",
        "        # 3. Calcular entropía por posición y promedio del batch\n",
        "        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)\n",
        "        mean_entropy = entropy.mean()\n",
        "\n",
        "        # 4. Factor de temperatura basado en entropía\n",
        "        temperature = torch.sigmoid(mean_entropy)\n",
        "        temperature = temperature.clamp(0.1, 2.0)\n",
        "\n",
        "        # 5. Calcular factor adaptativo basado en confianza\n",
        "        confidence_factor = torch.sigmoid(self.smoothing_scale * (max_probs - 0.5))\n",
        "\n",
        "        # 6. Combinar efectos\n",
        "        # - Alta temperatura reduce el efecto del suavizado\n",
        "        # - Baja temperatura mantiene el suavizado basado en confianza\n",
        "        adaptive_factor = confidence_factor / temperature.unsqueeze(-1).expand_as(confidence_factor)\n",
        "\n",
        "        # 7. Escalar al rango final y aplicar límites\n",
        "        smoothing = (self.base_smoothing * adaptive_factor).clamp(0.0, self.base_smoothing)\n",
        "\n",
        "        return smoothing\n",
        "\n",
        "    def _initialize_mask_cache(self, max_seq_length: int):\n",
        "        masks = torch.triu(torch.ones(max_seq_length, max_seq_length), diagonal=1).bool()\n",
        "        self.register_buffer('cached_masks', masks, persistent=False)\n",
        "\n",
        "    def get_causal_mask(self, seq_length: int) -> torch.Tensor:\n",
        "        if seq_length <= self.cached_masks.size(0):\n",
        "            return self.cached_masks[:seq_length, :seq_length]\n",
        "        return torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
        "\n",
        "    def _apply_local_skip(self, x: torch.Tensor, skip: torch.Tensor, layer_idx: int) -> torch.Tensor:\n",
        "        weighted_skip = self.skip_dropout(self.local_skip_weights[layer_idx]) * skip\n",
        "        gate_input = torch.cat([x, weighted_skip], dim=-1)\n",
        "        gate = self.local_gates[layer_idx](gate_input)\n",
        "        out = x + self.learnable_dropout(gate * weighted_skip)\n",
        "        return out\n",
        "\n",
        "    def _apply_global_skip(self, x: torch.Tensor, skip: torch.Tensor, layer_idx: int) -> torch.Tensor:\n",
        "        weighted_skip = self.skip_dropout(self.global_skip_weights[layer_idx]) * skip\n",
        "        gate_input = torch.cat([x, weighted_skip], dim=-1)\n",
        "        gate = self.global_gates[layer_idx](gate_input)\n",
        "        out = x + self.learnable_dropout(gate * weighted_skip)\n",
        "        return out\n",
        "\n",
        "    # ========================================================\n",
        "    #                FORWARD PRINCIPAL\n",
        "    # ========================================================\n",
        "    def forward(self, input_bytes: torch.Tensor, return_probabilities: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Procesa la entrada aplicando análisis local y global mediante atención cruzada,\n",
        "        combinando características de diferentes niveles de abstracción.\n",
        "\n",
        "        Este método implementa el forward pass completo del modelo que:\n",
        "        1. Procesa los bytes de entrada mediante embeddings\n",
        "        2. Aplica análisis local y global en paralelo\n",
        "        3. Combina información mediante atención cruzada\n",
        "        4. Utiliza skip connections y gates adaptativos\n",
        "        5. Genera probabilidades o entropía según se requiera\n",
        "\n",
        "        Args:\n",
        "            input_bytes (torch.Tensor): Tensor de entrada con forma [batch_size, seq_length]\n",
        "                                    conteniendo índices de bytes.\n",
        "            return_probabilities (bool, opcional): Si True, retorna distribución de probabilidades\n",
        "                                                suavizada. Si False, retorna entropía.\n",
        "                                                Por defecto: False.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Si return_probabilities es True:\n",
        "                        - Tensor de probabilidades suavizadas con forma [batch_size, seq_length, vocab_size]\n",
        "                        Si return_probabilities es False:\n",
        "                        - Tensor de entropía con forma [batch_size, seq_length] o [batch_size, window_size]\n",
        "\n",
        "        Notas sobre el flujo de datos:\n",
        "            1. Embeddings: input_bytes -> x [B, S, H]\n",
        "            2. Análisis Local: x -> local_features [B, S, H]\n",
        "            3. Análisis Global: x -> global_x -> global_features [B, S, G]\n",
        "            4. Atención Cruzada: (local_features, global_features) -> (local_attn, global_attn)\n",
        "            5. Skip Connections: Mejora flujo de gradientes\n",
        "            6. Combinación Adaptativa: Mezcla características locales y globales\n",
        "            7. Gate Final: Controla flujo de información residual\n",
        "            8. Salida: Genera logits -> probabilidades/entropía\n",
        "\n",
        "        Donde:\n",
        "            B = batch_size\n",
        "            S = sequence_length\n",
        "            H = hidden_size\n",
        "            G = global_size\n",
        "        \"\"\"\n",
        "        # Asegurar que las dimensiones de entrada sean escalares\n",
        "        batch_size, seq_length = input_bytes.shape\n",
        "        seq_length = int(seq_length.item()) if isinstance(seq_length, torch.Tensor) else int(seq_length)\n",
        "        device = input_bytes.device\n",
        "\n",
        "        # ========= EMBEDDINGS INICIALES =========\n",
        "        x = self.byte_embedding(input_bytes)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        # ========= MÁSCARA CAUSAL =========\n",
        "        mask = self.get_causal_mask(seq_length).to(device)\n",
        "\n",
        "        # Usamos autocast para mezclar precisión (solo si usamos GPUs con AMP)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # ========= ANALISIS LOCAL =========\n",
        "            local_features = self.local_encoder(src=x, mask=mask, is_causal=True)\n",
        "\n",
        "            # ========= ANALISIS GLOBAL =========\n",
        "            # 1. Reducción MLP (captura no lineal)\n",
        "            global_x = self.global_reduction(x)\n",
        "            global_x = self.global_reduction_dropout(global_x)\n",
        "\n",
        "            # 2. Encoder Global\n",
        "            global_features = self.global_encoder(src=global_x, mask=mask, is_causal=True)\n",
        "\n",
        "            # ========= ATENCIÓN CRUZADA (Local <-> Global) =========\n",
        "            local_attn, global_attn = self.cross_attention(\n",
        "                local_feats=local_features,\n",
        "                global_feats=global_features,\n",
        "                local_mask=mask,       # Opcional, enmascarado local\n",
        "                global_mask=mask       # Opcional, enmascarado global\n",
        "            )\n",
        "\n",
        "            # ========= SKIP CONNECTIONS POR CAPA =========\n",
        "            for layer_idx in range(len(self.local_skip_weights)):\n",
        "                local_attn = self._apply_local_skip(local_attn, local_features, layer_idx)\n",
        "                global_attn = self._apply_global_skip(global_attn, global_features, layer_idx)\n",
        "\n",
        "            # ========= COMBINACIÓN ADAPTATIVA LOCAL-GLOBAL =========\n",
        "            combined_features = torch.cat([local_attn, global_attn], dim=-1)\n",
        "            weights = self.weight_network(combined_features)  # [B, S, 1]\n",
        "\n",
        "            weighted_local = local_attn * weights\n",
        "            weighted_global = global_attn * (1 - weights)\n",
        "            final_features = torch.cat([weighted_local, weighted_global], dim=-1)\n",
        "\n",
        "            # ========= CONCATENACION PARA GATE FINAL =========\n",
        "            initial_state = torch.cat([x, global_x], dim=-1)\n",
        "            final_gate_input = torch.cat([final_features, initial_state], dim=-1)\n",
        "            final_gate = self.output_gate(final_gate_input)\n",
        "            combined = final_features + self.learnable_dropout(final_gate * initial_state)\n",
        "\n",
        "            # ========= DROPOUT Y SALIDA =========\n",
        "            combined = self.output_dropout(combined)\n",
        "            logits = self.output(combined)\n",
        "            probabilities = F.softmax(logits, dim=-1, dtype=torch.float32)\n",
        "\n",
        "        if return_probabilities:\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Aplicar suavizado adaptativo\n",
        "            smoothing = self._compute_adaptive_smoothing(logits)\n",
        "            vocab_size = logits.size(-1)\n",
        "            smoothed_probs = probs * (1 - smoothing.unsqueeze(-1)) + smoothing.unsqueeze(-1) / vocab_size\n",
        "            return smoothed_probs\n",
        "\n",
        "        return self.compute_entropy(probabilities)\n",
        "\n",
        "    # ========================================================\n",
        "    #             CÁLCULO DE ENTROPÍA\n",
        "    # ========================================================\n",
        "    def compute_entropy(self, probabilities: torch.Tensor, use_sliding_window: bool = True) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calcula la entropía H(P) = -Σ p(x)*log2 p(x).\n",
        "\n",
        "        Si `use_sliding_window` es True y la longitud de la secuencia es mayor o igual a `window_size`,\n",
        "        utiliza ventanas deslizantes para calcular la entropía de forma local y luego promedia los resultados.\n",
        "        De lo contrario, calcula la entropía de manera global sobre toda la secuencia.\n",
        "\n",
        "        Args:\n",
        "            probabilities (torch.Tensor): Tensor de probabilidades con forma [B, S, V],\n",
        "                                        donde\n",
        "                                        B = tamaño del batch,\n",
        "                                        S = longitud de la secuencia,\n",
        "                                        V = número de categorías o variables.\n",
        "            use_sliding_window (bool, opcional): Indica si se debe usar una ventana deslizante para secuencias largas.\n",
        "                                                Por defecto es True.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor de entropía con forma [B, window_size] si se usa ventana deslizante,\n",
        "                        o [B, S] si se calcula globalmente.\n",
        "        \"\"\"\n",
        "        B, S, V = probabilities.shape\n",
        "        seq_length = S.item() if isinstance(S, torch.Tensor) else S\n",
        "\n",
        "        if use_sliding_window and seq_length >= self.window_size:\n",
        "            # Generar ventanas deslizantes\n",
        "            # La función `unfold` crea un nuevo tensor con ventanas deslizantes a lo largo de la dimensión de la secuencia.\n",
        "            # Parámetros:\n",
        "            #   dimension=1: dimensión de la secuencia.\n",
        "            #   size=self.window_size: tamaño de cada ventana.\n",
        "            #   step=max(1, self.window_size // 2): desplazamiento entre ventanas (mitad del tamaño de la ventana o al menos 1).\n",
        "            # Resultado: [B, num_windows, window_size, V]\n",
        "            windows = probabilities.unfold(1, self.window_size, max(1, self.window_size // 2))\n",
        "\n",
        "            # Reorganizar dimensiones para facilitar el cálculo vectorizado de la entropía\n",
        "            # Cambiar a [B, window_size, num_windows, V]\n",
        "            windows = windows.permute(0, 2, 1, 3)\n",
        "\n",
        "            # Cálculo vectorizado de la entropía por ventana\n",
        "            # Aplicamos log2 a las probabilidades, asegurando que no haya valores menores que 1e-10 para evitar log(0)\n",
        "            # Multiplicamos elemento a elemento por las probabilidades y sumamos sobre la última dimensión (V)\n",
        "            # Resultado: [B, window_size, num_windows]\n",
        "            ent = -torch.sum(windows * torch.log2(torch.clamp(windows, min=1e-10)), dim=-1)\n",
        "\n",
        "            # Promediar la entropía sobre el número de ventanas (dim=2)\n",
        "            # Resultado final: [B, window_size]\n",
        "            return ent.mean(dim=2)\n",
        "        else:\n",
        "            # Cálculo directo de la entropía sobre toda la secuencia\n",
        "            # Aplicamos log2 a las probabilidades, asegurando que no haya valores menores que 1e-10 para evitar log(0)\n",
        "            # Multiplicamos elemento a elemento por las probabilidades y sumamos sobre la última dimensión (V)\n",
        "            # Resultado: [B, S]\n",
        "            ent = -torch.sum(probabilities * torch.log2(torch.clamp(probabilities, min=1e-10)), dim=-1)\n",
        "            return ent\n",
        "# =============================================================================\n",
        "#                           BLT (Byte-Level Transformer)\n",
        "# =============================================================================\n",
        "\n",
        "class BLT(nn.Module):\n",
        "    \"\"\"\n",
        "    Byte-Level Transformer (BLT) con parcheo adaptativo optimizado,\n",
        "    SIN usar cache en ningún lugar.\n",
        "\n",
        "    Esta versión mejorada integra explícitamente las probabilidades\n",
        "    generadas por EntropyLM, con el fin de refinar la lógica de\n",
        "    segmentación de parches y aprovechar la información de confiabilidad.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            config: Objeto de configuración (BLTConfig o similar) que contiene:\n",
        "                - hidden_size: Dimensión oculta de los embeddings\n",
        "                - resid_dropout: Tasa de dropout residual\n",
        "                - attention_dropout: Tasa de dropout en la atención\n",
        "                - min_patch_size: Tamaño mínimo de parche\n",
        "                - max_patch_size: Tamaño máximo de parche\n",
        "                - initial_entropy_threshold: Valor inicial del umbral de entropía\n",
        "                - num_heads, etc.: Otros hiperparámetros relevantes\n",
        "                - (Opcional) param prob_factor: Factor para combinar entropía y prob.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # =================== Submódulos Principales ===================\n",
        "        self.local_encoder = LocalEncoder(config)\n",
        "        self.global_transformer = GlobalTransformer(config)\n",
        "        self.local_decoder = LocalDecoder(config)\n",
        "\n",
        "        # EntropyLM mejora: Se usará para obtener entropía y/o probabilidades\n",
        "        self.entropy_model = EntropyLM(\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_layers=config.entropy_model_layers,\n",
        "            num_heads=config.num_heads,\n",
        "            context_size=config.entropy_context_size,\n",
        "            dropout=config.attention_dropout\n",
        "        )\n",
        "\n",
        "        # Normalización y Dropout\n",
        "        self.global_norm = RMSNorm(config.hidden_size)\n",
        "        self.global_dropout = nn.Dropout(config.resid_dropout)\n",
        "\n",
        "        # Configuración de Parcheo\n",
        "        self.min_patch_size = config.min_patch_size\n",
        "        self.max_patch_size = config.max_patch_size\n",
        "\n",
        "        # Parámetros Aprendibles para Umbrales\n",
        "        # (Se utilizan para calcular el umbral adaptativo de entropía)\n",
        "        self.learnable_base_threshold = nn.Parameter(torch.tensor(config.initial_entropy_threshold))\n",
        "        self.learnable_std_scale = nn.Parameter(torch.tensor(0.5))\n",
        "\n",
        "        # Parámetro adicional para ponderar el uso de probabilidades\n",
        "        # Ajusta cuánto peso se le da a la \"discrepancia\" de prob. frente a la entropía\n",
        "        # (Podrías exponerlo como config.prob_factor si lo deseas)\n",
        "        self.prob_factor = getattr(config, \"prob_factor\", 0.3)\n",
        "\n",
        "        # Tamaño de la ventana para cálculos de estadística adaptativa\n",
        "        self.window_size = 128\n",
        "        self.stats_buffer = {}\n",
        "\n",
        "        # Dropout para parámetros\n",
        "        self.param_dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        # Atributos para rastrear el progreso de entrenamiento\n",
        "        self.current_step = 0\n",
        "        self.total_steps = 1000\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #         Funciones Internas para Cálculo del Umbral Adaptativo\n",
        "    # ------------------------------------------------------------------\n",
        "    def _compute_adaptive_threshold(self, entropies: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Ajusta dinámicamente el umbral de entropía basándose en estadísticos\n",
        "        (media y desviación estándar). Soporta secuencias cortas.\n",
        "\n",
        "        Args:\n",
        "            entropies (torch.Tensor): Tensor [B, S] con las entropías.\n",
        "        Returns:\n",
        "            threshold (torch.Tensor): Umbrales por cada muestra del batch [B, 1].\n",
        "        \"\"\"\n",
        "        batch_size = entropies.size(0)\n",
        "        means = []\n",
        "        stds = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            seq_len = entropies[i].size(0)\n",
        "            # Si la secuencia >= window_size, usar ventanas deslizantes\n",
        "            if seq_len >= self.window_size:\n",
        "                windows = entropies[i].unfold(0, self.window_size, max(1, self.window_size // 2))\n",
        "                window_mean = windows.mean(dim=1, keepdim=True)\n",
        "                window_std = torch.sqrt(\n",
        "                    torch.var(windows, dim=1, keepdim=True, unbiased=False) + 1e-6\n",
        "                )\n",
        "                means.append(window_mean.mean())\n",
        "                stds.append(window_std.mean())\n",
        "            else:\n",
        "                # Calcular media y std en toda la secuencia\n",
        "                entire_mean = entropies[i].mean()\n",
        "                entire_std = entropies[i].std(unbiased=False)\n",
        "                means.append(entire_mean)\n",
        "                stds.append(entire_std)\n",
        "\n",
        "        mean = torch.stack(means).view(batch_size, 1)\n",
        "        std = torch.stack(stds).view(batch_size, 1)\n",
        "\n",
        "        base_threshold = self.param_dropout(self.learnable_base_threshold)\n",
        "        std_scale = self.param_dropout(self.learnable_std_scale)\n",
        "\n",
        "        # Aplicar sigmoide y limitar en [0.1, 0.9]\n",
        "        threshold = torch.sigmoid(base_threshold + std_scale * std).clamp(min=0.1, max=0.9)\n",
        "        return threshold\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #          Función Auxiliar para Integrar Prob y Entropía\n",
        "    # ------------------------------------------------------------------\n",
        "    def _compute_boundary_score(self, entropies: torch.Tensor, probabilities: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calcula un vector de puntajes de corte, combinando la entropía\n",
        "        y la discrepancia entre probabilidades consecutivas.\n",
        "\n",
        "        Args:\n",
        "            entropies: [S] Entropías de la secuencia (una por posición).\n",
        "            probabilities: [S, V=256] Probabilidades por cada posición.\n",
        "\n",
        "        Returns:\n",
        "            scores: [S-1] Puntaje para cada posición j, j in [0..S-2],\n",
        "                    indicando cuán favorable es hacer un corte entre j y j+1.\n",
        "        \"\"\"\n",
        "        seq_len = probabilities.size(0)\n",
        "        if seq_len < 2:\n",
        "            # Si la secuencia es muy corta, no calculamos \"discrepancia\"\n",
        "            return entropies[:-1]  # Devolvemos algo sencillo\n",
        "\n",
        "        # Discrepancia de prob.: Distancia L1 entre P_j e P_{j+1}\n",
        "        # shape: [S-1]\n",
        "        prob_diff = torch.sum(\n",
        "            torch.abs(probabilities[:-1] - probabilities[1:]),\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        # Por simplicidad, el puntaje de corte en la posición j se define como:\n",
        "        # score_j = ent_j + prob_factor * prob_diff_j\n",
        "        # Donde ent_j = entropía en la posición j, y prob_diff_j indica\n",
        "        # el \"salto\" en la distribución de prob. entre j y j+1.\n",
        "        # Podríamos refinar con ent_{j+1} también, etc.\n",
        "        # Ajustar a gusto la mezcla.\n",
        "        cut_scores = entropies[:-1] + (self.prob_factor * prob_diff)\n",
        "\n",
        "        return cut_scores\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #             Cálculo de Parques con Entropía + Probabilidades\n",
        "    # ------------------------------------------------------------------\n",
        "    def compute_patches(self, bytes_input: torch.Tensor) -> List[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Calcula las fronteras (boundaries) de los parches en la secuencia de bytes de entrada\n",
        "        utilizando una combinación adaptativa de entropía y discrepancia en las probabilidades.\n",
        "\n",
        "        Esta versión mejorada integra tanto la entropía como las probabilidades generadas\n",
        "        por el modelo `EntropyLM` para decidir de manera adaptativa dónde segmentar los parches.\n",
        "        Además, asegura que todas las condiciones booleanas se manejen correctamente para\n",
        "        evitar errores como \"Boolean value of Tensor with more than one value is ambiguous\".\n",
        "\n",
        "        Args:\n",
        "            bytes_input (torch.Tensor): Tensor de entrada con forma [batch_size, seq_length],\n",
        "                                        conteniendo índices de bytes.\n",
        "\n",
        "        Returns:\n",
        "            List[torch.Tensor]: Lista de tensores, cada uno conteniendo las posiciones de\n",
        "                                corte (boundaries) para cada muestra en el batch.\n",
        "        \"\"\"\n",
        "        batch_size = bytes_input.size(0)\n",
        "        boundaries_list = []\n",
        "\n",
        "        # ====================== Progreso de Entrenamiento ======================\n",
        "        # Calcula el progreso de entrenamiento para ajustar la mezcla de boundaries fijos y adaptativos.\n",
        "        training_progress = (\n",
        "            getattr(self, 'current_step', 0) / getattr(self, 'total_steps', 1000)\n",
        "        ) if self.training else 1.0\n",
        "\n",
        "        # Determina el factor de mezcla basado en el progreso de entrenamiento.\n",
        "        if training_progress < 0.2:\n",
        "            mix_factor = 0.0  # Fase inicial: completamente fija\n",
        "        elif training_progress > 0.8:\n",
        "            mix_factor = 1.0  # Fase final: completamente adaptativa\n",
        "        else:\n",
        "            # Fase intermedia: mezcla gradual entre fija y adaptativa\n",
        "            raw_mix = (training_progress - 0.2) / 0.6\n",
        "            mix_factor = 1 / (1 + torch.exp(torch.tensor(-10 * (raw_mix - 0.5))))\n",
        "\n",
        "        # ====================== Boundaries Fijos Base ======================\n",
        "        # Calcula los boundaries fijos basados en el tamaño mínimo de parche.\n",
        "        seq_len = bytes_input.size(1)\n",
        "        if self.min_patch_size <= seq_len:\n",
        "            base_boundaries = torch.arange(\n",
        "                self.min_patch_size,\n",
        "                seq_len,\n",
        "                self.min_patch_size,\n",
        "                device=bytes_input.device\n",
        "            )\n",
        "        else:\n",
        "            base_boundaries = torch.tensor([], device=bytes_input.device, dtype=torch.long)\n",
        "\n",
        "        # ====================== Fase Completamente Fija ======================\n",
        "        # Si el mix_factor es 0, retorna únicamente los boundaries fijos.\n",
        "        if mix_factor == 0:\n",
        "            return [base_boundaries for _ in range(batch_size)]\n",
        "\n",
        "        # ====================== Cálculo de Entropía y Probabilidades ======================\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # 1. Obtiene las probabilidades del modelo EntropyLM\n",
        "            probabilities = self.entropy_model(bytes_input, return_probabilities=True)  # [B, S, V]\n",
        "\n",
        "            # 2. Obtiene las entropías del modelo EntropyLM\n",
        "            entropies = self.entropy_model(bytes_input, return_probabilities=False)  # [B, S]\n",
        "\n",
        "            # 3. Aplica dropout a las entropías si está en modo de entrenamiento\n",
        "            if self.training:\n",
        "                entropies = F.dropout(entropies, p=0.1, training=True)\n",
        "\n",
        "        # ====================== Cálculo de Umbrales Adaptativos ======================\n",
        "        # Calcula umbrales de entropía adaptativos para cada posición en la secuencia.\n",
        "        thresholds = self._compute_adaptive_threshold(entropies)  # [B, 1]\n",
        "\n",
        "        # ====================== Procesamiento por Cada Muestra del Batch ======================\n",
        "        for b_idx in range(batch_size):\n",
        "            current_ent = entropies[b_idx]         # [S]\n",
        "            current_prob = probabilities[b_idx]    # [S, V]\n",
        "            threshold = thresholds[b_idx].item()    # Valor escalar\n",
        "\n",
        "            # Combina entropía y discrepancias de probabilidad para calcular scores de corte.\n",
        "            # shape: [S-1]\n",
        "            boundary_scores = self._compute_boundary_score(current_ent, current_prob)\n",
        "\n",
        "            adaptive_boundaries = []\n",
        "            last_boundary = 0\n",
        "            S = current_ent.size(0)\n",
        "\n",
        "            # Itera sobre cada posición en la secuencia para decidir si cortar.\n",
        "            for pos in range(S):\n",
        "                current_size = pos - last_boundary + 1\n",
        "\n",
        "                if pos < S - 1:\n",
        "                    # Verifica las condiciones de corte:\n",
        "                    # 1) Tamaño del parche >= max_patch_size\n",
        "                    # 2) Tamaño del parche >= min_patch_size y score de corte > umbral adaptativo\n",
        "                    condition1 = current_size >= self.max_patch_size\n",
        "                    condition2 = (\n",
        "                        (current_size >= self.min_patch_size) and\n",
        "                        (boundary_scores[pos].item() > threshold * (1 + mix_factor))\n",
        "                    )\n",
        "                    cut_condition = condition1 or condition2\n",
        "                else:\n",
        "                    # Forzar el corte al final de la secuencia.\n",
        "                    cut_condition = (pos == S - 1)\n",
        "\n",
        "                # Si se cumple la condición de corte, agregar la posición como boundary.\n",
        "                if cut_condition:\n",
        "                    adaptive_boundaries.append(pos)\n",
        "                    last_boundary = pos + 1\n",
        "\n",
        "            # Convierte la lista de boundaries adaptativos a un tensor.\n",
        "            adaptive_boundaries = torch.tensor(\n",
        "                adaptive_boundaries, device=bytes_input.device, dtype=torch.long\n",
        "            )\n",
        "\n",
        "            # ====================== Mezcla de Boundaries Fijos y Adaptativos ======================\n",
        "            if mix_factor == 1:\n",
        "                # Si mix_factor es 1, usa solo boundaries adaptativos.\n",
        "                final_boundaries = adaptive_boundaries\n",
        "            else:\n",
        "                # Combina boundaries fijos y adaptativos ponderados por mix_factor.\n",
        "                fixed_weight = 1 - mix_factor\n",
        "                adaptive_weight = mix_factor\n",
        "                num_boundaries = int(\n",
        "                    fixed_weight * len(base_boundaries) +\n",
        "                    adaptive_weight * len(adaptive_boundaries)\n",
        "                )\n",
        "\n",
        "                if num_boundaries == 0:\n",
        "                    # Si no hay boundaries, retorna un tensor vacío.\n",
        "                    final_boundaries = torch.tensor([], device=bytes_input.device, dtype=torch.long)\n",
        "                else:\n",
        "                    # Combina los boundaries fijos y adaptativos.\n",
        "                    if len(base_boundaries) > 0 and len(adaptive_boundaries) > 0:\n",
        "                        combined = torch.cat([\n",
        "                            base_boundaries * fixed_weight,\n",
        "                            adaptive_boundaries.float() * adaptive_weight  # Convierte a float para la multiplicación\n",
        "                        ])\n",
        "                    elif len(base_boundaries) > 0:\n",
        "                        combined = base_boundaries.float() * fixed_weight\n",
        "                    else:\n",
        "                        combined = adaptive_boundaries.float() * adaptive_weight\n",
        "\n",
        "                    # Selecciona los top boundaries según num_boundaries.\n",
        "                    if combined.numel() > num_boundaries:\n",
        "                        _, indices = torch.topk(torch.abs(combined), num_boundaries)\n",
        "                        final_boundaries = torch.sort(combined[indices])[0].long()\n",
        "                    else:\n",
        "                        final_boundaries = torch.sort(combined)[0].long()\n",
        "\n",
        "            # ====================== Asegurar Límites y Limpieza ======================\n",
        "            # Filtra boundaries que estén dentro de los límites permitidos.\n",
        "            final_boundaries = final_boundaries[\n",
        "                (final_boundaries >= self.min_patch_size) &\n",
        "                (final_boundaries <= seq_len - 1)\n",
        "            ]\n",
        "\n",
        "            # Agrega los boundaries finales al listado.\n",
        "            boundaries_list.append(final_boundaries)\n",
        "\n",
        "        return boundaries_list\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #                 Forward Principal del BLT\n",
        "    # ------------------------------------------------------------------\n",
        "    def forward(self, bytes_input: torch.Tensor,\n",
        "        patch_boundaries: Optional[List[torch.Tensor]] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward principal:\n",
        "          1) Cálculo de byte_encodings vía LocalEncoder\n",
        "          2) Segmentación en parches (adaptativa) y reducción (mean)\n",
        "          3) Paso por GlobalTransformer\n",
        "          4) Decodificación (LocalDecoder) para obtener logits finales\n",
        "\n",
        "        Args:\n",
        "            bytes_input: [B, S] Tensor con bytes de entrada\n",
        "            patch_boundaries: Lista opcional de boundaries para forzar parches\n",
        "\n",
        "        Returns:\n",
        "            logits: [B, S, 256] Logits sobre bytes.\n",
        "        \"\"\"\n",
        "        batch_size, seq_length = bytes_input.size()\n",
        "\n",
        "        # -------------------- Encodings Locales --------------------\n",
        "        byte_encodings = self.local_encoder(bytes_input)\n",
        "        byte_encodings = self.global_dropout(byte_encodings)\n",
        "\n",
        "        # -------------------- Boundaries Adaptativos --------------------\n",
        "        if patch_boundaries is None:\n",
        "            patch_boundaries = self.compute_patches(bytes_input)\n",
        "\n",
        "        # -------------------- Calcular Patches --------------------\n",
        "        patch_means = []\n",
        "        max_patches = 0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            boundaries = patch_boundaries[i]\n",
        "            patches = []\n",
        "            start = 0\n",
        "\n",
        "            # Asegurar que boundaries tenga un final en seq_length\n",
        "            if boundaries.numel() > 0:\n",
        "                if boundaries.dim() == 0:\n",
        "                    boundaries = boundaries.unsqueeze(0)\n",
        "                boundaries = torch.cat([\n",
        "                    boundaries,\n",
        "                    torch.tensor([seq_length], device=boundaries.device)\n",
        "                ])\n",
        "            else:\n",
        "                boundaries = torch.tensor([seq_length], device=bytes_input.device)\n",
        "\n",
        "            # Reducir cada parche\n",
        "            for end in boundaries:\n",
        "                if end > start:\n",
        "                    patch = byte_encodings[i, start:end].mean(dim=0)\n",
        "                    patches.append(patch)\n",
        "                    start = end\n",
        "\n",
        "            # Si no hay parches creados, usar toda la secuencia\n",
        "            if not patches:\n",
        "                patches.append(byte_encodings[i].mean(dim=0))\n",
        "\n",
        "            patches_tensor = torch.stack(patches)\n",
        "            patch_means.append(patches_tensor)\n",
        "            max_patches = max(max_patches, patches_tensor.size(0))\n",
        "\n",
        "        # -------------------- Padding de Parches --------------------\n",
        "        padded_patches = torch.zeros(\n",
        "            batch_size, max_patches, self.config.hidden_size,\n",
        "            device=bytes_input.device,\n",
        "            dtype=byte_encodings.dtype\n",
        "        )\n",
        "\n",
        "        for i, patches in enumerate(patch_means):\n",
        "            num_patches = patches.size(0)\n",
        "            padded_patches[i, :num_patches] = patches\n",
        "\n",
        "        # -------------------- Global Transformer --------------------\n",
        "        global_output = self.global_transformer(padded_patches)\n",
        "        global_output = self.global_norm(global_output)\n",
        "        global_output = self.global_dropout(global_output)\n",
        "\n",
        "        # -------------------- Decodificación Local --------------------\n",
        "        logits = self.local_decoder(\n",
        "            self.global_dropout(byte_encodings),\n",
        "            global_output\n",
        "        )\n",
        "        return logits\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #                 Utilidad: Progreso de Entrenamiento\n",
        "    # ------------------------------------------------------------------\n",
        "    def update_training_progress(self, current_step, total_steps):\n",
        "        \"\"\"\n",
        "        Actualiza el progreso del entrenamiento para ajustar gradualmente\n",
        "        la mezcla de boundaries fijos y adaptativos.\n",
        "        \"\"\"\n",
        "        self.current_step = current_step\n",
        "        self.total_steps = total_steps\n",
        "\n",
        "# =============================================================================\n",
        "#                       CONFIGURACIÓN DEL MODELO BLT\n",
        "# =============================================================================\n",
        "\n",
        "class BLTConfig:\n",
        "    \"\"\"\n",
        "    Configuración del Byte-Level Transformer (BLT).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size=256,\n",
        "        intermediate_size=1024,\n",
        "        num_heads=4,\n",
        "        encoder_layers=1,\n",
        "        global_layers=6,\n",
        "        decoder_layers=2,\n",
        "        attention_dropout=0.1,\n",
        "        resid_dropout=0.12,\n",
        "        ngram_vocab_size=10000,\n",
        "        window_size=512,\n",
        "        max_position_embeddings=1024,\n",
        "        entropy_model_layers=2,\n",
        "        entropy_context_size=512,\n",
        "        entropy_threshold=0.5,\n",
        "        min_patch_size=32,\n",
        "        max_patch_size=512,\n",
        "        initial_entropy_threshold=0.5\n",
        "    ):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_heads = num_heads\n",
        "        self.encoder_layers = encoder_layers\n",
        "        self.global_layers = global_layers\n",
        "        self.decoder_layers = decoder_layers\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.resid_dropout = resid_dropout\n",
        "        self.ngram_vocab_size = ngram_vocab_size\n",
        "        self.window_size = window_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.entropy_model_layers = entropy_model_layers\n",
        "        self.entropy_context_size = entropy_context_size\n",
        "        self.entropy_threshold = entropy_threshold\n",
        "        self.min_patch_size = min_patch_size\n",
        "        self.max_patch_size = max_patch_size\n",
        "        self.initial_entropy_threshold = initial_entropy_threshold\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                          UTILIDADES DE MÁSCARAS\n",
        "# =============================================================================\n",
        "\n",
        "def create_block_causal_mask(seq_length: int, window_size: int = None):\n",
        "    \"\"\"\n",
        "    Crea una máscara causal con ventana opcional (sin cache).\n",
        "    \"\"\"\n",
        "    mask = torch.ones((seq_length, seq_length), dtype=torch.bool)\n",
        "    mask = torch.triu(mask, diagonal=1)\n",
        "\n",
        "    if window_size:\n",
        "        indices = torch.arange(seq_length)\n",
        "        window_mask = (indices.unsqueeze(1) - indices.unsqueeze(0)).abs() <= window_size\n",
        "        mask = mask | ~window_mask\n",
        "\n",
        "    return mask\n",
        "\n",
        "def create_patch_mask(patch_boundaries, seq_length):\n",
        "    \"\"\"\n",
        "    Crea una máscara para parches con optimizaciones de memoria y soporte\n",
        "    para parches dinámicos (sin cache).\n",
        "    \"\"\"\n",
        "    if isinstance(patch_boundaries, list):\n",
        "        patch_boundaries = torch.tensor(patch_boundaries)\n",
        "    elif patch_boundaries.numel() == 0:\n",
        "        return torch.zeros((seq_length, seq_length), dtype=torch.bool)\n",
        "\n",
        "    if not torch.all(patch_boundaries[1:] > patch_boundaries[:-1]):\n",
        "        patch_boundaries, _ = torch.sort(patch_boundaries)\n",
        "\n",
        "    mask = torch.zeros((seq_length, seq_length), dtype=torch.bool)\n",
        "\n",
        "    start = 0\n",
        "    for end in patch_boundaries:\n",
        "        if end > start:\n",
        "            mask[start:end, start:end] = True\n",
        "            start = end\n",
        "\n",
        "    if start < seq_length:\n",
        "        mask[start:, start:] = True\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                         FUNCIONES DE ENTRENAMIENTO\n",
        "# =============================================================================\n",
        "\n",
        "def train_step(model, optimizer, batch, patch_config):\n",
        "    \"\"\"\n",
        "    Realiza un paso de entrenamiento, calculando la pérdida y aplicando backpropagation.\n",
        "    \"\"\"\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    input_bytes = batch[:, :-1]\n",
        "    target_bytes = batch[:, 1:]\n",
        "    patch_boundaries = None\n",
        "\n",
        "    if patch_config.scheme == 'entropy':\n",
        "        with torch.no_grad():\n",
        "            entropies = model.entropy_model(input_bytes)\n",
        "            indices = torch.where(entropies > patch_config.entropy_threshold)\n",
        "            if indices[0].numel() == 0:\n",
        "                patch_boundaries = torch.tensor([], dtype=torch.long, device=entropies.device)\n",
        "            else:\n",
        "                patch_boundaries = indices[1]\n",
        "\n",
        "    elif patch_config.scheme == 'space':\n",
        "        indices = torch.where(input_bytes == 32)\n",
        "        if indices[0].numel() == 0:\n",
        "            patch_boundaries = torch.tensor([], dtype=torch.long, device=input_bytes.device)\n",
        "        else:\n",
        "            patch_boundaries = indices[1] + 1\n",
        "\n",
        "    elif patch_config.scheme == 'fixed':\n",
        "        stride = patch_config.stride\n",
        "        seq_length = input_bytes.size(1)\n",
        "        patch_boundaries = torch.arange(\n",
        "            stride, seq_length, stride, device=input_bytes.device\n",
        "        )\n",
        "\n",
        "    logits = model(input_bytes, patch_boundaries)\n",
        "\n",
        "    logits_reshaped = logits.view(-1, logits.size(-1))\n",
        "    target_bytes_reshaped = target_bytes.view(-1)\n",
        "    loss = F.cross_entropy(logits_reshaped, target_bytes_reshaped)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#                           FUNCIÓN DE GENERACIÓN\n",
        "# =============================================================================\n",
        "\n",
        "def generate(model, start_bytes, max_length=1000, temperature=1.0, top_k=20, patch_config=None, device='cpu'):\n",
        "    \"\"\"\n",
        "    Genera una secuencia de bytes a partir de un contexto inicial.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    generated = list(start_bytes)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while len(generated) < max_length:\n",
        "            input_bytes = torch.tensor(generated, device=device).unsqueeze(0)\n",
        "\n",
        "            if patch_config is not None:\n",
        "                if patch_config.scheme == 'entropy':\n",
        "                    entropies = model.entropy_model(input_bytes)\n",
        "                    if patch_config.use_monotonic:\n",
        "                        entropy_diff = entropies[:, 1:] - entropies[:, :-1]\n",
        "                        patch_boundaries = torch.where(entropy_diff > patch_config.entropy_threshold)[1] + 1\n",
        "                    else:\n",
        "                        patch_boundaries = torch.where(entropies > patch_config.entropy_threshold)[1]\n",
        "                else:\n",
        "                    patch_boundaries = torch.arange(\n",
        "                        patch_config.stride,\n",
        "                        len(generated),\n",
        "                        patch_config.stride,\n",
        "                        device=device)\n",
        "            else:\n",
        "                patch_boundaries = None\n",
        "\n",
        "            logits = model(input_bytes, patch_boundaries)\n",
        "\n",
        "            # Forma: [1, seq_length, vocab_size]\n",
        "            if logits.dim() == 3:\n",
        "                logits = logits[0, -1] / temperature\n",
        "            elif logits.dim() == 2:\n",
        "                logits = logits[0] / temperature\n",
        "            else:\n",
        "                break\n",
        "\n",
        "            top_k = min(top_k, logits.size(-1))\n",
        "            topk_logits, topk_indices = torch.topk(logits, top_k)\n",
        "            topk_probs = F.softmax(topk_logits, dim=-1)\n",
        "\n",
        "            next_byte = topk_indices[torch.multinomial(topk_probs, 1).item()].item()\n",
        "            generated.append(next_byte)\n",
        "\n",
        "            if next_byte == 0:\n",
        "                break\n",
        "\n",
        "    return generated\n",
        "\n"
      ],
      "metadata": {
        "id": "kbIvoJxbjMpO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ttblt/bltqwen.py\n",
        "# https://github.com/ianbarber/ttblt/blob/main/ttblt/bltqwen.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Any, Dict, List, Mapping, Optional, Tuple, Union\n",
        "from torchtune.modules.transformer import TransformerDecoder\n",
        "from torchtune.modules import (MultiHeadAttention, RMSNorm, TransformerCrossAttentionLayer, TransformerSelfAttentionLayer,)\n",
        "from torchtune.modules.model_fusion import FusionLayer\n",
        "from torchtune.models.qwen2._component_builders import qwen2_mlp\n",
        "\n",
        "PAD_ID = 256\n",
        "BOS_ID = 257\n",
        "EOS_ID = 258\n",
        "NUM_SPECIAL_TOKENS = 3\n",
        "VOCAB_SIZE = 256 + NUM_SPECIAL_TOKENS\n",
        "\n",
        "################################################\n",
        "# Local encoder/decoder (with cross-attn)\n",
        "################################################\n",
        "\n",
        "class LocalDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        global_dim: int,\n",
        "        vocab_size: int,\n",
        "        num_layers: int = 8,\n",
        "        num_cross_layers: int = 4,\n",
        "        num_heads: int = 8,\n",
        "        num_kv_heads: int = 8,\n",
        "        hidden_dim: int = 4096,\n",
        "        norm_eps: float = 1e-5,\n",
        "        attn_dropout: float = 0.0,\n",
        "        max_seq_len: int = 4096,\n",
        "        dtype=torch.bfloat16,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Self-attention layers\n",
        "        layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            layer = TransformerSelfAttentionLayer(\n",
        "                attn=self_attn,\n",
        "                mlp=mlp,\n",
        "                sa_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "                mlp_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "            )\n",
        "            layers.append(layer)\n",
        "\n",
        "        # Decoder with no token embeddings\n",
        "        self.decoder = TransformerDecoder(\n",
        "            tok_embeddings=nn.Identity(),  # No embedding layer needed\n",
        "            layers=layers,\n",
        "            max_seq_len=max_seq_len,\n",
        "            num_heads=num_heads,\n",
        "            head_dim=head_dim,\n",
        "            norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "            output=nn.Identity(),\n",
        "        )\n",
        "\n",
        "        # Cross-attention layers\n",
        "        self.cross_attn_layers = nn.ModuleList()\n",
        "        for _ in range(num_cross_layers):\n",
        "            cross_attn = MultiHeadAttention(\n",
        "                embed_dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                num_kv_heads=num_kv_heads,\n",
        "                head_dim=head_dim,\n",
        "                is_causal=False,\n",
        "            )\n",
        "            mlp = qwen2_mlp(dim=embed_dim, hidden_dim=hidden_dim)\n",
        "            cross_layer = TransformerCrossAttentionLayer(\n",
        "                attn=cross_attn,\n",
        "                mlp=mlp,\n",
        "                ca_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "                mlp_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "            )\n",
        "            self.cross_attn_layers.append(cross_layer)\n",
        "\n",
        "        self.output = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "        self.to(dtype=dtype)\n",
        "\n",
        "    def forward(self, byte_embeds, patch_embs, patch_ids):\n",
        "        # Pass byte embeddings directly to the decoder\n",
        "        x = self.decoder(byte_embeds, mask=None)  # Shape: [batch_size, seq_len, embed_dim]\n",
        "        x = x.to(torch.bfloat16) # TODO: Fix cast because no chunking.\n",
        "\n",
        "        # Apply cross-attention with patch embeddings\n",
        "        if self.cross_attn_layers:\n",
        "            num_patches = patch_embs.size(1)\n",
        "            mask = (\n",
        "                patch_ids.unsqueeze(2) == torch.arange(num_patches, device=byte_embeds.device).unsqueeze(0).unsqueeze(0)\n",
        "            )  # Shape: [batch_size, num_patches, seq_len]\n",
        "            for cross_layer in self.cross_attn_layers:\n",
        "                x = cross_layer(x, encoder_input=patch_embs, encoder_mask=mask)\n",
        "\n",
        "        # Compute logits\n",
        "        logits = self.output(x)  # Shape: [batch_size, seq_len, vocab_size]\n",
        "        return logits\n",
        "\n",
        "class LocalEncoderWithPooling(nn.Module):\n",
        "    def __init__(self, base_encoder, cross_attn_layers, embed_dim, global_dim):\n",
        "        super().__init__()\n",
        "        self.base_encoder = base_encoder\n",
        "        self.cross_attn_layers = cross_attn_layers\n",
        "        self.patch_projector = PatchToGlobalProjector(embed_dim, global_dim)\n",
        "\n",
        "    def forward(self, bytes, patch_ids):\n",
        "        # Get byte-level embeddings from the base encoder\n",
        "        byte_embeds = self.base_encoder(bytes, mask=None)\n",
        "        byte_embeds = byte_embeds.to(torch.bfloat16)\n",
        "\n",
        "        # Apply cross-attention if layers exist\n",
        "        if self.cross_attn_layers:\n",
        "            # num_patches = patch_ids.max().item() + 1\n",
        "            local_encoder_mask = (patch_ids.unsqueeze(1) == patch_ids.unsqueeze(2))  # Shape: [batch_size, seq_len, seq_len]\n",
        "            patch_embs = byte_embeds  # Start with byte embeddings\n",
        "            for cross_layer in self.cross_attn_layers:\n",
        "                patch_embs = cross_layer(patch_embs, encoder_input=byte_embeds, encoder_mask=local_encoder_mask)\n",
        "        else:\n",
        "            patch_embs = byte_embeds\n",
        "\n",
        "        # Reduce byte-level embeddings to patch-level embeddings\n",
        "        patch_embs = patch_reduce(patch_embs, patch_ids, reduce_op=\"mean\")  # Shape: [batch_size, num_patches, embed_dim]\n",
        "\n",
        "        # Project to global dimension\n",
        "        patch_embs = self.patch_projector(patch_embs)  # Shape: [batch_size, num_patches, global_dim]\n",
        "\n",
        "        return byte_embeds, patch_embs  # Return both byte embeddings and reduced patch embeddings\n",
        "\n",
        "def build_local_encoder(\n",
        "    global_dim: int,\n",
        "    vocab_size: int = VOCAB_SIZE,\n",
        "    embed_dim: int = 2048,\n",
        "    num_heads: int = 8,\n",
        "    num_kv_heads: int = 8,\n",
        "    hidden_dim: int = 4096,\n",
        "    norm_eps: float = 1e-5,\n",
        "    attn_dropout: float = 0.0,\n",
        "    max_seq_len: int = 2048,\n",
        "    num_layers: int = 4,\n",
        "    num_cross_layers = 4,\n",
        "    dtype=torch.bfloat16,\n",
        "    use_hash_ngrams=True,\n",
        "    max_ngram: int = 8,\n",
        "    num_ngram_buckets: int = 500000,\n",
        "):\n",
        "    head_dim = embed_dim // num_heads\n",
        "\n",
        "    if use_hash_ngrams:\n",
        "        tok_embeddings = HashNGramEmbedder(\n",
        "            embed_dim=embed_dim,\n",
        "            max_n=max_ngram,\n",
        "            num_buckets=num_ngram_buckets,\n",
        "            vocab_size=vocab_size\n",
        "        )\n",
        "    else:\n",
        "        tok_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    # Build self-attention layers with Qwen MLP\n",
        "    layers = nn.ModuleList()\n",
        "    for _ in range(num_layers):\n",
        "        # TODO: KV cache?\n",
        "        self_attn = MultiHeadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_kv_heads=num_kv_heads,\n",
        "            head_dim=head_dim,\n",
        "            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=True),\n",
        "            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=True),\n",
        "            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=True),\n",
        "            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),\n",
        "            max_seq_len=max_seq_len,\n",
        "            attn_dropout=attn_dropout,\n",
        "            is_causal=True,\n",
        "        )\n",
        "        mlp = qwen2_mlp(dim=embed_dim, hidden_dim=hidden_dim)\n",
        "        layer = TransformerSelfAttentionLayer(\n",
        "            attn=self_attn,\n",
        "            mlp=mlp,\n",
        "            sa_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "            mlp_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "        )\n",
        "        layers.append(layer)\n",
        "\n",
        "    base_encoder = TransformerDecoder(\n",
        "        tok_embeddings=tok_embeddings,\n",
        "        layers=layers,\n",
        "        max_seq_len=max_seq_len,\n",
        "        num_heads=num_heads,\n",
        "        head_dim=head_dim,\n",
        "        norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "        output=nn.Identity(),  # no final projection\n",
        "    )\n",
        "\n",
        "    # Cross-attention layers (optional)\n",
        "    cross_attn_layers = nn.ModuleList()\n",
        "    for _ in range(num_cross_layers):\n",
        "        cross_attn = MultiHeadAttention(\n",
        "            embed_dim=global_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_kv_heads=num_kv_heads,\n",
        "            head_dim=head_dim,\n",
        "            q_proj=nn.Linear(global_dim, num_heads * head_dim, bias=True),\n",
        "            k_proj=nn.Linear(global_dim, num_kv_heads * head_dim, bias=True),\n",
        "            v_proj=nn.Linear(global_dim, num_kv_heads * head_dim, bias=True),\n",
        "            output_proj=nn.Linear(global_dim, global_dim, bias=False),\n",
        "            max_seq_len=max_seq_len,\n",
        "            attn_dropout=attn_dropout,\n",
        "            is_causal=False,\n",
        "        )\n",
        "        mlp = qwen2_mlp(dim=embed_dim, hidden_dim=hidden_dim)\n",
        "        cross_layer = TransformerCrossAttentionLayer(\n",
        "            attn=cross_attn,\n",
        "            mlp=mlp,\n",
        "            ca_norm=RMSNorm(global_dim, eps=norm_eps),\n",
        "            mlp_norm=RMSNorm(global_dim, eps=norm_eps),\n",
        "        )\n",
        "        cross_attn_layers.append(cross_layer)\n",
        "\n",
        "    local_encoder = LocalEncoderWithPooling(base_encoder, cross_attn_layers, embed_dim, global_dim)\n",
        "    return local_encoder.to(dtype=dtype)\n",
        "\n",
        "################################################\n",
        "# dynamic patching\n",
        "################################################\n",
        "\n",
        "def compute_local_entropy(bytes_tensor, window_size=8):\n",
        "    \"\"\"Return a per-token \"entropy\" measure to guide patching\n",
        "\n",
        "    Arguments:\n",
        "        bytes_tensor: Torch.tensor[batch_size, seq_len] byttes to calc entropy on\n",
        "        window_size: int size to window across\n",
        "\n",
        "        local_entropy: torch.Tensor[batch_size, seq_len]\n",
        "    \"\"\"\n",
        "    # bytes_tensor:\n",
        "    device = bytes_tensor.device\n",
        "    batch_size, seq_len = bytes_tensor.shape\n",
        "\n",
        "    # We’ll keep a sliding frequency table. Initialize all zeros:\n",
        "    freq = torch.zeros(batch_size, VOCAB_SIZE, device=device)\n",
        "    local_entropy = torch.zeros(batch_size, seq_len, device=device)\n",
        "\n",
        "    for pos in range(seq_len):\n",
        "        # add current byte\n",
        "        current_byte = bytes_tensor[:, pos]\n",
        "        freq[torch.arange(batch_size), current_byte] += 1\n",
        "\n",
        "        # compute distribution\n",
        "        dist = freq / freq.sum(dim=1, keepdim=True).clamp_min(1e-8)\n",
        "        # compute -p*log2(p)\n",
        "        ent = -(dist * (dist + 1e-8).log2()).sum(dim=1)\n",
        "        local_entropy[:, pos] = ent\n",
        "\n",
        "        # remove oldest byte if we exceed window size\n",
        "        if pos >= window_size:\n",
        "            oldest_byte = bytes_tensor[:, pos - window_size]\n",
        "            freq[torch.arange(batch_size), oldest_byte] -= 1\n",
        "    return local_entropy\n",
        "\n",
        "def dynamic_patch(\n",
        "    bytes_tensor: torch.Tensor,\n",
        "    threshold: float = 3.0,   # starting entropy threshold in bits\n",
        "    min_threshold: float = 2.0,    # lower bound\n",
        "    max_threshold: float = 5.0,    # upper bound\n",
        "    threshold_step_down: float = 0.1,  # how much to decrease threshold if no patches triggered\n",
        "    threshold_step_up: float = 0.1,    # how much to increase threshold if we trigger a patch\n",
        "    patch_size: int = 4,\n",
        "    window_size: int = 8\n",
        "):\n",
        "    \"\"\"\n",
        "    A dynamic patching approach that adjusts the entropy threshold\n",
        "    upward/downward depending on whether patches are being triggered too often or not enough.\n",
        "\n",
        "    Args:\n",
        "        bytes_tensor: [batch_size, seq_len]\n",
        "        threshold: initial bits threshold for local entropy\n",
        "        min_threshold, max_threshold: clamp thresholds\n",
        "        threshold_step_down, threshold_step_up: step sizes\n",
        "        patch_size: max patch length if we haven't triggered a boundary earlier\n",
        "        window_size: for computing local entropy\n",
        "\n",
        "    Returns:\n",
        "        patch_ids: [batch_size, seq_len] with patch ID for each token\n",
        "        local_ent: [batch_size, seq_len] local entropy in bits\n",
        "    \"\"\"\n",
        "    local_ent = compute_local_entropy(bytes_tensor, window_size=window_size)\n",
        "    batch_size, seq_len = bytes_tensor.shape\n",
        "\n",
        "    # Each row’s patch assignment\n",
        "    patch_ids = torch.zeros(batch_size, seq_len, dtype=torch.long, device=bytes_tensor.device)\n",
        "\n",
        "    current_patch = torch.zeros(batch_size, dtype=torch.long, device=bytes_tensor.device)\n",
        "    patch_lengths = torch.zeros(batch_size, dtype=torch.long, device=bytes_tensor.device)\n",
        "\n",
        "    # Keep track of how many consecutive tokens we have processed w/o triggering any new patch\n",
        "    # so we can adjust threshold if we go too long\n",
        "    consecutive_no_trigger = 0\n",
        "\n",
        "    for pos in range(seq_len):\n",
        "        patch_lengths += 1\n",
        "\n",
        "        # Which batch elements exceed threshold or hit patch size\n",
        "        trigger_new_patch = (local_ent[:, pos] > threshold) | (patch_lengths >= patch_size)\n",
        "        triggered_rows = trigger_new_patch.nonzero(as_tuple=False).flatten()\n",
        "\n",
        "        if len(triggered_rows) > 0:\n",
        "            # For each row that triggered, increment patch and reset patch_lengths\n",
        "            current_patch[triggered_rows] += 1\n",
        "            patch_lengths[triggered_rows] = 0\n",
        "\n",
        "            # Because at least 1 row triggered a patch, we can optionally adjust threshold upward\n",
        "            # or downward. For example, *raising* threshold if we keep splitting too often:\n",
        "            threshold = min(threshold + threshold_step_up, max_threshold)\n",
        "\n",
        "            consecutive_no_trigger = 0\n",
        "        else:\n",
        "            # No new patch was triggered\n",
        "            consecutive_no_trigger += 1\n",
        "            # If we haven't triggered for a while, lower threshold so that we become more likely\n",
        "            # to split in the future.\n",
        "            threshold = max(threshold - threshold_step_down, min_threshold)\n",
        "\n",
        "        patch_ids[:, pos] = current_patch\n",
        "\n",
        "    return patch_ids, local_ent\n",
        "\n",
        "def patch_reduce(h, patch_ids, reduce_op=\"mean\"):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        h: [batch_size, seq_len, emb_dim]\n",
        "        patch_ids: [batch_size, seq_len]\n",
        "        reduce_op: e.g. \"mean\", \"amin\", \"amax\"\n",
        "\n",
        "    returns: [batch_size, num_patches, emb_dim]\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, emb_dim = h.shape\n",
        "    num_patches = patch_ids.max().item() + 1  # patch IDs go from 0..max\n",
        "\n",
        "    # expand dims so we can scatter:\n",
        "    expanded_ids = patch_ids.unsqueeze(-1).expand(-1, -1, emb_dim)\n",
        "    reduced = torch.zeros(batch_size, num_patches, emb_dim, device=h.device, dtype=h.dtype)\n",
        "    reduced = reduced.scatter_reduce(\n",
        "        dim=1,\n",
        "        index=expanded_ids,\n",
        "        src=h,\n",
        "        reduce=reduce_op,\n",
        "        include_self=False,\n",
        "    )\n",
        "    return reduced\n",
        "\n",
        "def compute_patch_size(so_far: torch.Tensor, threshold=3.0, max_patch=8):\n",
        "    \"\"\"\n",
        "    heuristic function for deciding the patch length\n",
        "    based on dynamic_patch logic or local entropy.\n",
        "\n",
        "    so_far: [seq_len] or [1, seq_len], the current context of tokens (including newly generated ones).\n",
        "    threshold: approximate bits threshold for deciding to break the patch.\n",
        "    max_patch: a max patch length to avoid overly large chunks.\n",
        "\n",
        "    Returns:\n",
        "        predicted_patch_size: int\n",
        "            the number of bytes to decode in the *next* patch in a single forward pass\n",
        "    \"\"\"\n",
        "    # re-run dynamic_patch() on the entire sequence and see how big the last patch is.\n",
        "    # this is probably pretty wasteful!s\n",
        "    # Then we decide how big the *next* patch would be if we continued.\n",
        "    # This is a simple way to reuse your dynamic_patch code.\n",
        "    if so_far.dim() == 1:\n",
        "        so_far = so_far.unsqueeze(0)  # [batch=1, seq_len]\n",
        "\n",
        "    patch_ids, _ = dynamic_patch(so_far, threshold=threshold, patch_size=max_patch)\n",
        "\n",
        "    # The ID of the last patch in that sequence:\n",
        "    last_patch_id = patch_ids[0, -1].item()  # e.g. 3 means patches 0..3\n",
        "    # Count how many tokens so far belong to the last patch\n",
        "    # (i.e. sum of patch_ids == last_patch_id)\n",
        "    count_last_patch = (patch_ids[0] == last_patch_id).sum().item()\n",
        "\n",
        "    # guess that the next patch might be similar in size:\n",
        "    # If we already used up to 'count_last_patch' tokens for the last patch,\n",
        "    # we can try the same or smaller for the next patch. A simple approach is:\n",
        "    predicted_size = max(1, min(count_last_patch, max_patch))\n",
        "\n",
        "    return predicted_size\n",
        "\n",
        "################################################\n",
        "# Projection layer\n",
        "################################################\n",
        "class PatchToGlobalProjector(nn.Module):\n",
        "    def __init__(self, local_dim, global_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(local_dim, global_dim)\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "################################################\n",
        "# ByteLatentQwen2p5Decoder with cross-attn\n",
        "################################################\n",
        "class ByteLatentQwen2p5Decoder(TransformerDecoder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        qwen_cfg: Dict[str, Any],\n",
        "        local_encoder_cfg: Dict[str, Any],\n",
        "        patch_size: int = 4,\n",
        "        patching_threshold: float = 3.0,\n",
        "        freeze_global_for_n_steps: int = 0,\n",
        "    ):\n",
        "        layers = nn.ModuleList()\n",
        "        head_dim = qwen_cfg['embed_dim'] // qwen_cfg['num_heads']\n",
        "\n",
        "        for _ in range(qwen_cfg['num_layers']):\n",
        "            layer = TransformerSelfAttentionLayer(\n",
        "                attn=self_attn,\n",
        "                mlp=mlp,\n",
        "                sa_norm=RMSNorm(dim=qwen_cfg['embed_dim'], eps=qwen_cfg['norm_eps']),\n",
        "                mlp_norm=RMSNorm(dim=qwen_cfg['embed_dim'], eps=qwen_cfg['norm_eps']),\n",
        "            )\n",
        "            layers.append(layer)\n",
        "\n",
        "        output = nn.Linear(\n",
        "            qwen_cfg['embed_dim'],\n",
        "            VOCAB_SIZE  ,  # bytes\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "        output = nn.Identity()  # Global transformer doesn’t output logits\n",
        "        emb = nn.Identity()\n",
        "        super().__init__(\n",
        "            tok_embeddings=emb,\n",
        "            layers=layers,\n",
        "            max_seq_len=qwen_cfg['max_seq_len'],\n",
        "            num_heads=qwen_cfg['num_heads'],\n",
        "            head_dim=head_dim,\n",
        "            norm=RMSNorm(qwen_cfg['embed_dim'], eps=qwen_cfg['norm_eps']),\n",
        "            output=output,\n",
        "        )\n",
        "        self.local_encoder = build_local_encoder(**local_encoder_cfg, global_dim=qwen_cfg['embed_dim'])\n",
        "        self.local_decoder = LocalDecoder(\n",
        "            embed_dim=qwen_cfg['embed_dim'],\n",
        "            global_dim=qwen_cfg['embed_dim'],\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            max_seq_len=local_encoder_cfg['max_seq_len'],\n",
        "        )\n",
        "\n",
        "         # Collect parameters for different learning rates\n",
        "        self.qwen_params = []\n",
        "        self.new_params = []\n",
        "\n",
        "        # Qwen parts\n",
        "        self.qwen_params.extend(self.norm.parameters())\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, TransformerSelfAttentionLayer):\n",
        "                self.qwen_params.extend(layer.parameters())\n",
        "\n",
        "        # New parts\n",
        "        self.new_params.extend(self.tok_embeddings.parameters())\n",
        "        self.new_params.extend(self.output.parameters())\n",
        "        self.new_params.extend(self.local_encoder.parameters())\n",
        "        self.new_params.extend(self.local_decoder.parameters())\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.patching_threshold = patching_threshold\n",
        "        self.freeze_global_for_n_steps = freeze_global_for_n_steps\n",
        "        self.current_step = 0\n",
        "        self.global_frozen = freeze_global_for_n_steps > 0\n",
        "        if self.global_frozen:\n",
        "            self._update_freezing()\n",
        "\n",
        "        # We'll store how many chunks the user wants for final output\n",
        "        self.num_output_chunks = 0  # default\n",
        "\n",
        "    def _update_freezing(self):\n",
        "        if self.global_frozen:\n",
        "            for param in self.qwen_params:\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def set_num_output_chunks(self, num_output_chunks: int) -> None:\n",
        "        super().set_num_output_chunks(num_output_chunks)\n",
        "        self.num_output_chunks = num_output_chunks\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tokens: torch.Tensor,\n",
        "        *,\n",
        "        mask: Optional[Union[torch.Tensor, float]] = None,\n",
        "        encoder_input: Optional[torch.Tensor] = None,\n",
        "        encoder_mask: Optional[torch.Tensor] = None,\n",
        "        input_pos: Optional[torch.Tensor] = None,\n",
        "        patch_ids: Optional[torch.Tensor] = None,\n",
        "    ) -> Union[torch.Tensor, List[torch.Tensor]]:\n",
        "        # Update freezing state if in training\n",
        "        if self.training and self.global_frozen:\n",
        "            self.current_step += 1\n",
        "            if self.current_step >= self.freeze_global_for_n_steps:\n",
        "                self.global_frozen = False\n",
        "                self._update_freezing()\n",
        "\n",
        "        if patch_ids is None:\n",
        "            patch_ids, _ = dynamic_patch(tokens, threshold=self.patching_threshold, patch_size=self.patch_size)\n",
        "\n",
        "        byte_embeds, patch_embs = self.local_encoder(tokens, patch_ids=patch_ids)\n",
        "        global_out = super().forward(patch_embs, mask=mask, input_pos=input_pos)\n",
        "\n",
        "        # Assuming the outs are chunked, take entry[0] of outputs.\n",
        "        if self.num_output_chunks == 0:\n",
        "            global_out = global_out.to(torch.bfloat16) # TODO: Another fix for the 0 chunk float. Need to move this dtype definiton.\n",
        "            logits = self.local_decoder(byte_embeds, global_out, patch_ids)\n",
        "        else:\n",
        "            global_out_combined = torch.cat(global_out, dim=1) # Unchunking - it would be better not to pass this through I think.\n",
        "            clogits = self.local_decoder(byte_embeds, global_out_combined, patch_ids)\n",
        "            logits = [chunk for chunk in clogits.chunk(self.num_output_chunks, dim=1)]\n",
        "        return logits\n",
        "\n",
        "    def unified_generate(\n",
        "        self,\n",
        "        prompt: Union[torch.LongTensor, List[int]],\n",
        "        max_new_tokens: int = 128,\n",
        "        eos_id: int = EOS_ID,\n",
        "        temperature: float = 0.7,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0,\n",
        "        frequency_penalty: float = 0.1,\n",
        "        repetition_penalty: float = 1.5,\n",
        "        greedy: bool = False,\n",
        "    ) -> torch.Tensor:\n",
        "        # 1) Convert prompt to a tensor [1, seq_len]\n",
        "        if isinstance(prompt, list):\n",
        "            prompt = torch.tensor(prompt, dtype=torch.long, device=next(self.parameters()).device)\n",
        "        if prompt.dim() == 1:\n",
        "            prompt = prompt.unsqueeze(0) # Add a batch dimension.\n",
        "\n",
        "        device = prompt.device\n",
        "        all_tokens = prompt.clone()\n",
        "\n",
        "        # Track usage for freq / repetition penalty\n",
        "        token_counts = torch.bincount(all_tokens[0], minlength=VOCAB_SIZE).to(device)\n",
        "        recent_tokens = []\n",
        "\n",
        "        # Start generating\n",
        "        for step_i in range(max_new_tokens):\n",
        "\n",
        "            # No patching => standard forward\n",
        "            with torch.no_grad():\n",
        "                logits = self.forward(all_tokens)\n",
        "\n",
        "            if isinstance(logits, list):\n",
        "                logits = torch.cat(logits, dim=1)\n",
        "            step_logits = logits[0, -1, :]\n",
        "\n",
        "            # Apply temperature\n",
        "            if temperature != 1.0:\n",
        "                step_logits = step_logits / temperature\n",
        "\n",
        "            # Repetition penalty\n",
        "            for tk in recent_tokens:\n",
        "                step_logits[tk] /= repetition_penalty\n",
        "\n",
        "            # Frequency penalty\n",
        "            step_logits -= token_counts * frequency_penalty\n",
        "\n",
        "            # top-k\n",
        "            if top_k > 0:\n",
        "                vals, idxs = torch.topk(step_logits, top_k)\n",
        "                mask = torch.ones_like(step_logits, dtype=torch.bool)\n",
        "                mask[idxs] = False\n",
        "                step_logits[mask] = float('-inf')\n",
        "\n",
        "            # top-p\n",
        "            probs = torch.softmax(step_logits, dim=-1)\n",
        "            if top_p > 0:\n",
        "                sorted_logits, sorted_indices = torch.sort(step_logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(probs, dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "                sorted_indices_to_remove[:, 0] = 0\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                step_logits = step_logits.scatter(-1, indices_to_remove, -1e9)\n",
        "\n",
        "            # Re‐normalize\n",
        "            total_p = probs.sum()\n",
        "            if total_p > 0:\n",
        "                probs /= total_p\n",
        "            else:\n",
        "                probs = torch.ones_like(probs) / probs.size(-1)\n",
        "\n",
        "            # Next token\n",
        "            if greedy:\n",
        "                next_token = torch.argmax(probs).item()\n",
        "            else:\n",
        "                next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # Add new token\n",
        "            all_tokens = torch.cat(\n",
        "                [all_tokens, torch.tensor([[next_token]], device=device)],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "            # Update counters\n",
        "            token_counts[next_token] += 1\n",
        "            recent_tokens.append(next_token)\n",
        "            if len(recent_tokens) > 5:\n",
        "                recent_tokens.pop(0)\n",
        "\n",
        "            # EOS break\n",
        "            if next_token == eos_id:\n",
        "                break\n",
        "        return all_tokens\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZLe4wsUDc0aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ttblt/bltqwen.py\n",
        "# https://github.com/ianbarber/ttblt/blob/main/ttblt/bltqwen.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Any, Dict, List, Mapping, Optional, Tuple, Union\n",
        "from torchtune.modules.transformer import TransformerDecoder\n",
        "from torchtune.modules import (MultiHeadAttention, RMSNorm, TransformerCrossAttentionLayer, TransformerSelfAttentionLayer,)\n",
        "from torchtune.modules.model_fusion import FusionLayer\n",
        "from torchtune.models.qwen2._component_builders import qwen2_mlp\n",
        "\n",
        "PAD_ID = 256\n",
        "BOS_ID = 257\n",
        "EOS_ID = 258\n",
        "NUM_SPECIAL_TOKENS = 3\n",
        "VOCAB_SIZE = 256 + NUM_SPECIAL_TOKENS\n",
        "\n",
        "################################################\n",
        "# Local encoder/decoder (with cross-attn)\n",
        "################################################\n",
        "\n",
        "class HashNGramEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps a main byte embedding plus additional hash-based\n",
        "    n-gram embedding lookups. Two modes are supported:\n",
        "\n",
        "    1. Separate tables mode (default): One embedding table per n in [3, max_n].\n",
        "    2. Shared table mode (if shared_table=True): A single embedding table is used\n",
        "       for all n-gram sizes.\n",
        "\n",
        "    When using the shared table mode, a learned n-gram size embedding is added so that the model can\n",
        "    distinguish among n-gram lengths.\n",
        "\n",
        "    The final embedding at each position is:\n",
        "       main_embed(byte) + sum_{n in 3..max_n} (ngram_embed [+ ngram_size_embed if shared])\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int = 2048,\n",
        "        max_n: int = 8,\n",
        "        num_buckets: int = 500_000,\n",
        "        vocab_size: int = VOCAB_SIZE,\n",
        "        hash_base: int = 0,\n",
        "        hash_mod: int = 2**23,\n",
        "        shared_table: bool = True  # switchable mode\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_n = max_n\n",
        "        self.num_buckets = num_buckets\n",
        "        self.hash_base = hash_base\n",
        "        if (self.hash_base == 0):\n",
        "            self.hash_base = vocab_size + 1\n",
        "        self.hash_mod = hash_mod\n",
        "        self.shared_table = shared_table\n",
        "\n",
        "        # Main byte embedding.\n",
        "        self.main_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        if shared_table:\n",
        "            # One shared table for all n-gram sizes.\n",
        "            self.shared_ngram_table = nn.Embedding(num_buckets, embed_dim)\n",
        "            nn.init.normal_(self.shared_ngram_table.weight, mean=0.0, std=0.02)\n",
        "            # n-gram size embedding: one learned vector per n in [3, max_n]\n",
        "            # (max_n - 2) distinct n values.\n",
        "            self.ngram_size_embed = nn.Embedding(max_n - 2, embed_dim)\n",
        "            nn.init.normal_(self.ngram_size_embed.weight, mean=0.0, std=0.02)\n",
        "        else:\n",
        "            # Separate table per n-gram size.\n",
        "            self.ngram_tables = nn.ModuleDict()\n",
        "            for n in range(3, max_n + 1):\n",
        "                table = nn.Embedding(num_buckets, embed_dim)\n",
        "                nn.init.normal_(table.weight, mean=0.0, std=0.02)\n",
        "                self.ngram_tables[str(n)] = table\n",
        "\n",
        "        # Precompute powers for rolling hash for each n in [3, max_n] using modular exponentiation.\n",
        "        for n in range(3, max_n + 1):\n",
        "            powers_list = [pow(hash_base, n - 1 - k, hash_mod) for k in range(n)]\n",
        "            powers = torch.tensor(powers_list, dtype=torch.int32)\n",
        "            self.register_buffer(f'powers_{n}', powers)\n",
        "\n",
        "    def forward(self, tokens: torch.LongTensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        tokens: [batch_size, seq_len] of byte IDs in [0..255].\n",
        "        Returns final embeddings of shape [batch_size, seq_len, embed_dim].\n",
        "        All hash arithmetic is done in int32.\n",
        "        \"\"\"\n",
        "        bsz, seq_len = tokens.shape\n",
        "        if seq_len == 0:\n",
        "            return torch.empty(bsz, 0, self.embed_dim, device=tokens.device)\n",
        "\n",
        "        # Main byte embedding.\n",
        "        out = self.main_embed(tokens)  # [bsz, seq_len, embed_dim]\n",
        "\n",
        "        # For each n-gram size, compute and add n-gram embeddings.\n",
        "        for n in range(3, self.max_n + 1):\n",
        "            if seq_len < n:\n",
        "                continue\n",
        "\n",
        "            powers = getattr(self, f'powers_{n}')  # shape: [n]\n",
        "            ngrams = tokens.unfold(1, n, 1).to(torch.int32)  # [bsz, seq_len - n + 1, n]\n",
        "            if ngrams.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            temp = (ngrams * powers.unsqueeze(0).unsqueeze(0)) % self.hash_mod\n",
        "            hashed_vals = temp.sum(dim=2) % self.hash_mod  # [bsz, seq_len - n + 1]\n",
        "\n",
        "            hashed_idxs = torch.zeros((bsz, seq_len), dtype=torch.int32, device=tokens.device)\n",
        "            hashed_idxs[:, n - 1:] = hashed_vals\n",
        "            hashed_idxs = hashed_idxs % self.num_buckets\n",
        "\n",
        "            if self.shared_table:\n",
        "                ngram_embed = self.shared_ngram_table(hashed_idxs.long())  # [bsz, seq_len, embed_dim]\n",
        "                # Add n-gram size embedding only in shared mode.\n",
        "                size_embed = self.ngram_size_embed(torch.tensor(n - 3, device=tokens.device))\n",
        "                size_embed = size_embed.unsqueeze(0).unsqueeze(0)  # [1, 1, embed_dim]\n",
        "                ngram_embed = ngram_embed + size_embed\n",
        "            else:\n",
        "                table = self.ngram_tables[str(n)]\n",
        "                ngram_embed = table(hashed_idxs.long())\n",
        "\n",
        "            out += ngram_embed\n",
        "\n",
        "        num_contrib = 1 + (self.max_n - 2)  # main embedding + contributions for each n from 3 to max_n\n",
        "        out = out / num_contrib\n",
        "        return out\n",
        "\n",
        "class LocalDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        global_dim: int,\n",
        "        vocab_size: int,\n",
        "        num_layers: int = 8,\n",
        "        num_cross_layers: int = 4,\n",
        "        num_heads: int = 8,\n",
        "        num_kv_heads: int = 8,\n",
        "        hidden_dim: int = 4096,\n",
        "        norm_eps: float = 1e-5,\n",
        "        attn_dropout: float = 0.0,\n",
        "        max_seq_len: int = 4096,\n",
        "        dtype=torch.bfloat16,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Self-attention layers\n",
        "        layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self_attn = MultiHeadAttention(\n",
        "                embed_dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                num_kv_heads=num_kv_heads,\n",
        "                head_dim=head_dim,\n",
        "                q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=True),\n",
        "                k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=True),\n",
        "                v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=True),\n",
        "                output_proj=nn.Linear(embed_dim, embed_dim, bias=False),\n",
        "                max_seq_len=max_seq_len,\n",
        "                attn_dropout=attn_dropout,\n",
        "                is_causal=True,\n",
        "            )\n",
        "            mlp = qwen2_mlp(dim=embed_dim, hidden_dim=hidden_dim)\n",
        "            layer = TransformerSelfAttentionLayer(\n",
        "                attn=self_attn,\n",
        "                mlp=mlp,\n",
        "                sa_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "                mlp_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "            )\n",
        "            layers.append(layer)\n",
        "\n",
        "        # Decoder with no token embeddings\n",
        "        self.decoder = TransformerDecoder(\n",
        "            tok_embeddings=nn.Identity(),  # No embedding layer needed\n",
        "            layers=layers,\n",
        "            max_seq_len=max_seq_len,\n",
        "            num_heads=num_heads,\n",
        "            head_dim=head_dim,\n",
        "            norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "            output=nn.Identity(),\n",
        "        )\n",
        "\n",
        "        # Cross-attention layers\n",
        "        self.cross_attn_layers = nn.ModuleList()\n",
        "        for _ in range(num_cross_layers):\n",
        "            cross_attn = MultiHeadAttention(\n",
        "                embed_dim=embed_dim,\n",
        "                num_heads=num_heads,\n",
        "                num_kv_heads=num_kv_heads,\n",
        "                head_dim=head_dim,\n",
        "                q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=True),\n",
        "                k_proj=nn.Linear(global_dim, num_kv_heads * head_dim, bias=True),\n",
        "                v_proj=nn.Linear(global_dim, num_kv_heads * head_dim, bias=True),\n",
        "                output_proj=nn.Linear(embed_dim, embed_dim, bias=False),\n",
        "                max_seq_len=max_seq_len,\n",
        "                attn_dropout=attn_dropout,\n",
        "                is_causal=False,\n",
        "            )\n",
        "            mlp = qwen2_mlp(dim=embed_dim, hidden_dim=hidden_dim)\n",
        "            cross_layer = TransformerCrossAttentionLayer(\n",
        "                attn=cross_attn,\n",
        "                mlp=mlp,\n",
        "                ca_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "                mlp_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "            )\n",
        "            self.cross_attn_layers.append(cross_layer)\n",
        "\n",
        "        self.output = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "        self.to(dtype=dtype)\n",
        "\n",
        "    def forward(self, byte_embeds, patch_embs, patch_ids):\n",
        "        # Pass byte embeddings directly to the decoder\n",
        "        x = self.decoder(byte_embeds, mask=None)  # Shape: [batch_size, seq_len, embed_dim]\n",
        "        x = x.to(torch.bfloat16) # TODO: Fix cast because no chunking.\n",
        "\n",
        "        # Apply cross-attention with patch embeddings\n",
        "        if self.cross_attn_layers:\n",
        "            num_patches = patch_embs.size(1)\n",
        "            mask = (\n",
        "                patch_ids.unsqueeze(2) == torch.arange(num_patches, device=byte_embeds.device).unsqueeze(0).unsqueeze(0)\n",
        "            )  # Shape: [batch_size, num_patches, seq_len]\n",
        "            for cross_layer in self.cross_attn_layers:\n",
        "                x = cross_layer(x, encoder_input=patch_embs, encoder_mask=mask)\n",
        "\n",
        "        # Compute logits\n",
        "        logits = self.output(x)  # Shape: [batch_size, seq_len, vocab_size]\n",
        "        return logits\n",
        "\n",
        "class LocalEncoderWithPooling(nn.Module):\n",
        "    def __init__(self, base_encoder, cross_attn_layers, embed_dim, global_dim):\n",
        "        super().__init__()\n",
        "        self.base_encoder = base_encoder\n",
        "        self.cross_attn_layers = cross_attn_layers\n",
        "        self.patch_projector = PatchToGlobalProjector(embed_dim, global_dim)\n",
        "\n",
        "    def forward(self, bytes, patch_ids):\n",
        "        # Get byte-level embeddings from the base encoder\n",
        "        byte_embeds = self.base_encoder(bytes, mask=None)\n",
        "        byte_embeds = byte_embeds.to(torch.bfloat16)\n",
        "\n",
        "        # Apply cross-attention if layers exist\n",
        "        if self.cross_attn_layers:\n",
        "            # num_patches = patch_ids.max().item() + 1\n",
        "            local_encoder_mask = (patch_ids.unsqueeze(1) == patch_ids.unsqueeze(2))  # Shape: [batch_size, seq_len, seq_len]\n",
        "            patch_embs = byte_embeds  # Start with byte embeddings\n",
        "            for cross_layer in self.cross_attn_layers:\n",
        "                patch_embs = cross_layer(patch_embs, encoder_input=byte_embeds, encoder_mask=local_encoder_mask)\n",
        "        else:\n",
        "            patch_embs = byte_embeds\n",
        "\n",
        "        # Reduce byte-level embeddings to patch-level embeddings\n",
        "        patch_embs = patch_reduce(patch_embs, patch_ids, reduce_op=\"mean\")  # Shape: [batch_size, num_patches, embed_dim]\n",
        "\n",
        "        # Project to global dimension\n",
        "        patch_embs = self.patch_projector(patch_embs)  # Shape: [batch_size, num_patches, global_dim]\n",
        "\n",
        "        return byte_embeds, patch_embs  # Return both byte embeddings and reduced patch embeddings\n",
        "\n",
        "def build_local_encoder(\n",
        "    global_dim: int,\n",
        "    vocab_size: int = VOCAB_SIZE,\n",
        "    embed_dim: int = 2048,\n",
        "    num_heads: int = 8,\n",
        "    num_kv_heads: int = 8,\n",
        "    hidden_dim: int = 4096,\n",
        "    norm_eps: float = 1e-5,\n",
        "    attn_dropout: float = 0.0,\n",
        "    max_seq_len: int = 2048,\n",
        "    num_layers: int = 4,\n",
        "    num_cross_layers = 4,\n",
        "    dtype=torch.bfloat16,\n",
        "    use_hash_ngrams=True,\n",
        "    max_ngram: int = 8,\n",
        "    num_ngram_buckets: int = 500000,\n",
        "):\n",
        "    head_dim = embed_dim // num_heads\n",
        "\n",
        "    if use_hash_ngrams:\n",
        "        tok_embeddings = HashNGramEmbedder(\n",
        "            embed_dim=embed_dim,\n",
        "            max_n=max_ngram,\n",
        "            num_buckets=num_ngram_buckets,\n",
        "            vocab_size=vocab_size\n",
        "        )\n",
        "    else:\n",
        "        tok_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    # Build self-attention layers with Qwen MLP\n",
        "    layers = nn.ModuleList()\n",
        "    for _ in range(num_layers):\n",
        "        # TODO: KV cache?\n",
        "        self_attn = MultiHeadAttention(\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_kv_heads=num_kv_heads,\n",
        "            head_dim=head_dim,\n",
        "            q_proj=nn.Linear(embed_dim, num_heads * head_dim, bias=True),\n",
        "            k_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=True),\n",
        "            v_proj=nn.Linear(embed_dim, num_kv_heads * head_dim, bias=True),\n",
        "            output_proj=nn.Linear(embed_dim, embed_dim, bias=False),\n",
        "            max_seq_len=max_seq_len,\n",
        "            attn_dropout=attn_dropout,\n",
        "            is_causal=True,\n",
        "        )\n",
        "        mlp = qwen2_mlp(dim=embed_dim, hidden_dim=hidden_dim)\n",
        "        layer = TransformerSelfAttentionLayer(\n",
        "            attn=self_attn,\n",
        "            mlp=mlp,\n",
        "            sa_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "            mlp_norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "        )\n",
        "        layers.append(layer)\n",
        "\n",
        "    base_encoder = TransformerDecoder(\n",
        "        tok_embeddings=tok_embeddings,\n",
        "        layers=layers,\n",
        "        max_seq_len=max_seq_len,\n",
        "        num_heads=num_heads,\n",
        "        head_dim=head_dim,\n",
        "        norm=RMSNorm(embed_dim, eps=norm_eps),\n",
        "        output=nn.Identity(),  # no final projection\n",
        "    )\n",
        "\n",
        "    # Cross-attention layers (optional)\n",
        "    cross_attn_layers = nn.ModuleList()\n",
        "    for _ in range(num_cross_layers):\n",
        "        cross_attn = MultiHeadAttention(\n",
        "            embed_dim=global_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_kv_heads=num_kv_heads,\n",
        "            head_dim=head_dim,\n",
        "            q_proj=nn.Linear(global_dim, num_heads * head_dim, bias=True),\n",
        "            k_proj=nn.Linear(global_dim, num_kv_heads * head_dim, bias=True),\n",
        "            v_proj=nn.Linear(global_dim, num_kv_heads * head_dim, bias=True),\n",
        "            output_proj=nn.Linear(global_dim, global_dim, bias=False),\n",
        "            max_seq_len=max_seq_len,\n",
        "            attn_dropout=attn_dropout,\n",
        "            is_causal=False,\n",
        "        )\n",
        "        mlp = qwen2_mlp(dim=embed_dim, hidden_dim=hidden_dim)\n",
        "        cross_layer = TransformerCrossAttentionLayer(\n",
        "            attn=cross_attn,\n",
        "            mlp=mlp,\n",
        "            ca_norm=RMSNorm(global_dim, eps=norm_eps),\n",
        "            mlp_norm=RMSNorm(global_dim, eps=norm_eps),\n",
        "        )\n",
        "        cross_attn_layers.append(cross_layer)\n",
        "\n",
        "    local_encoder = LocalEncoderWithPooling(base_encoder, cross_attn_layers, embed_dim, global_dim)\n",
        "    return local_encoder.to(dtype=dtype)\n",
        "\n",
        "################################################\n",
        "# dynamic patching\n",
        "################################################\n",
        "\n",
        "def compute_local_entropy(bytes_tensor, window_size=8):\n",
        "    \"\"\"Return a per-token \"entropy\" measure to guide patching\n",
        "\n",
        "    Arguments:\n",
        "        bytes_tensor: Torch.tensor[batch_size, seq_len] byttes to calc entropy on\n",
        "        window_size: int size to window across\n",
        "\n",
        "        local_entropy: torch.Tensor[batch_size, seq_len]\n",
        "    \"\"\"\n",
        "    # bytes_tensor:\n",
        "    device = bytes_tensor.device\n",
        "    batch_size, seq_len = bytes_tensor.shape\n",
        "\n",
        "    # We’ll keep a sliding frequency table. Initialize all zeros:\n",
        "    freq = torch.zeros(batch_size, VOCAB_SIZE, device=device)\n",
        "    local_entropy = torch.zeros(batch_size, seq_len, device=device)\n",
        "\n",
        "    for pos in range(seq_len):\n",
        "        # add current byte\n",
        "        current_byte = bytes_tensor[:, pos]\n",
        "        freq[torch.arange(batch_size), current_byte] += 1\n",
        "\n",
        "        # compute distribution\n",
        "        dist = freq / freq.sum(dim=1, keepdim=True).clamp_min(1e-8)\n",
        "        # compute -p*log2(p)\n",
        "        ent = -(dist * (dist + 1e-8).log2()).sum(dim=1)\n",
        "        local_entropy[:, pos] = ent\n",
        "\n",
        "        # remove oldest byte if we exceed window size\n",
        "        if pos >= window_size:\n",
        "            oldest_byte = bytes_tensor[:, pos - window_size]\n",
        "            freq[torch.arange(batch_size), oldest_byte] -= 1\n",
        "    return local_entropy\n",
        "\n",
        "def dynamic_patch(\n",
        "    bytes_tensor: torch.Tensor,\n",
        "    threshold: float = 3.0,   # starting entropy threshold in bits\n",
        "    min_threshold: float = 2.0,    # lower bound\n",
        "    max_threshold: float = 5.0,    # upper bound\n",
        "    threshold_step_down: float = 0.1,  # how much to decrease threshold if no patches triggered\n",
        "    threshold_step_up: float = 0.1,    # how much to increase threshold if we trigger a patch\n",
        "    patch_size: int = 4,\n",
        "    window_size: int = 8\n",
        "):\n",
        "    \"\"\"\n",
        "    A dynamic patching approach that adjusts the entropy threshold\n",
        "    upward/downward depending on whether patches are being triggered too often or not enough.\n",
        "\n",
        "    Args:\n",
        "        bytes_tensor: [batch_size, seq_len]\n",
        "        threshold: initial bits threshold for local entropy\n",
        "        min_threshold, max_threshold: clamp thresholds\n",
        "        threshold_step_down, threshold_step_up: step sizes\n",
        "        patch_size: max patch length if we haven't triggered a boundary earlier\n",
        "        window_size: for computing local entropy\n",
        "\n",
        "    Returns:\n",
        "        patch_ids: [batch_size, seq_len] with patch ID for each token\n",
        "        local_ent: [batch_size, seq_len] local entropy in bits\n",
        "    \"\"\"\n",
        "    local_ent = compute_local_entropy(bytes_tensor, window_size=window_size)\n",
        "    batch_size, seq_len = bytes_tensor.shape\n",
        "\n",
        "    # Each row’s patch assignment\n",
        "    patch_ids = torch.zeros(batch_size, seq_len, dtype=torch.long, device=bytes_tensor.device)\n",
        "\n",
        "    current_patch = torch.zeros(batch_size, dtype=torch.long, device=bytes_tensor.device)\n",
        "    patch_lengths = torch.zeros(batch_size, dtype=torch.long, device=bytes_tensor.device)\n",
        "\n",
        "    # Keep track of how many consecutive tokens we have processed w/o triggering any new patch\n",
        "    # so we can adjust threshold if we go too long\n",
        "    consecutive_no_trigger = 0\n",
        "\n",
        "    for pos in range(seq_len):\n",
        "        patch_lengths += 1\n",
        "\n",
        "        # Which batch elements exceed threshold or hit patch size\n",
        "        trigger_new_patch = (local_ent[:, pos] > threshold) | (patch_lengths >= patch_size)\n",
        "        triggered_rows = trigger_new_patch.nonzero(as_tuple=False).flatten()\n",
        "\n",
        "        if len(triggered_rows) > 0:\n",
        "            # For each row that triggered, increment patch and reset patch_lengths\n",
        "            current_patch[triggered_rows] += 1\n",
        "            patch_lengths[triggered_rows] = 0\n",
        "\n",
        "            # Because at least 1 row triggered a patch, we can optionally adjust threshold upward\n",
        "            # or downward. For example, *raising* threshold if we keep splitting too often:\n",
        "            threshold = min(threshold + threshold_step_up, max_threshold)\n",
        "\n",
        "            consecutive_no_trigger = 0\n",
        "        else:\n",
        "            # No new patch was triggered\n",
        "            consecutive_no_trigger += 1\n",
        "            # If we haven't triggered for a while, lower threshold so that we become more likely\n",
        "            # to split in the future.\n",
        "            threshold = max(threshold - threshold_step_down, min_threshold)\n",
        "\n",
        "        patch_ids[:, pos] = current_patch\n",
        "\n",
        "    return patch_ids, local_ent\n",
        "\n",
        "def patch_reduce(h, patch_ids, reduce_op=\"mean\"):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        h: [batch_size, seq_len, emb_dim]\n",
        "        patch_ids: [batch_size, seq_len]\n",
        "        reduce_op: e.g. \"mean\", \"amin\", \"amax\"\n",
        "\n",
        "    returns: [batch_size, num_patches, emb_dim]\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, emb_dim = h.shape\n",
        "    num_patches = patch_ids.max().item() + 1  # patch IDs go from 0..max\n",
        "\n",
        "    # expand dims so we can scatter:\n",
        "    expanded_ids = patch_ids.unsqueeze(-1).expand(-1, -1, emb_dim)\n",
        "    reduced = torch.zeros(batch_size, num_patches, emb_dim, device=h.device, dtype=h.dtype)\n",
        "    reduced = reduced.scatter_reduce(\n",
        "        dim=1,\n",
        "        index=expanded_ids,\n",
        "        src=h,\n",
        "        reduce=reduce_op,\n",
        "        include_self=False,\n",
        "    )\n",
        "    return reduced\n",
        "\n",
        "def compute_patch_size(so_far: torch.Tensor, threshold=3.0, max_patch=8):\n",
        "    \"\"\"\n",
        "    heuristic function for deciding the patch length\n",
        "    based on dynamic_patch logic or local entropy.\n",
        "\n",
        "    so_far: [seq_len] or [1, seq_len], the current context of tokens (including newly generated ones).\n",
        "    threshold: approximate bits threshold for deciding to break the patch.\n",
        "    max_patch: a max patch length to avoid overly large chunks.\n",
        "\n",
        "    Returns:\n",
        "        predicted_patch_size: int\n",
        "            the number of bytes to decode in the *next* patch in a single forward pass\n",
        "    \"\"\"\n",
        "    # re-run dynamic_patch() on the entire sequence and see how big the last patch is.\n",
        "    # this is probably pretty wasteful!s\n",
        "    # Then we decide how big the *next* patch would be if we continued.\n",
        "    # This is a simple way to reuse your dynamic_patch code.\n",
        "    if so_far.dim() == 1:\n",
        "        so_far = so_far.unsqueeze(0)  # [batch=1, seq_len]\n",
        "\n",
        "    patch_ids, _ = dynamic_patch(so_far, threshold=threshold, patch_size=max_patch)\n",
        "\n",
        "    # The ID of the last patch in that sequence:\n",
        "    last_patch_id = patch_ids[0, -1].item()  # e.g. 3 means patches 0..3\n",
        "    # Count how many tokens so far belong to the last patch\n",
        "    # (i.e. sum of patch_ids == last_patch_id)\n",
        "    count_last_patch = (patch_ids[0] == last_patch_id).sum().item()\n",
        "\n",
        "    # guess that the next patch might be similar in size:\n",
        "    # If we already used up to 'count_last_patch' tokens for the last patch,\n",
        "    # we can try the same or smaller for the next patch. A simple approach is:\n",
        "    predicted_size = max(1, min(count_last_patch, max_patch))\n",
        "\n",
        "    return predicted_size\n",
        "\n",
        "################################################\n",
        "# Projection layer\n",
        "################################################\n",
        "class PatchToGlobalProjector(nn.Module):\n",
        "    def __init__(self, local_dim, global_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(local_dim, global_dim)\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "################################################\n",
        "# ByteLatentQwen2p5Decoder with cross-attn\n",
        "################################################\n",
        "class ByteLatentQwen2p5Decoder(TransformerDecoder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        qwen_cfg: Dict[str, Any],\n",
        "        local_encoder_cfg: Dict[str, Any],\n",
        "        patch_size: int = 4,\n",
        "        patching_threshold: float = 3.0,\n",
        "        freeze_global_for_n_steps: int = 0,\n",
        "    ):\n",
        "        layers = nn.ModuleList()\n",
        "        head_dim = qwen_cfg['embed_dim'] // qwen_cfg['num_heads']\n",
        "\n",
        "        for _ in range(qwen_cfg['num_layers']):\n",
        "            self_attn = MultiHeadAttention(\n",
        "                embed_dim=qwen_cfg['embed_dim'],\n",
        "                num_heads=qwen_cfg['num_heads'],\n",
        "                num_kv_heads=qwen_cfg['num_kv_heads'],\n",
        "                head_dim=head_dim,\n",
        "                q_proj=nn.Linear(qwen_cfg['embed_dim'], qwen_cfg['num_heads'] * head_dim, bias=True),\n",
        "                k_proj=nn.Linear(qwen_cfg['embed_dim'], qwen_cfg['num_kv_heads'] * head_dim, bias=True),\n",
        "                v_proj=nn.Linear(qwen_cfg['embed_dim'], qwen_cfg['num_kv_heads'] * head_dim, bias=True),\n",
        "                output_proj=nn.Linear(qwen_cfg['embed_dim'], qwen_cfg['embed_dim'], bias=False),\n",
        "                kv_cache=None,\n",
        "                max_seq_len=qwen_cfg['max_seq_len'],\n",
        "                attn_dropout=qwen_cfg['attn_dropout'],\n",
        "            )\n",
        "            mlp = qwen2_mlp(dim=qwen_cfg['embed_dim'], hidden_dim=qwen_cfg['intermediate_dim'])\n",
        "            layer = TransformerSelfAttentionLayer(\n",
        "                attn=self_attn,\n",
        "                mlp=mlp,\n",
        "                sa_norm=RMSNorm(dim=qwen_cfg['embed_dim'], eps=qwen_cfg['norm_eps']),\n",
        "                mlp_norm=RMSNorm(dim=qwen_cfg['embed_dim'], eps=qwen_cfg['norm_eps']),\n",
        "            )\n",
        "            layers.append(layer)\n",
        "\n",
        "        output = nn.Linear(\n",
        "            qwen_cfg['embed_dim'],\n",
        "            VOCAB_SIZE  ,  # bytes\n",
        "            bias=False,\n",
        "        )\n",
        "        # Initialize output bytes.\n",
        "        init_std = qwen_cfg['embed_dim'] ** -0.5\n",
        "        with torch.no_grad():\n",
        "            nn.init.trunc_normal_(\n",
        "                output.weight,\n",
        "                mean=0.0,\n",
        "                std=init_std,\n",
        "                a=-3*init_std,\n",
        "                b=3*init_std,\n",
        "            )\n",
        "\n",
        "        output = nn.Identity()  # Global transformer doesn’t output logits\n",
        "        emb = nn.Identity()\n",
        "        super().__init__(\n",
        "            tok_embeddings=emb,\n",
        "            layers=layers,\n",
        "            max_seq_len=qwen_cfg['max_seq_len'],\n",
        "            num_heads=qwen_cfg['num_heads'],\n",
        "            head_dim=head_dim,\n",
        "            norm=RMSNorm(qwen_cfg['embed_dim'], eps=qwen_cfg['norm_eps']),\n",
        "            output=output,\n",
        "        )\n",
        "        self.local_encoder = build_local_encoder(**local_encoder_cfg, global_dim=qwen_cfg['embed_dim'])\n",
        "        self.local_decoder = LocalDecoder(\n",
        "            embed_dim=qwen_cfg['embed_dim'],\n",
        "            global_dim=qwen_cfg['embed_dim'],\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            max_seq_len=local_encoder_cfg['max_seq_len'],\n",
        "        )\n",
        "\n",
        "         # Collect parameters for different learning rates\n",
        "        self.qwen_params = []\n",
        "        self.new_params = []\n",
        "\n",
        "        # Qwen parts\n",
        "        self.qwen_params.extend(self.norm.parameters())\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, TransformerSelfAttentionLayer):\n",
        "                self.qwen_params.extend(layer.parameters())\n",
        "\n",
        "        # New parts\n",
        "        self.new_params.extend(self.tok_embeddings.parameters())\n",
        "        self.new_params.extend(self.output.parameters())\n",
        "        self.new_params.extend(self.local_encoder.parameters())\n",
        "        self.new_params.extend(self.local_decoder.parameters())\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.patching_threshold = patching_threshold\n",
        "        self.freeze_global_for_n_steps = freeze_global_for_n_steps\n",
        "        self.current_step = 0\n",
        "        self.global_frozen = freeze_global_for_n_steps > 0\n",
        "        if self.global_frozen:\n",
        "            self._update_freezing()\n",
        "\n",
        "        # We'll store how many chunks the user wants for final output\n",
        "        self.num_output_chunks = 0  # default\n",
        "\n",
        "    def _update_freezing(self):\n",
        "        if self.global_frozen:\n",
        "            for param in self.qwen_params:\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def set_num_output_chunks(self, num_output_chunks: int) -> None:\n",
        "        super().set_num_output_chunks(num_output_chunks)\n",
        "        self.num_output_chunks = num_output_chunks\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tokens: torch.Tensor,\n",
        "        *,\n",
        "        mask: Optional[Union[torch.Tensor, float]] = None,\n",
        "        encoder_input: Optional[torch.Tensor] = None,\n",
        "        encoder_mask: Optional[torch.Tensor] = None,\n",
        "        input_pos: Optional[torch.Tensor] = None,\n",
        "        patch_ids: Optional[torch.Tensor] = None,\n",
        "    ) -> Union[torch.Tensor, List[torch.Tensor]]:\n",
        "        # Update freezing state if in training\n",
        "        if self.training and self.global_frozen:\n",
        "            self.current_step += 1\n",
        "            if self.current_step >= self.freeze_global_for_n_steps:\n",
        "                self.global_frozen = False\n",
        "                self._update_freezing()\n",
        "\n",
        "        if patch_ids is None:\n",
        "            patch_ids, _ = dynamic_patch(tokens, threshold=self.patching_threshold, patch_size=self.patch_size)\n",
        "\n",
        "        byte_embeds, patch_embs = self.local_encoder(tokens, patch_ids=patch_ids)\n",
        "        global_out = super().forward(patch_embs, mask=mask, input_pos=input_pos)\n",
        "\n",
        "        # Assuming the outs are chunked, take entry[0] of outputs.\n",
        "        if self.num_output_chunks == 0:\n",
        "            global_out = global_out.to(torch.bfloat16) # TODO: Another fix for the 0 chunk float. Need to move this dtype definiton.\n",
        "            logits = self.local_decoder(byte_embeds, global_out, patch_ids)\n",
        "        else:\n",
        "            global_out_combined = torch.cat(global_out, dim=1) # Unchunking - it would be better not to pass this through I think.\n",
        "            clogits = self.local_decoder(byte_embeds, global_out_combined, patch_ids)\n",
        "            logits = [chunk for chunk in clogits.chunk(self.num_output_chunks, dim=1)]\n",
        "        return logits\n",
        "\n",
        "    def unified_generate(\n",
        "        self,\n",
        "        prompt: Union[torch.LongTensor, List[int]],\n",
        "        max_new_tokens: int = 128,\n",
        "        eos_id: int = EOS_ID,\n",
        "        temperature: float = 0.7,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0,\n",
        "        frequency_penalty: float = 0.1,\n",
        "        repetition_penalty: float = 1.5,\n",
        "        greedy: bool = False,\n",
        "    ) -> torch.Tensor:\n",
        "        # 1) Convert prompt to a tensor [1, seq_len]\n",
        "        if isinstance(prompt, list):\n",
        "            prompt = torch.tensor(prompt, dtype=torch.long, device=next(self.parameters()).device)\n",
        "        if prompt.dim() == 1:\n",
        "            prompt = prompt.unsqueeze(0) # Add a batch dimension.\n",
        "\n",
        "        device = prompt.device\n",
        "        all_tokens = prompt.clone()\n",
        "\n",
        "        # Track usage for freq / repetition penalty\n",
        "        token_counts = torch.bincount(all_tokens[0], minlength=VOCAB_SIZE).to(device)\n",
        "        recent_tokens = []\n",
        "\n",
        "        # Start generating\n",
        "        for step_i in range(max_new_tokens):\n",
        "\n",
        "            # No patching => standard forward\n",
        "            with torch.no_grad():\n",
        "                logits = self.forward(all_tokens)\n",
        "\n",
        "            if isinstance(logits, list):\n",
        "                logits = torch.cat(logits, dim=1)\n",
        "            step_logits = logits[0, -1, :]\n",
        "\n",
        "            # Apply temperature\n",
        "            if temperature != 1.0:\n",
        "                step_logits = step_logits / temperature\n",
        "\n",
        "            # Repetition penalty\n",
        "            for tk in recent_tokens:\n",
        "                step_logits[tk] /= repetition_penalty\n",
        "\n",
        "            # Frequency penalty\n",
        "            step_logits -= token_counts * frequency_penalty\n",
        "\n",
        "            # top-k\n",
        "            if top_k > 0:\n",
        "                vals, idxs = torch.topk(step_logits, top_k)\n",
        "                mask = torch.ones_like(step_logits, dtype=torch.bool)\n",
        "                mask[idxs] = False\n",
        "                step_logits[mask] = float('-inf')\n",
        "\n",
        "            # top-p\n",
        "            probs = torch.softmax(step_logits, dim=-1)\n",
        "            if top_p > 0:\n",
        "                sorted_logits, sorted_indices = torch.sort(step_logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(probs, dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "                sorted_indices_to_remove[:, 0] = 0\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                step_logits = step_logits.scatter(-1, indices_to_remove, -1e9)\n",
        "\n",
        "            # Re‐normalize\n",
        "            total_p = probs.sum()\n",
        "            if total_p > 0:\n",
        "                probs /= total_p\n",
        "            else:\n",
        "                probs = torch.ones_like(probs) / probs.size(-1)\n",
        "\n",
        "            # Next token\n",
        "            if greedy:\n",
        "                next_token = torch.argmax(probs).item()\n",
        "            else:\n",
        "                next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # Add new token\n",
        "            all_tokens = torch.cat(\n",
        "                [all_tokens, torch.tensor([[next_token]], device=device)],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "            # Update counters\n",
        "            token_counts[next_token] += 1\n",
        "            recent_tokens.append(next_token)\n",
        "            if len(recent_tokens) > 5:\n",
        "                recent_tokens.pop(0)\n",
        "\n",
        "            # EOS break\n",
        "            if next_token == eos_id:\n",
        "                break\n",
        "\n",
        "        return all_tokens\n",
        "\n",
        "############################\n",
        "# Tokenizer\n",
        "############################\n",
        "\n",
        "class ByteLatentModelTokenizer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        bpe_delim: bool = False,\n",
        "        add_bos: bool = True,\n",
        "        add_eos: bool = True,\n",
        "        special_tokens: Optional[Dict[str, int]] = None,\n",
        "        max_seq_len: Optional[int] = None,\n",
        "        prompt_template: Optional[Any] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if special_tokens is None:\n",
        "            special_tokens = {\n",
        "                \"<|bos|>\": BOS_ID,\n",
        "                \"<|eos|>\": EOS_ID,\n",
        "                \"<|pad|>\": PAD_ID,\n",
        "            }\n",
        "        self.special_tokens = special_tokens\n",
        "        self.bos_id = self.special_tokens.get(\"<|bos|>\", BOS_ID)\n",
        "        self.eos_id = self.special_tokens.get(\"<|eos|>\", EOS_ID)\n",
        "        self.pad_id = self.special_tokens.get(\"<|pad|>\", PAD_ID)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.prompt_template = prompt_template\n",
        "\n",
        "    def encode(self, text: str, add_bos: bool = True, add_eos: bool = True) -> List[int]:\n",
        "        # naive UTF-8 byte approach:\n",
        "        byte_data = list(bytes(text, encoding=\"utf-8\", errors=\"ignore\"))\n",
        "        tokens = byte_data\n",
        "        if add_bos:\n",
        "            tokens = [self.bos_id] + tokens\n",
        "        if add_eos:\n",
        "            tokens = tokens + [self.eos_id]\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens: List[int]) -> str:\n",
        "        # naive decode - skip special tokens\n",
        "        return bytes([t for t in tokens if t < 256]).decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "\n",
        "    def tokenize_messages(self, messages: List[Dict[str, Any]], add_eos: bool = True) -> Tuple[List[int], List[bool]]:\n",
        "        tokenized_messages = []\n",
        "        mask = []\n",
        "        for message in messages:\n",
        "            if message.role != \"ipython\":\n",
        "                tokenized_messages.extend(self.encode(\"\".join([message.role, \"\\n\"]), add_bos=False, add_eos=False))\n",
        "            for item in message.content:\n",
        "                if item['type'] == \"text\":\n",
        "                    tokenized_messages.extend(\n",
        "                        self.encode(item['content'], add_bos=False, add_eos=False)\n",
        "                    )\n",
        "        if add_eos:\n",
        "            tokenized_messages.append(self.eos_id)\n",
        "        mask = [False] * len(tokenized_messages)\n",
        "        if self.max_seq_len:\n",
        "            tokenized_messages = tokenized_messages[: self.max_seq_len]\n",
        "            mask = mask[: self.max_seq_len]\n",
        "        return tokenized_messages, mask\n",
        "\n",
        "    def forward(self, sample: Mapping[str, Any], inference: bool = False) -> Mapping[str, Any]:\n",
        "        messages = sample.pop(\"messages\")\n",
        "        tokens, mask = self.tokenize_messages(messages)\n",
        "        sample[\"tokens\"] = tokens\n",
        "        sample[\"mask\"] = mask\n",
        "        return sample\n",
        "\n",
        "############################\n",
        "# Helper constructor\n",
        "############################\n",
        "\n",
        "def blt_tokenizer(\n",
        "    special_tokens_path: Optional[str] = None,\n",
        "    max_seq_len: Optional[int] = None,\n",
        "    prompt_template: Optional[Any] = None,\n",
        "    **kwargs,\n",
        ") -> ByteLatentModelTokenizer:\n",
        "    return ByteLatentModelTokenizer(\n",
        "        max_seq_len=max_seq_len,\n",
        "        prompt_template=prompt_template,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "# Define Qwen 2.5 base model with additional layers, we will expect this to be loaded\n",
        "# with a pretrained Qwen checkpoint which should match.\n",
        "def qwen2_5_blt(\n",
        "    # Warm up decoder/encoder. Tried it with both 100 and 0 and it seemed to work better without\n",
        "    # the warmup, but never tested a longer warmup.\n",
        "    freeze_global_for_n_steps=500,\n",
        "    use_hash_ngrams=True,\n",
        "    patch_size=4,\n",
        "    max_seq_len=4096\n",
        ") -> ByteLatentQwen2p5Decoder:\n",
        "    qwen_cfg = dict(\n",
        "        vocab_size=151936, # Kinda irrelevant\n",
        "        embed_dim=2048,\n",
        "        num_layers=36,\n",
        "        num_heads=16,\n",
        "        num_kv_heads=2,\n",
        "        max_seq_len=max_seq_len,\n",
        "        intermediate_dim=11008,\n",
        "        attn_dropout=0.0,\n",
        "        norm_eps=1e-6,\n",
        "    )\n",
        "\n",
        "    local_enc_cfg = dict(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        embed_dim=2048, # Keeping same as Qwen\n",
        "        num_layers=4,\n",
        "        num_heads=8,\n",
        "        num_kv_heads=8,\n",
        "        max_seq_len=max_seq_len,\n",
        "        hidden_dim=4096,\n",
        "        norm_eps=1e-5,\n",
        "        num_cross_layers = 4,\n",
        "        use_hash_ngrams=use_hash_ngrams,\n",
        "        max_ngram= 8,\n",
        "        num_ngram_buckets=500000,\n",
        "    )\n",
        "\n",
        "    return ByteLatentQwen2p5Decoder(\n",
        "        qwen_cfg=qwen_cfg,\n",
        "        local_encoder_cfg=local_enc_cfg,\n",
        "        patch_size=patch_size,\n",
        "        patching_threshold=3.0,\n",
        "        freeze_global_for_n_steps=freeze_global_for_n_steps\n",
        "    )\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oWuFBudReQ4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/model/local_models.py\n",
        "# perceiver cross attn\n",
        "\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/model/latent_transformer.py#L30\n"
      ],
      "metadata": {
        "id": "YKEI0ztP7mRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title facebookresearch patcher.py entropy monotonicity\n",
        "\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/data/patcher.py\n",
        "\n",
        "def entropy(scores):\n",
        "    \"\"\"\n",
        "    scores: [bs, seq_len, vocab]\n",
        "    returns [bs, seq_len]\n",
        "    Computes the entropy for each token in the batch.\n",
        "    Note: uses natural log.\n",
        "    \"\"\"\n",
        "    log_probs = F.log_softmax(scores, dim=-1)\n",
        "    probs = torch.exp(log_probs)\n",
        "    p_log_p = log_probs * probs\n",
        "    entropy = -p_log_p.sum(dim=-1)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def patch_start_mask_global_and_monotonicity(entropies, t, t_add=0):\n",
        "    \"\"\"\n",
        "    entropies: [bs, seq_len] torch tensor of entropies\n",
        "    t: threshold\n",
        "    returns [bs, seq_len] mask where True indicates the start of a patch\n",
        "    \"\"\"\n",
        "    bs, seq_len = entropies.shape\n",
        "    if seq_len == 0: return entropies > t\n",
        "    mask = torch.zeros_like(entropies, dtype=torch.bool)\n",
        "    mask[:, 0] = True\n",
        "    # Calculate differences between consecutive elements along the sequence length\n",
        "    differences = entropies[:, 1:] - entropies[:, :-1]\n",
        "    # Calculate conditions for all elements except the first one in each sequence\n",
        "    # condition = (differences > t_add) & (entropies[:, 1:] > t) & (~mask[:, :-1])\n",
        "    condition = differences > t_add\n",
        "    # Update the mask based on the condition\n",
        "    mask[:, 1:] = condition\n",
        "    return mask\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "P0RNtIeqzs9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJq9KRhWIIiT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch bytelatent/model/blt.py\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/model/blt.py\n",
        "from enum import Enum, auto\n",
        "from typing import Any, Optional\n",
        "\n",
        "import torch\n",
        "from pydantic import ConfigDict, model_validator\n",
        "from torch import nn\n",
        "from torch.nn.attention.flex_attention import create_block_mask\n",
        "from typing_extensions import Self\n",
        "\n",
        "from bytelatent.base_transformer import (\n",
        "    BaseTransformerArgs,\n",
        "    InitStdFactor,\n",
        "    SequenceModelWithOutput,\n",
        "    TransformerBlock,\n",
        ")\n",
        "from bytelatent.data.patcher import Patcher, PatcherArgs\n",
        "from bytelatent.model.latent_transformer import GlobalTransformer\n",
        "from bytelatent.model.local_models import LocalDecoder, LocalEncoder, LocalModelArgs\n",
        "from bytelatent.model.utils import downsample\n",
        "from bytelatent.tokenizers.constants import BOE_ID, BOS_ID, EOS_ID, OFFSET, PAD_ID\n",
        "\n",
        "\n",
        "\n",
        "def causal_mask(b, h, q_idx, kv_idx):\n",
        "    return q_idx >= kv_idx\n",
        "\n",
        "\n",
        "def setattrs(_self, **kwargs):\n",
        "    for k, v in kwargs.items():\n",
        "        setattr(_self, k, v)\n",
        "\n",
        "\n",
        "def get_encoder_dim_token_emb(args):\n",
        "    if args.dim_token is not None:\n",
        "        dim_token_emb = args.dim_token\n",
        "    elif args.use_local_encoder_transformer:\n",
        "        dim_token_emb = args.dim_local_encoder\n",
        "    else:\n",
        "        dim_token_emb = args.dim_global // args.patch_size\n",
        "    return dim_token_emb\n",
        "\n",
        "\n",
        "def get_encoder_dim_patch_emb(args):\n",
        "    dim_patch_emb = None\n",
        "    if args.cross_attn_encoder:\n",
        "        if args.cross_attn_init_by_pooling:\n",
        "            dim_patch_emb = args.dim_local_encoder\n",
        "        else:\n",
        "            dim_patch_emb = args.dim_global\n",
        "    return dim_patch_emb\n",
        "\n",
        "\n",
        "def get_global_dim_patch_emb(args):\n",
        "    dim_token_emb = get_encoder_dim_token_emb(args)\n",
        "    if args.cross_attn_encoder:\n",
        "        dim_patch_emb = dim_token_emb * args.cross_attn_k\n",
        "    elif (\n",
        "        args.downsampling_by_pooling is None\n",
        "        or not args.downsampling_by_pooling\n",
        "        or len(args.downsampling_by_pooling) == 0\n",
        "    ):\n",
        "        dim_patch_emb = dim_token_emb * args.patch_size\n",
        "    else:\n",
        "        dim_patch_emb = dim_token_emb * sum([pooling in args.downsampling_by_pooling for pooling in [\"avg\", \"min\", \"max\"]])\n",
        "    return dim_patch_emb\n",
        "\n",
        "\n",
        "def get_decoder_dim_token_emb(args):\n",
        "    if args.share_encoder_decoder_emb:\n",
        "        dim_token_emb = get_encoder_dim_token_emb(args)\n",
        "    elif args.dim_token is not None:\n",
        "        dim_token_emb = args.dim_token\n",
        "    else:\n",
        "        dim_token_emb = args.dim_local_decoder\n",
        "    return dim_token_emb\n",
        "\n",
        "\n",
        "def parse_ngram_to_size(ngram_to_size_str: str | None) -> dict[int, int]:\n",
        "    if ngram_to_size_str is None:\n",
        "        return None\n",
        "    ngram_to_size = {}\n",
        "    for entry in ngram_to_size_str.split(\",\"):\n",
        "        ngram, size = entry.split(\":\")\n",
        "        ngram = int(ngram)\n",
        "        size = int(size)\n",
        "        ngram_to_size[ngram] = size\n",
        "    return ngram_to_size\n",
        "\n",
        "\n",
        "def fill_tokens(tokens, patch_size, fill_id):\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    if seq_len % patch_size == 0:\n",
        "        return tokens\n",
        "    else:\n",
        "        remaining = patch_size - seq_len % patch_size\n",
        "        final_padding = tokens.new(batch_size, remaining).fill_(fill_id)\n",
        "        return torch.cat((tokens, final_padding), dim=1)\n",
        "\n",
        "\n",
        "def decoder_patch_ids_from_lengths(patch_lengths, nb_boe, seq_len):\n",
        "    first_patch_length = patch_lengths[0, 0]\n",
        "    assert torch.all(\n",
        "        first_patch_length == patch_lengths[:, 0]\n",
        "    ), \"first patch should always be the same size (1 for dynamic, patch_size for static).\"\n",
        "    assert (\n",
        "        first_patch_length - nb_boe == 1\n",
        "    ), f\"First patch (patch length: {first_patch_length}) should have one non-boe token (boe toks: {nb_boe})\"\n",
        "    # Remove first patch from patch_ids for local decoder inputs and shift the last patch.\n",
        "    # decoder_patch_lengths = patch_lengths[:, 1:].clone()\n",
        "    # decoder_patch_lengths = add_to_last_nonzero_patch(decoder_patch_lengths, 1)\n",
        "    decoder_patch_lengths = patch_lengths[:, 1:]\n",
        "    assert (\n",
        "        decoder_patch_lengths.sum() + (nb_boe + 1) * patch_lengths.shape[0]\n",
        "        == patch_lengths.sum()\n",
        "    ), f\"{decoder_patch_lengths.sum() + (nb_boe + 1) * patch_lengths.shape[0]} != {patch_lengths.sum()}\"\n",
        "    assert torch.all(decoder_patch_lengths >= 0), f\"{decoder_patch_lengths}\"\n",
        "    decoder_patch_ids = patch_ids_from_lengths(\n",
        "        patch_lengths=decoder_patch_lengths, seq_len=seq_len\n",
        "    )\n",
        "    return decoder_patch_ids\n",
        "\n",
        "\n",
        "primes = [\n",
        "    1000000007,\n",
        "    5915587277,\n",
        "    1500450271,\n",
        "    3267000013,\n",
        "    5754853343,\n",
        "    4093082899,\n",
        "    9576890767,\n",
        "    3628273133,\n",
        "    2860486313,\n",
        "    5463458053,\n",
        "    3367900313,\n",
        "]\n",
        "\n",
        "\n",
        "def rolling_polynomial_hash(t, hash_func_nb: int = 0):\n",
        "    prime = torch.tensor(primes[hash_func_nb], dtype=torch.int64, device=t.device)\n",
        "    prime_powers = torch.stack([prime**i for i in range(t.shape[-1])])\n",
        "    return torch.sum(t * prime_powers, dim=-1)\n",
        "\n",
        "\n",
        "def get_rolling_polynomial_hash_fn(hash_func_nb: int = 0, group_size: int = 2):\n",
        "    prime = torch.tensor(primes[hash_func_nb], dtype=torch.int64)\n",
        "    prime_powers = torch.stack([prime**i for i in range(group_size)])\n",
        "\n",
        "    def rolling_polynomial_hash_fn(t):\n",
        "        return torch.sum(t * prime_powers, dim=-1)\n",
        "\n",
        "    return rolling_polynomial_hash_fn\n",
        "\n",
        "\n",
        "def byte_group_hash_function(\n",
        "    x: torch.Tensor, group_size: int = 2, hash_func_nb: int = 0, max_hash: int = 30000\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a hash of the input x and maps it to a value in the range [0, max_hash].\n",
        "\n",
        "    expects: x of shape (batch_size, seq_len) with values as ids in the token vocab.\n",
        "    returns a tensor  of shape (batch_size, seq_len) with values in the range [0, max_hash].\n",
        "\n",
        "    Note: max hash can make a big difference on the number of collisions.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        bs, seq_len = x.shape\n",
        "        # x_numpy = x.numpy()\n",
        "        # hash_values = torch.zeros(bs, seq_len, dtype=torch.int64, requires_grad=False)\n",
        "        # for i in range(bs):\n",
        "        #     for j in range(seq_len):\n",
        "        #         start = max(j, j-group_size+1)\n",
        "        #         end = j+1\n",
        "        #         hash_values[i, j] = hash_array(x_numpy[i, start:end], max_hash)\n",
        "\n",
        "        prefix = torch.zeros(bs, group_size - 1, dtype=torch.int64, device=x.device)\n",
        "        x = torch.cat([prefix, x], dim=1)\n",
        "        windows = x.unfold(1, group_size, 1)\n",
        "        # hashes = get_rolling_polynomial_hash_fn(hash_func_nb, group_size)(windows)\n",
        "        hashes = rolling_polynomial_hash(windows, hash_func_nb)\n",
        "        hash_values_range = hashes % max_hash\n",
        "    hash_values_range.requires_grad = False\n",
        "    return hash_values_range\n",
        "\n",
        "\n",
        "def create_patch_mask_from_ids(\n",
        "    patch_ids, num_patches, window=None, patches_as_queries=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a tensor of shape [bs, seq_len, num_patches] where each element at position (i, j, k)\n",
        "    is True if the patch id at position (i, j) is less than or equal to k.\n",
        "    Args:\n",
        "        patch_ids (torch.Tensor): Tensor of shape [bs, seq_len] containing patch ids.\n",
        "        num_patches (int): Total number of patches.\n",
        "        window (int): If not None, only considers patches within a window of size window.\n",
        "        patches_as_queries (bool): If True, the patches are used as queries\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of shape [bs, q_len, kv_len] with the desired mask.\n",
        "    \"\"\"\n",
        "    bs, seq_len = patch_ids.shape\n",
        "    if not patches_as_queries:\n",
        "        q_ids = patch_ids.unsqueeze(-1).expand(bs, seq_len, num_patches)\n",
        "        kv_ids = (\n",
        "            torch.arange(num_patches, device=patch_ids.device)\n",
        "            .unsqueeze(0)\n",
        "            .unsqueeze(0)\n",
        "            .expand(bs, seq_len, num_patches)\n",
        "        )\n",
        "    else:\n",
        "        kv_ids = patch_ids.unsqueeze(1).expand(bs, num_patches, seq_len)\n",
        "        q_ids = (\n",
        "            torch.arange(num_patches, device=patch_ids.device)\n",
        "            .unsqueeze(0)\n",
        "            .unsqueeze(-1)\n",
        "            .expand(bs, num_patches, seq_len)\n",
        "        )\n",
        "    if window is None:\n",
        "        mask = q_ids == kv_ids\n",
        "    else:\n",
        "        mask = (kv_ids <= q_ids) & (q_ids < kv_ids + window)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def cross_attn_mask(\n",
        "    patch_ids,\n",
        "    patch_lengths,\n",
        "    N,\n",
        "    patches_as_queries=False,\n",
        "    cross_attn_k=1,\n",
        "    window=None,\n",
        "    block_mask=True,\n",
        "):\n",
        "    bs = patch_ids.shape[0]\n",
        "    with torch.no_grad():\n",
        "        # Create the patch mask\n",
        "        cross_mask = create_patch_mask_from_ids(\n",
        "            patch_ids,\n",
        "            patch_lengths.shape[1],\n",
        "            window=window,\n",
        "            patches_as_queries=patches_as_queries,\n",
        "        ).repeat_interleave(cross_attn_k, dim=1 if patches_as_queries else -1)\n",
        "        q_len = patch_lengths.shape[1] * cross_attn_k if patches_as_queries else N\n",
        "        kv_len = N if patches_as_queries else patch_lengths.shape[1] * cross_attn_k\n",
        "        assert cross_mask.shape == (\n",
        "            bs,\n",
        "            q_len,\n",
        "            kv_len,\n",
        "        ), f\"{cross_mask.shape} != {(bs, q_len, kv_len)}\"\n",
        "        if block_mask:\n",
        "\n",
        "            def patch_mask(b, h, q_idx, kv_idx):\n",
        "                return cross_mask[b, q_idx, kv_idx]\n",
        "\n",
        "            block_mask = create_block_mask(\n",
        "                patch_mask,\n",
        "                B=bs,\n",
        "                H=None,\n",
        "                Q_LEN=q_len,\n",
        "                KV_LEN=kv_len,\n",
        "                _compile=True,\n",
        "            )\n",
        "            return block_mask\n",
        "        else:\n",
        "            return torch.where(\n",
        "                cross_mask, torch.tensor(0.0), torch.tensor(float(\"-inf\"))\n",
        "            ).unsqueeze(\n",
        "                1\n",
        "            )  # [bs, 1, q_len, kv_len]\n",
        "\n",
        "\n",
        "def get_blt_input(\n",
        "    tokens: torch.Tensor,\n",
        "    enforce_patch_size_multiple: bool,\n",
        "    nb_boe: torch.Tensor,\n",
        "    patch_size: int,\n",
        "    boe_id: int,\n",
        "):\n",
        "    \"\"\"\n",
        "        This function returns X_et, X_gt and X_dt, the encoder, global, and decoder\n",
        "    tokens respectively.\n",
        "\n",
        "    Consider the input and target sequences:\n",
        "    X=[3,4,5,6,7,eos,bos,8,9,10,eos,bos,11,12,13]\n",
        "    Y=[4,5,6,7,eos,bos,8,9,10,eos,bos,11,12,13,14]\n",
        "    with patch_size=4\n",
        "\n",
        "    Note 1: that there will be no special tokens introduced at the patch level.\n",
        "    Note 2: X_e needs to be trimmed to be passed to Global\n",
        "\n",
        "    Current without boe:\n",
        "    X_et = [[boe,boe,boe,boe] [3,4,5,6],      [7,eos,bos,8],    [9,10,eos,bos] [11,12,13, pad]]\n",
        "    X_g =  [[boe,boe,boe,boe] [3,4,5,6],      [7,eos,bos,8],    [9,10,eos,bos] [11,12,13, pad]] # remove last glob patch\n",
        "    X_dt = [[3,4,5,6]         [7,eos,bos,8],  [9,10,eos,bos],   [11,12,13]]\n",
        "    Y =    [[4,5,6,7]         [eos,bos,8,9],  [10,eos,bos,11],  [12,13,14]]\n",
        "\n",
        "    --> lag fix:\n",
        "    X_et = [[boe,boe,boe,3]   [4,5,6,7],      [eos,bos,8,9],    [10,eos,bos,11] [12,13,pad,pad]]\n",
        "    X_g =  [[boe,boe,boe,3]   [4,5,6,7],      [eos,bos,8,9],    [10,eos,bos,11]]\n",
        "    X_dt = [[3,4,5,6]         [7,eos,bos,8],  [9,10,eos,bos],   [11,12,13]]\n",
        "    Y =    [[4,5,6,7]    \t  [eos,bos,8,9],  [10,eos,bos,11],  [12,13,14]]\n",
        "\n",
        "    Dynamic (current):\n",
        "    X = [3,4,5,6,7,eos,bos,8,9,10,eos,bos]\n",
        "    Y = [4,5,6,7,eos,bos,8,9,10,eos,bos,11]\n",
        "\n",
        "    entropy patching:\n",
        "    input: 7, bos, 9, 10\n",
        "    pred (high entropy): eos, 8, 10, eos\n",
        "\n",
        "    X_et = [[boe,3,4,5,6,7,eos,bos,8,9,10,eos,bos]\n",
        "    X_g =  [[boe],      [3,4,5,6], [7,eos],[bos,8],[9],     [10,eos]]\n",
        "    X_dt = [[3,4,5,6],  [7,eos],   [bos,8],[9],    [10,eos],[bos]]\n",
        "    Y =    [4,5,6,7,eos,bos,8,9,10,eos,bos,11]\n",
        "\n",
        "    --> lag fix no boe (force single byte first patch):\n",
        "    X_et = [[3,4,5,6,7,eos,bos,8,9,10,eos,bos,11,12]\n",
        "    X_g =  [[3],        [4,5,6,7], [eos,bos],[8,9], [10],       [eos,bos],      [11,12]] # remove last global patch\n",
        "    X_dt = [[3,4,5,6],  [7,eos],   [bos,8], [9],    [10,eos],   [bos,11,12]]\n",
        "    Y =    [4,5,6,7,    eos,bos,    8,9,    10,     eos,bos,    11,12,13]\n",
        "\n",
        "    input: 4, 7, bos, 9, 10\n",
        "    pred (high entropy): 5, eos, 8, 10, eos\n",
        "\n",
        "    X_et = [[3,4,5,6,7,eos,bos,8,9,10,eos,bos,11,12]\n",
        "    X_g =  [[3],        [4]   ,   [5,6,7], [eos,bos],[8,9], [10],       [eos,bos],      [11,12]] # remove last global patch\n",
        "    X_dt = [[3]         [4,5,6],  [7,eos],   [bos,8], [9],    [10,eos],   [bos,11,12]]\n",
        "    Y =    [4,]         [5,6,7,    eos,bos,    8,9,    10,     eos,bos,    11,12,13]\n",
        "\n",
        "    Handle the last byte properly.\n",
        "    patch_lengths = [1, 1,         3,      2,         2      1           2               2         1]\n",
        "    X_et = [[3,4,5,6,7,eos,bos,8,9,10,eos,bos,11,12]\n",
        "    X_g =  [[3],        [4]   ,   [5,6,7], [eos,bos],[8,9], [10],       [eos,bos],      [11,12]] # do not remove last global patch\n",
        "    X_dt = [[3]         [4,5,6],  [7,eos],   [bos,8], [9],    [10,eos],   [bos,11]       [12]]\n",
        "    Y =    [4,]         [5,6,7,    eos,bos,    8,9,    10,     eos,bos,    11,12,        13]]\n",
        "\n",
        "\n",
        "    bpe delim\n",
        "    X_et = [[3,4,5,6,7,<d>,eos,bos,<d>,8,9,<d>,10,<d>,eos,bos,11,12]\n",
        "    X_g =  [[3],          [4,5,6,7,<d>],     [eos,bos,<d>], ..\n",
        "    X_dt = [[3,4,5,6,7],  [<d>,eos,bos],     [<d>,bos,8], ..\n",
        "    Y =    [4,5,6,7,<d>,    eos,bos,<d>       8,9,<d>, ..\n",
        "\n",
        "\n",
        "    Note 1: that there will be no special tokens introduced at the patch level.\n",
        "    Note 2: X_e needs to be trimmed to be passed to Global\n",
        "    \"\"\"\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    local_encoder_tokens = tokens\n",
        "    local_decoder_tokens = tokens\n",
        "\n",
        "    if nb_boe > 0:\n",
        "        padded_patch = tokens.new(batch_size, nb_boe).fill_(boe_id)\n",
        "        local_encoder_tokens = torch.cat((padded_patch, local_encoder_tokens), dim=1)\n",
        "    # global_tokens = tokens.new(batch_size, ((seq_len-1) // patch_size)+1).fill_(boe_id)\n",
        "\n",
        "    # create global tokens, contains boe tokens and eos\n",
        "    # padded_local_encoder_tokens = fill_tokens(local_encoder_tokens, patch_size, boe_id)\n",
        "    # patches = padded_local_encoder_tokens.view(batch_size, -1, patch_size)\n",
        "    # global_tokens = (patches.eq(eos_id).any(dim=2).int() * eos_id)[:, 1:]\n",
        "    # global_tokens += global_tokens.eq(0).int() * boe_id\n",
        "    # TODO: fix this when we want to use block causal in the global.\n",
        "\n",
        "    if enforce_patch_size_multiple and local_encoder_tokens.shape[-1] % patch_size != 0:\n",
        "        local_encoder_tokens = fill_tokens(local_encoder_tokens, patch_size, boe_id)\n",
        "\n",
        "    return local_encoder_tokens, None, local_decoder_tokens\n",
        "\n",
        "\n",
        "def patch_ids_from_lengths(patch_lengths, seq_len):\n",
        "    bs, num_patches = patch_lengths.shape\n",
        "    # Create a tensor of cumulative sums of the patch lengths\n",
        "    cum_d = torch.cat(\n",
        "        [\n",
        "            torch.zeros(bs, 1, dtype=patch_lengths.dtype, device=patch_lengths.device),\n",
        "            patch_lengths.cumsum(dim=-1),\n",
        "        ],\n",
        "        dim=-1,\n",
        "    )\n",
        "    patch_ids = (cum_d.unsqueeze(-1) <= torch.arange(seq_len, device=cum_d.device)).sum(\n",
        "        dim=-2\n",
        "    ) - 1\n",
        "    assert not (\n",
        "        torch.max(patch_ids) > patch_lengths.shape[-1] or torch.min(patch_ids) < 0\n",
        "    ), f\"{torch.max(patch_ids)} > {patch_lengths.shape[-1]} or {torch.min(patch_ids)} < 0\"\n",
        "    return patch_ids\n",
        "\n",
        "\n",
        "class ByteLatentTransformerArgs(BaseTransformerArgs):\n",
        "    # Basic model configuration\n",
        "    seed: int = 42\n",
        "    vocab_size: int = -1\n",
        "    dim: int = 512\n",
        "    n_layers: int = 8\n",
        "    n_heads: int = 8\n",
        "    # TODO: What is the purpose of this parameter?\n",
        "    weight_tying: bool = False\n",
        "    patch_in_forward: bool = False\n",
        "\n",
        "    # Architecture and dimensions\n",
        "    dim_token: int | None = None\n",
        "    dim_global: int = 512\n",
        "    dim_local_decoder: int = 512\n",
        "    dim_local_encoder: int = 512\n",
        "    n_layers_global: int = 8\n",
        "    n_layers_local_decoder: int = 8\n",
        "    n_layers_local_encoder: int = 8\n",
        "\n",
        "    # Tokenization and patching\n",
        "    patch_size: float | None = None\n",
        "    patching_mode: str | None = None\n",
        "    patching_threshold: float | None = None\n",
        "    patching_threshold_add: float | None = None\n",
        "    monotonicity: bool = False\n",
        "    patching_batch_size: int = 1\n",
        "    patching_device: str = \"cuda\"\n",
        "    max_patch_length: int | None = None\n",
        "\n",
        "    # Encoder/Decoder configuration\n",
        "    tie_local_encoder_decoder_logits: bool = False\n",
        "    use_local_encoder_transformer: bool = False\n",
        "    encoder_lm_loss: bool = False\n",
        "    max_encoder_seq_length: int | None = None\n",
        "    pad_to_max_length: bool = False\n",
        "    encoder_enable_byte_ngrams: bool = False\n",
        "    encoder_enable_byte_group_hash: bool = False\n",
        "    ngram_vocab_sizes: int | None = None\n",
        "\n",
        "    # Cross attention configurations\n",
        "    cross_attn_encoder: bool = False\n",
        "    cross_attn_decoder: bool = False\n",
        "    cross_attn_window_encoder: int | None = None\n",
        "    cross_attn_window_decoder: int | None = None\n",
        "    cross_attn_k: int | None = None\n",
        "    cross_attn_nheads: int | None = None\n",
        "    cross_attn_all_layers_decoder: bool = False\n",
        "    cross_attn_all_layers_encoder: bool = False\n",
        "    cross_attn_use_flex_attention: bool = True\n",
        "    cross_attn_init_by_pooling: bool = False\n",
        "\n",
        "    # Encoder hash configurations\n",
        "    encoder_hash_byte_group_size: Any | None = None\n",
        "    encoder_hash_byte_group_vocab: int = 30000\n",
        "    encoder_hash_byte_group_nb_functions: int = 3\n",
        "\n",
        "    # Model behavior and optimization\n",
        "    log_patch_lengths: bool = False\n",
        "    non_linearity: str = \"swiglu\"\n",
        "    use_rope: bool = True\n",
        "    recompute_fc1_out: bool = False\n",
        "    recompute_fc3_out: bool = False\n",
        "    recompute_attn: bool = True\n",
        "    custom_bwd: bool = False\n",
        "    layer_ckpt: str = \"all\"\n",
        "\n",
        "    # Initialization and attention\n",
        "    init_use_gaussian: bool = True\n",
        "    init_use_depth: str = \"current\"\n",
        "    attn_bias_type: str = \"causal\"\n",
        "    alpha_depth: str = \"disabled\"\n",
        "    max_length: int = 2048\n",
        "\n",
        "    # Norm configuration\n",
        "    norm_eps: float = 1e-5\n",
        "    norm_affine: bool = True\n",
        "    pre_norm: bool = True\n",
        "    norm_type: str = \"rmsnorm\"\n",
        "\n",
        "    # Additional configurations\n",
        "    multiple_of: int = 256\n",
        "    ffn_dim_multiplier: float = 1.0\n",
        "    dropout: float = 0\n",
        "    output_size: int = -1\n",
        "\n",
        "    # Additional parameters from ModelArgs\n",
        "    architecture: str = \"vanilla\"\n",
        "    share_encoder_decoder_emb: bool = True\n",
        "    global_local_decoder_residual_layer: str | None = None\n",
        "\n",
        "    tokenize_with_bpe_delimiter: bool = False\n",
        "    patching_thresholds_str: str | None = None\n",
        "    tie_local_encoder_decoder: bool = False\n",
        "    encoder_preds_low_entropy_toks: float | None = None\n",
        "    encoder_preds_random_toks: float | None = None\n",
        "    dim_token_emb: int | None = None\n",
        "    dim_patch_emb: int | None = None\n",
        "\n",
        "    encoder_ngram_table_dir: str | None = None\n",
        "    encoder_ngram_to_size_str: str | None = None\n",
        "\n",
        "    # Model architecture params\n",
        "    entropy_model_checkpoint_dir: str | None = None\n",
        "    entropy_model_is_ngram_model: bool = False\n",
        "    downsampling_by_pooling: str | None = None\n",
        "    n_heads_global: int = 8\n",
        "    n_heads_local_decoder: int = 8\n",
        "    n_heads_local_encoder: int = 8\n",
        "    n_kv_heads: int | None = None\n",
        "    n_kv_heads_global: int | None = None\n",
        "    conv_kernel_size: int | None = None\n",
        "    local_attention_window_len: int | None = None\n",
        "\n",
        "    # Performance optimization\n",
        "    sequence_parallel: bool = False\n",
        "    loss_parallel: bool = False\n",
        "    fuse_sequence_parallel: bool = False\n",
        "    use_fsdp: bool = True\n",
        "    attn_to_keep: str = \"all\"\n",
        "\n",
        "    # Parameter mixing\n",
        "    pm_size: int = 0\n",
        "\n",
        "    # Logging\n",
        "    full_logging_n_layers: int = 4\n",
        "\n",
        "    @model_validator(mode=\"after\")\n",
        "    def check_hash_byte_sizes(self) -> Self:\n",
        "        if (\n",
        "            self.encoder_hash_byte_group_size is not None\n",
        "            and type(self.encoder_hash_byte_group_size) == str\n",
        "        ):\n",
        "            self.encoder_hash_byte_group_size = [\n",
        "                int(x)\n",
        "                for x in self.encoder_hash_byte_group_size.split(\",\")\n",
        "                if len(x) > 0\n",
        "            ]\n",
        "        return self\n",
        "\n",
        "\n",
        "class GlobalTransformerArgs(ByteLatentTransformerArgs):\n",
        "    # Global encoder specific dimensions\n",
        "    dim_token_emb: int | None = None\n",
        "    dim_patch_emb: int | None = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Override base args with global encoder specific values\n",
        "        self.dim = self.dim_global\n",
        "        self.n_layers = self.n_layers_global\n",
        "        self.n_heads = self.n_heads_global\n",
        "        self.n_kv_heads = self.n_kv_heads_global\n",
        "        self.local_attention_window_len = None\n",
        "        self.cross_attn_encoder = False\n",
        "        self.cross_attn_decoder = False\n",
        "\n",
        "\n",
        "class LocalDecoderArgs(ByteLatentTransformerArgs):\n",
        "    # Local decoder specific dimensions\n",
        "    dim_token_emb: int | None = None\n",
        "    dim_patch_emb: int | None = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Override base args with local decoder specific values\n",
        "        self.dim = self.dim_local_decoder\n",
        "        self.n_layers = self.n_layers_local_decoder\n",
        "        self.n_heads = self.n_heads_local_decoder\n",
        "        self.cross_attn_encoder = False\n",
        "        self.cross_attn_init_by_pooling = False\n",
        "        self.attn_bias_type = \"local_block_causal\"\n",
        "\n",
        "\n",
        "def create_global_transformer(args: ByteLatentTransformerArgs) -> GlobalTransformer:\n",
        "    global_args = args.model_copy(\n",
        "        deep=True,\n",
        "        update=dict(\n",
        "            dim=args.dim_global,\n",
        "            n_layers=args.n_layers_global,\n",
        "            n_heads=args.n_heads_global,\n",
        "            n_kv_heads=args.n_kv_heads_global,\n",
        "            local_attention_window_len=None,\n",
        "            dim_token_emb=get_global_dim_patch_emb(args),\n",
        "            dim_patch_emb=None,\n",
        "            cross_attn_encoder=False,\n",
        "            cross_attn_decoder=False,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return GlobalTransformer(global_args)\n",
        "\n",
        "\n",
        "def create_local_encoder(args: ByteLatentTransformerArgs) -> LocalEncoder:\n",
        "    local_encoder_args = LocalModelArgs(\n",
        "        # Updated args\n",
        "        dim=args.dim_local_encoder,\n",
        "        n_layers=args.n_layers_local_encoder,\n",
        "        n_heads=args.n_heads_local_encoder,\n",
        "        dim_token_emb=get_encoder_dim_token_emb(args),\n",
        "        dim_patch_emb=get_encoder_dim_patch_emb(args),\n",
        "        cross_attn_encoder=args.cross_attn_encoder,\n",
        "        cross_attn_decoder=False,\n",
        "        cross_attn_k=args.cross_attn_k if args.cross_attn_encoder else None,\n",
        "        cross_attn_init_by_pooling=args.cross_attn_init_by_pooling,\n",
        "        # Defaults\n",
        "        head_dim=args.head_dim,\n",
        "        max_seqlen=args.max_encoder_seq_length,\n",
        "        dropout=args.dropout,\n",
        "        vocab_size=args.vocab_size + args.pm_size,\n",
        "        norm_eps=args.norm_eps,\n",
        "        patch_size=args.patch_size,\n",
        "        sliding_window=args.local_attention_window_len,\n",
        "        use_rope=args.use_rope,\n",
        "        rope_theta=args.rope_theta,\n",
        "        rope_use_fp32_in_outer_product=args.rope_use_fp32_in_outer_product,\n",
        "        init_base_std=args.init_base_std,\n",
        "        init_std_factor=args.init_std_factor,\n",
        "        n_kv_heads=args.n_kv_heads,\n",
        "        attn_impl=args.attn_impl,\n",
        "        attn_bias_type=\"local_block_causal\",\n",
        "        multiple_of=args.multiple_of,\n",
        "        ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
        "        patching_mode=args.patching_mode,\n",
        "        use_local_encoder_transformer=args.use_local_encoder_transformer,\n",
        "        downsampling_by_pooling=args.downsampling_by_pooling,\n",
        "        encoder_hash_byte_group_size=args.encoder_hash_byte_group_size,\n",
        "        cross_attn_all_layers_encoder=args.cross_attn_all_layers_encoder,\n",
        "        cross_attn_all_layers_decoder=args.cross_attn_all_layers_decoder,\n",
        "        cross_attn_nheads=args.cross_attn_nheads,\n",
        "        eos_id=args.eos_id,\n",
        "    )\n",
        "\n",
        "    return LocalEncoder(local_encoder_args)\n",
        "\n",
        "\n",
        "def create_local_decoder(args: ByteLatentTransformerArgs) -> LocalDecoder:\n",
        "    # First deep copy the original args\n",
        "    local_decoder_args = LocalModelArgs(\n",
        "        dim=args.dim_local_decoder,\n",
        "        n_layers=args.n_layers_local_decoder,\n",
        "        n_heads=args.n_heads_local_decoder,\n",
        "        dim_token_emb=get_decoder_dim_token_emb(args),\n",
        "        dim_patch_emb=args.dim_global,\n",
        "        cross_attn_encoder=False,\n",
        "        cross_attn_decoder=args.cross_attn_decoder,\n",
        "        cross_attn_init_by_pooling=False,  # states are already defined\n",
        "        cross_attn_k=args.cross_attn_k if args.cross_attn_decoder else None,\n",
        "        # Defaults\n",
        "        head_dim=args.head_dim,\n",
        "        max_seqlen=args.max_encoder_seq_length,\n",
        "        dropout=args.dropout,\n",
        "        vocab_size=args.vocab_size + args.pm_size,\n",
        "        norm_eps=args.norm_eps,\n",
        "        patch_size=args.patch_size,\n",
        "        sliding_window=args.local_attention_window_len,\n",
        "        use_rope=args.use_rope,\n",
        "        rope_theta=args.rope_theta,\n",
        "        rope_use_fp32_in_outer_product=args.rope_use_fp32_in_outer_product,\n",
        "        init_base_std=args.init_base_std,\n",
        "        init_std_factor=args.init_std_factor,\n",
        "        n_kv_heads=args.n_kv_heads,\n",
        "        attn_impl=args.attn_impl,\n",
        "        attn_bias_type=\"local_block_causal\",\n",
        "        multiple_of=args.multiple_of,\n",
        "        ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
        "        patching_mode=args.patching_mode,\n",
        "        use_local_encoder_transformer=args.use_local_encoder_transformer,\n",
        "        downsampling_by_pooling=args.downsampling_by_pooling,\n",
        "        encoder_hash_byte_group_size=args.encoder_hash_byte_group_size,\n",
        "        cross_attn_all_layers_encoder=args.cross_attn_all_layers_encoder,\n",
        "        cross_attn_all_layers_decoder=args.cross_attn_all_layers_decoder,\n",
        "        cross_attn_nheads=args.cross_attn_nheads,\n",
        "        eos_id=args.eos_id,\n",
        "    )\n",
        "\n",
        "    return LocalDecoder(local_decoder_args)\n",
        "\n",
        "\n",
        "class EmbeddingType(Enum):\n",
        "    HASH_TOK = auto()\n",
        "    NGRAM = auto()\n",
        "\n",
        "\n",
        "def init_embeddings(\n",
        "    args,\n",
        "    embedding_type: EmbeddingType,\n",
        "    local_encoder_dim: int,\n",
        "    encoder_hash_byte_group_size: list = None,\n",
        "):\n",
        "    if (\n",
        "        embedding_type == EmbeddingType.HASH_TOK\n",
        "        and args.encoder_hash_byte_group_size is None\n",
        "    ):\n",
        "        return None\n",
        "    if embedding_type == EmbeddingType.NGRAM and args.encoder_ngram_to_size_str is None:\n",
        "        return None\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    if embedding_type == EmbeddingType.HASH_TOK:\n",
        "        emb_dim = local_encoder_dim\n",
        "        encoder_hash_byte_group_vocab = args.encoder_hash_byte_group_vocab\n",
        "        for _ in range(args.encoder_hash_byte_group_nb_functions):\n",
        "            for _ in encoder_hash_byte_group_size:\n",
        "                embeddings.append(\n",
        "                    nn.Embedding(encoder_hash_byte_group_vocab, emb_dim))\n",
        "\n",
        "    elif embedding_type == EmbeddingType.NGRAM:\n",
        "        encoder_ngram_to_size = parse_ngram_to_size(args.encoder_ngram_to_size_str)\n",
        "        emb_dim = local_encoder_dim\n",
        "        OFFSET = 4  # This should be passed as parameter if it's variable\n",
        "        for ngram_vocab_size in encoder_ngram_to_size.values():\n",
        "            embeddings.append(nn.Embedding(ngram_vocab_size + OFFSET, emb_dim))\n",
        "\n",
        "    return nn.ModuleList(embeddings)\n",
        "\n",
        "\n",
        "def compute_hash_embeddings(\n",
        "    local_encoder_tokens: torch.Tensor,\n",
        "    local_encoder,\n",
        "    encoder_hash_tok_embedding: nn.ModuleList,\n",
        "    encoder_hash_byte_group_nb_functions: int,\n",
        "    encoder_hash_byte_group_size: list,\n",
        "    encoder_hash_byte_group_vocab: int,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute embeddings using hash token embeddings.\n",
        "\n",
        "    Args:\n",
        "        local_encoder_tokens: Input tokens tensor\n",
        "        local_encoder: Encoder object with tok_embeddings method\n",
        "        encoder_hash_tok_embedding: ModuleList of hash token embeddings\n",
        "        encoder_hash_byte_group_nb_functions: Number of hash functions\n",
        "        encoder_hash_byte_group_size: List of byte group sizes\n",
        "        encoder_hash_byte_group_vocab: Vocabulary size for hash embeddings\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Combined embeddings\n",
        "    \"\"\"\n",
        "    if encoder_hash_tok_embedding is None:\n",
        "        return None\n",
        "\n",
        "    local_encoder_embeds = local_encoder.tok_embeddings(local_encoder_tokens)\n",
        "\n",
        "    i = 0\n",
        "    for func_nb in range(encoder_hash_byte_group_nb_functions):\n",
        "        for byte_group_size in encoder_hash_byte_group_size:\n",
        "            hash_ids = byte_group_hash_function(\n",
        "                local_encoder_tokens,\n",
        "                byte_group_size,\n",
        "                hash_func_nb=func_nb,\n",
        "                max_hash=encoder_hash_byte_group_vocab,\n",
        "            )\n",
        "            hash_tok_embedding = encoder_hash_tok_embedding[i]\n",
        "            local_encoder_embeds = local_encoder_embeds + hash_tok_embedding(hash_ids)\n",
        "            i += 1\n",
        "\n",
        "    assert i == len(encoder_hash_tok_embedding)\n",
        "    return local_encoder_embeds\n",
        "\n",
        "\n",
        "class ByteLatentTransformer(nn.Module, SequenceModelWithOutput):\n",
        "    \"\"\"\n",
        "    The ByteLatentTransformer (BLT) is a byte-level language model architecture that processes byte sequences\n",
        "    by dynamically segmenting them into patches. It uses a combination of local encoders, global transformers,\n",
        "    and local decoders to efficiently encode and decode byte sequences, leveraging patch-based processing for\n",
        "    improved performance and inference efficiency.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args: ByteLatentTransformerArgs):\n",
        "        super().__init__()\n",
        "\n",
        "        # General configuration\n",
        "        self.weight_tying = args.weight_tying\n",
        "        self.patch_size = args.patch_size\n",
        "        self.patching_mode = args.patching_mode\n",
        "        self.boe_id, self.bos_id, self.pad_id, self.eos_id = (\n",
        "            BOE_ID,\n",
        "            BOS_ID,\n",
        "            PAD_ID,\n",
        "            EOS_ID,\n",
        "        )\n",
        "        self.downsampling_by_pooling = args.downsampling_by_pooling\n",
        "        self.patching_threshold = args.patching_threshold\n",
        "        self.dim = args.dim\n",
        "        self.init_base_std = args.init_base_std\n",
        "        self.init_std_factor = InitStdFactor(args.init_std_factor)\n",
        "        self.max_seqlen = args.max_seqlen\n",
        "\n",
        "        # Cross attention configuration\n",
        "        self.cross_attn_encoder = args.cross_attn_encoder\n",
        "        self.cross_attn_decoder = args.cross_attn_decoder\n",
        "        self.cross_attn_k = args.cross_attn_k\n",
        "        self.cross_attn_window_encoder = args.cross_attn_window_encoder\n",
        "        self.cross_attn_window_decoder = args.cross_attn_window_decoder\n",
        "        self.cross_attn_use_flex_attention = args.cross_attn_use_flex_attention\n",
        "\n",
        "        # Encoder hash configuration\n",
        "        self.encoder_hash_byte_group_size = args.encoder_hash_byte_group_size\n",
        "        self.encoder_hash_byte_group_vocab = args.encoder_hash_byte_group_vocab\n",
        "        self.encoder_hash_byte_group_nb_functions = (\n",
        "            args.encoder_hash_byte_group_nb_functions\n",
        "        )\n",
        "\n",
        "        # ByteLatent modules\n",
        "        self.local_encoder = create_local_encoder(args)\n",
        "        self.global_transformer = create_global_transformer(args)\n",
        "        self.local_decoder = create_local_decoder(args)\n",
        "        self.encoder_hash_tok_embedding = init_embeddings(\n",
        "            args,\n",
        "            EmbeddingType.HASH_TOK,\n",
        "            local_encoder_dim=self.local_encoder.dim,\n",
        "            encoder_hash_byte_group_size=self.encoder_hash_byte_group_size,\n",
        "        )\n",
        "        self.encoder_ngram_embedding = init_embeddings(\n",
        "            args,\n",
        "            EmbeddingType.NGRAM,\n",
        "            local_encoder_dim=self.local_encoder.dim,\n",
        "            encoder_hash_byte_group_size=None,\n",
        "        )\n",
        "\n",
        "        # Encoder ngram embedding tables\n",
        "        self.encoder_ngram_embedding = None\n",
        "        if args.encoder_enable_byte_ngrams:\n",
        "            self.encoder_ngram_embedding = nn.ModuleList()\n",
        "            assert args.ngram_vocab_sizes is not None\n",
        "            self.encoder_ngram_to_size = parse_ngram_to_size(\n",
        "                args.encoder_ngram_to_size_str\n",
        "            )\n",
        "            ngram_emb_dim = self.local_encoder.dim\n",
        "            for ngram_vocab_size in self.encoder_ngram_to_size.values():\n",
        "                self.encoder_ngram_embedding.append(\n",
        "                    nn.Embedding(ngram_vocab_size + OFFSET, ngram_emb_dim)\n",
        "                )\n",
        "\n",
        "        # Output layer\n",
        "        assert args.vocab_size > 0, \"vocab_size must be greater than 0\"\n",
        "\n",
        "        # Patcher module\n",
        "        if args.patch_in_forward:\n",
        "            self.patcher = Patcher(\n",
        "                PatcherArgs(\n",
        "                    patch_size=args.patch_size,\n",
        "                    patching_mode=args.patching_mode,\n",
        "                    patching_threshold=args.patching_threshold,\n",
        "                    patching_threshold_add=args.patching_threshold_add,\n",
        "                    monotonicity=args.monotonicity,\n",
        "                    max_patch_length=args.max_patch_length,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def get_output_seq_len(self):\n",
        "        return self.max_seqlen\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tokens: torch.Tensor,\n",
        "        patch_lengths: Optional[torch.Tensor] = None,\n",
        "        ngram_ids: Optional[torch.Tensor] = None,\n",
        "    ):\n",
        "        # Ensure ngram_ids is either a tensor or None\n",
        "        assert (\n",
        "            isinstance(ngram_ids, torch.Tensor) or ngram_ids is None\n",
        "        ), f\"ngram_ids must be a tensor or None, but was: {type(ngram_ids)}\"\n",
        "\n",
        "        bs, N = tokens.shape  # Batch size and sequence length\n",
        "\n",
        "        # Get megabyte inputs\n",
        "        nb_boe = int(0 if self.patching_mode != \"\" else self.patch_size - 1)\n",
        "        local_encoder_tokens, _, local_decoder_tokens = get_blt_input(\n",
        "            tokens=tokens,\n",
        "            enforce_patch_size_multiple=False,\n",
        "            nb_boe=nb_boe,\n",
        "            patch_size=self.patch_size,\n",
        "            boe_id=self.boe_id,\n",
        "        )\n",
        "\n",
        "        # Patching\n",
        "        if patch_lengths is None:\n",
        "            assert (\n",
        "                getattr(self, \"patcher\", None) is not None\n",
        "            ), \"Patcher not defined and no patch_lengths passed.\"\n",
        "            patch_lengths, tok_scores = self.patcher.patch(\n",
        "                local_encoder_tokens,\n",
        "                include_next_token=True,\n",
        "                threshold=self.patcher.threshold,\n",
        "            )\n",
        "        else:\n",
        "            if nb_boe > 0:\n",
        "                patch_lengths[:, 0] += nb_boe\n",
        "\n",
        "        assert torch.min(patch_lengths) >= 0\n",
        "\n",
        "        # Generate patch IDs from patch_lengths\n",
        "        patch_ids = patch_ids_from_lengths(\n",
        "            patch_lengths, local_encoder_tokens.shape[-1]\n",
        "        )\n",
        "        assert torch.max(patch_ids) + 1 <= torch.max(\n",
        "            (patch_lengths != 0).sum(dim=-1)\n",
        "        ), f\"{torch.max(patch_ids) + 1} > {torch.max((patch_lengths != 0).sum(dim=-1))}\"\n",
        "\n",
        "        cross_attn_mask_enc = None\n",
        "        # Cross-attention encoder\n",
        "        if self.cross_attn_encoder:\n",
        "            cross_attn_mask_enc = cross_attn_mask(\n",
        "                patch_ids,\n",
        "                patch_lengths,\n",
        "                N,\n",
        "                patches_as_queries=True,\n",
        "                cross_attn_k=self.cross_attn_k,\n",
        "                window=self.cross_attn_window_encoder,\n",
        "                block_mask=self.cross_attn_use_flex_attention,\n",
        "            )\n",
        "\n",
        "        # Hashing and embedding\n",
        "        local_encoder_embeds = compute_hash_embeddings(\n",
        "            local_encoder_tokens=local_encoder_tokens,\n",
        "            local_encoder=self.local_encoder,\n",
        "            encoder_hash_tok_embedding=self.encoder_hash_tok_embedding,\n",
        "            encoder_hash_byte_group_nb_functions=self.encoder_hash_byte_group_nb_functions,\n",
        "            encoder_hash_byte_group_size=self.encoder_hash_byte_group_size,\n",
        "            encoder_hash_byte_group_vocab=self.encoder_hash_byte_group_vocab,\n",
        "        )\n",
        "\n",
        "        # N-gram table embeddings\n",
        "        if self.encoder_ngram_embedding is not None:\n",
        "            assert ngram_ids is not None, \"ngram_ids must be provided\"\n",
        "            if local_encoder_embeds is None:\n",
        "                local_encoder_embeds = self.local_encoder.tok_embeddings(\n",
        "                    local_encoder_tokens\n",
        "                )\n",
        "            assert len(ngram_ids) == len(\n",
        "                self.encoder_ngram_embedding\n",
        "            ), f\"ngram_ids.shape[0]={ngram_ids.shape[0]} versus len(encoder_ngram_embedding)={len(self.encoder_ngram_embedding)}, ngram_ids.shape={ngram_ids.shape}\"\n",
        "            for i in range(ngram_ids.shape[0]):\n",
        "                ngram_embedding = self.encoder_ngram_embedding[i]\n",
        "                ngram_embeds = ngram_embedding(ngram_ids[i])\n",
        "                assert (\n",
        "                    local_encoder_embeds.shape == ngram_embeds.shape\n",
        "                ), f\"Shape mismatch: {local_encoder_embeds.shape} vs {ngram_embeds.shape}, ngram_ids.shape={ngram_ids.shape}\"\n",
        "                local_encoder_embeds = local_encoder_embeds + ngram_embeds\n",
        "\n",
        "        # Local encoder\n",
        "        (h_encoder, h_cross), cache_encoder = self.local_encoder(\n",
        "            tokens=local_encoder_tokens,\n",
        "            embeds=local_encoder_embeds,\n",
        "            patch_embeds=None,\n",
        "            cross_mask=cross_attn_mask_enc,\n",
        "            num_patches=patch_lengths.shape[1],\n",
        "            patch_ids=patch_ids,\n",
        "        )\n",
        "\n",
        "        # Downsampling\n",
        "        if not self.cross_attn_encoder:\n",
        "            assert (\n",
        "                patch_ids.shape[1] == h_encoder.shape[1]\n",
        "            ), f\"{patch_ids.shape[1]} != {h_encoder.shape[1]}\"\n",
        "            h = downsample(\n",
        "                h_encoder,\n",
        "                patch_lengths.shape[1],\n",
        "                patch_lengths,\n",
        "                patch_ids,\n",
        "                downsampling_by_pooling=self.downsampling_by_pooling,\n",
        "                patch_size=self.patch_size,\n",
        "            )\n",
        "        else:\n",
        "            # Reshape h_cross\n",
        "            h = h_cross.view(bs, patch_lengths.shape[1], -1)\n",
        "\n",
        "        # Global transformer\n",
        "        global_tokens = tokens.new(h.shape[0], h.shape[1]).fill_(self.boe_id)\n",
        "        rows, cols = torch.where(local_encoder_tokens == self.eos_id)\n",
        "        eos_patch_ids = patch_ids[rows, cols]\n",
        "        global_tokens[rows, eos_patch_ids] = self.eos_id\n",
        "\n",
        "        h, _ = self.global_transformer(\n",
        "            embeds=h,\n",
        "            tokens=global_tokens,\n",
        "        )\n",
        "\n",
        "        # Unpatching\n",
        "        dec_embeds = h_encoder[:, nb_boe : nb_boe + N, :]\n",
        "\n",
        "        # Generate decoder patch IDs\n",
        "        decoder_patch_ids = decoder_patch_ids_from_lengths(\n",
        "            patch_lengths, nb_boe, local_decoder_tokens.shape[-1]\n",
        "        )\n",
        "        assert (\n",
        "            torch.max(decoder_patch_ids) + 1 <= h.shape[1]\n",
        "        ), f\"{torch.max(decoder_patch_ids) + 1} > {h.shape[1]}\"\n",
        "        assert (\n",
        "            decoder_patch_ids.shape[1] == dec_embeds.shape[1]\n",
        "        ), f\"{decoder_patch_ids.shape[1]} != {dec_embeds.shape[1]}\"\n",
        "\n",
        "        # Cross-attention decoder\n",
        "        if not self.cross_attn_decoder:\n",
        "            h = torch.gather(\n",
        "                h, 1, decoder_patch_ids.unsqueeze(-1).expand(-1, -1, h.shape[-1])\n",
        "            )\n",
        "            cross_attn_mask_dec = None\n",
        "            assert local_decoder_tokens.shape == h.shape[:-1]\n",
        "        else:\n",
        "            cross_attn_mask_dec = cross_attn_mask(\n",
        "                decoder_patch_ids,\n",
        "                patch_lengths,\n",
        "                N,\n",
        "                patches_as_queries=False,\n",
        "                cross_attn_k=self.cross_attn_k,\n",
        "                window=self.cross_attn_window_decoder,\n",
        "                block_mask=self.cross_attn_use_flex_attention,\n",
        "            )\n",
        "\n",
        "        # Local decoder\n",
        "        output, _ = self.local_decoder(\n",
        "            embeds=dec_embeds,\n",
        "            patch_embeds=h,\n",
        "            tokens=local_decoder_tokens,\n",
        "            cross_mask=cross_attn_mask_dec,\n",
        "        )\n",
        "        return output\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.local_encoder.init_weights()\n",
        "        self.global_transformer.init_weights()\n",
        "        self.local_decoder.init_weights()\n",
        "\n",
        "        emb_std = self.local_encoder.dim ** (-0.5)\n",
        "        for emb in self.encoder_hash_tok_embedding:\n",
        "            nn.init.trunc_normal_(\n",
        "                emb.weight,\n",
        "                mean=0.0,\n",
        "                std=emb_std,\n",
        "                a=-3 * emb_std,\n",
        "                b=3 * emb_std,\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## trash"
      ],
      "metadata": {
        "id": "y60_Smu8VzxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqC5uz00bapX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title big char dataloader fail\n",
        "\n",
        "\n",
        "\n",
        "# !pip install -qU datasets # restart?\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# dataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\", streaming=True, cache_dir=\"/content/hf\")\n",
        "dataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\", cache_dir=\"/content/hf\")\n",
        "data = dataset['text']\n",
        "chars = sorted(list(set(data)))\n",
        "print(chars)\n",
        "\n",
        "\n",
        "# chr()\n",
        "# ord()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = dataset['text']\n",
        "data = ''.join(data)\n",
        "chars = sorted(list(set(data)))\n",
        "print(chars[:200])\n",
        "\n",
        "print(len(data))\n",
        "# print(ord('\\n'))\n",
        "print([ord(x) for x in chars])\n",
        "print([chr(i) for i in [10, *list(range(32,127))]])\n",
        "# 10, 32-\n",
        "\n",
        "\n",
        "# @title hf stream dataset me\n",
        "!pip install -qU datasets # restart?\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import tiktoken # https://github.com/openai/tiktoken/tree/main\n",
        "\n",
        "class StreamDataset(Dataset):\n",
        "    def __init__(self, dataset, seq_len, buffer_size):\n",
        "        self.enc = tiktoken.get_encoding(\"gpt2\") # https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "        self.vocab_size = self.enc.n_vocab # gpt2:50257\n",
        "\n",
        "\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.data = iter(dataset)\n",
        "        self.seq_len = seq_len\n",
        "        self.buffer_size = buffer_size  # must be ≥ seq_len\n",
        "        self.buffer = []  # token buffer\n",
        "        self.fill_buffer()\n",
        "\n",
        "    def fill_buffer(self):\n",
        "        while len(self.buffer) < self.buffer_size:\n",
        "            x = next(self.data)\n",
        "            tokens = self.enc.encode(x[\"text\"]) # tiktoken\n",
        "            self.buffer.extend(tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        # /4.5/(4/3)\n",
        "        return 128000000\n",
        "        # return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print('get', idx)\n",
        "        if idx == 0: self.data = iter(self.dataset)\n",
        "        if len(self.buffer) < self.seq_len: self.fill_buffer()\n",
        "        if len(self.buffer) < self.seq_len:\n",
        "            raise StopIteration\n",
        "        x = self.buffer[:self.seq_len]\n",
        "        self.buffer = self.buffer[self.seq_len:]\n",
        "        return torch.tensor(x)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # print(batch)\n",
        "    return torch.stack(batch)\n",
        "\n",
        "# dataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\", streaming=True, cache_dir=\"/content/hf\")\n",
        "# dataset = load_dataset(\"maxtli/OpenWebText-2M\", split=\"train\", streaming=True, cache_dir=\"/content/hf\") # 8.7,3.8\n",
        "dataset = load_dataset(\"Skylion007/openwebtext\", trust_remote_code=True, split=\"train\", streaming=True, cache_dir=\"/content/hf\") # 8.7,3.8\n",
        "\n",
        "seq_len = 128\n",
        "buffer_size = seq_len*1\n",
        "train_data = StreamDataset(dataset, seq_len, buffer_size) # train_data = StreamDataset(dataset[\"train\"], seq_len, buffer_size)\n",
        "del dataset\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=2)\n",
        "del train_data\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, raw_data, seq_len):\n",
        "        data = ''.join(raw_data)\n",
        "        chars = sorted(list(set(data)))\n",
        "        self.vocab_size = len(chars) # 283\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.data = self.data_process(data) # list of int\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def data_process(self, data): # str 10780437\n",
        "        return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data) - self.seq_len\n",
        "        return len(self.data)//(self.seq_len+1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # dix = self.data[idx:idx + self.seq_len + 1]\n",
        "        dix = self.data[idx*(self.seq_len+1) : (idx+1)*(self.seq_len+1)]\n",
        "        x, y = dix[:-1], dix[1:]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "seq_len = 100 # 128\n",
        "train_data = CharDataset(text, seq_len) # one line of poem is roughly 50 characters\n",
        "test_data = CharDataset(test_text, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "test_loader = DataLoader(test_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "\n",
        "def encode(context): return torch.tensor([train_data.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "def decode(x): return ''.join([train_data.itos[int(i)] for i in x])\n",
        "# for x,y in train_loader:\n",
        "#     break\n",
        "# print(train_data.vocab_size)\n"
      ]
    }
  ]
}