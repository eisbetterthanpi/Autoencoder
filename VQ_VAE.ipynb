{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/Autoencoder/blob/main/VQ_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghQ8RSExs_A4",
        "outputId": "d8c281a2-baac-4648-c31e-b05caec904fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY\n",
            "From (redirected): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY&confirm=t&uuid=639d71a4-0580-4906-8b65-bddd0b5ab4ba\n",
            "To: /content/buffer512.pkl\n",
            "100% 706M/706M [00:09<00:00, 78.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title gdown\n",
        "import pickle\n",
        "!gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9UKkkuorG_b9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        # self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        state, action, reward = self.data[idx]\n",
        "        state = self.transform(state)\n",
        "        return state\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 # 128 512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mje-yFj88WlY",
        "outputId": "efe259a6-9702-46e9-9039-e1e6acfa9d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
            "        [-0.5000, -0.3333, -1.0000, -1.0000],\n",
            "        [-0.5000, -0.3333,  0.0000, -1.0000],\n",
            "        [ 0.0000, -0.3333,  0.0000, -1.0000],\n",
            "        [ 0.5000,  0.3333,  0.0000,  1.0000],\n",
            "        [ 0.5000,  0.3333,  1.0000,  1.0000],\n",
            "        [ 1.0000,  1.0000,  1.0000,  1.0000]], device='cuda:0')\n",
            "tensor([  0.,  30.,  32.,  56.,  87.,  89., 119.], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "fsq = FSQ(levels = [5,4,3,2])\n",
        "# print(fsq.codebook)\n",
        "batch_size, seq_len = 2, 4\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "x = torch.linspace(-2,2,7, device=device).repeat(4,1).T\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "lact = fsq.codes_to_indexes(la)\n",
        "print(lact)\n",
        "# la = fsq.indexes_to_codes(lact)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxGF8bYeGDA2",
        "outputId": "ef6c54b8-4c8c-420f-d7bb-3a2340da94a9",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 16, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title UIB\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class UIB(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, mult=4):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential( # norm,act,conv\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, in_ch, kernel, 1, kernel//2, groups=in_ch, bias=False),\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, mult*in_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, nn.Conv2d(mult*in_ch, mult*in_ch, kernel, 1, kernel//2, groups=mult*in_ch, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, zero_module(nn.Conv2d(mult*in_ch, out_ch, 1, bias=False)),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UIB(in_ch, out_ch)\n",
        "x = torch.rand(128, in_ch, 64, 64)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "2nu4Dzma_cD5"
      },
      "outputs": [],
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.res_conv = zero_module(nn.Conv2d(in_ch, out_ch, 1)) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        return self.block(x) + self.res_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjvZZswH1_KR",
        "outputId": "fb2db6f7-788e-45d6-caf5-ba082fe6a4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 16, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # if self.r>1: self.net = nn.Sequential(UIB(in_ch, out_ch*r**2, kernel), nn.PixelShuffle(r))\n",
        "        if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), UIB(in_ch*r**2, out_ch, kernel))\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Interpolate(nn.Module):\n",
        "    def __init__(self, scale_factor=2, mode=\"nearest-exact\", **kwargs):\n",
        "        super().__init__()\n",
        "        self.kwargs = kwargs\n",
        "    def forward(self, x):\n",
        "        # return F.interpolate(x, scale_factor=2, mode=\"nearest-exact\", **self.kwargs)\n",
        "        return F.adaptive_avg_pool2d(x, (x.shape[2]*2, x.shape[3]*2))\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        self.r = r\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # self.block = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # )\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(Interpolate(), nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.MaxPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.AvgPool2d(2,2))\n",
        "\n",
        "        # else: self.res_conv = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        out = self.block(x)\n",
        "        # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        return out + shortcut\n",
        "        # return out + shortcut + self.res_conv(x)\n",
        "        # return out + self.res_conv(x)\n",
        "        # return self.res_conv(x)\n",
        "\n",
        "# if out>in, inter=max=near. ave=ave\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# stride2\n",
        "# interconv/convpool\n",
        "# pixelconv\n",
        "# pixeluib\n",
        "# pixelres\n",
        "# shortcut\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "# model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CtmsyKatYT7h"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm(x)))\n",
        "        x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# model = AttentionBlock(d_model, d_head)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# # x = torch.rand(64, 3, 28,28, device=device)\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O9NnnDvZI37",
        "outputId": "af3ae664-c632-4552-f83e-7a065632d21b",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48890\n",
            "torch.Size([64, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title ae\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Res(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "    def forward(self, x): return x + self.model(x)\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, depth=4, num_res_blocks=1, n_head=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch # z_channels z_dim\n",
        "        # n_head = d_model // d_head\n",
        "        mult = [1,1,1,1] # [1,2,3,4] [1,1,1,1] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "        self.encoder = nn.Sequential(\n",
        "            # ResBlock(in_ch),\n",
        "            UpDownBlock(in_ch, ch_list[0], r=1/2),\n",
        "            # AttentionBlock(ch_list[0], d_head=d_head),\n",
        "            # AttentionBlock(ch_list[0], d_head=d_head),\n",
        "            # ResBlock(ch_list[0]),\n",
        "            # Res(UIB(ch_list[0], mult=4)),\n",
        "            UpDownBlock(ch_list[0], out_ch, r=1/2),\n",
        "            # ResBlock(out_ch),\n",
        "            # Res(UIB(out_ch, mult=4)),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            # ResBlock(out_ch),\n",
        "            # Res(UIB(out_ch, mult=4)),\n",
        "            UpDownBlock(out_ch, ch_list[0], r=2),\n",
        "            # AttentionBlock(ch_list[0], d_head=d_head),\n",
        "            # AttentionBlock(ch_list[0], d_head=d_head),\n",
        "            # ResBlock(ch_list[0]),\n",
        "            # Res(UIB(ch_list[0], mult=4)),\n",
        "            UpDownBlock(ch_list[0], in_ch, r=2),\n",
        "            # ResBlock(in_ch),\n",
        "        )\n",
        "        # self.vq = FSQ(levels = out_ch*[1024])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.quantise(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.quantise(x)\n",
        "    def decode(self, x):\n",
        "        x = self.quantise(x)\n",
        "        return self.decoder(x)\n",
        "    def quantise(self, x): # [b,c,h,w]->[b,h,w,c]->[b,c,h,w]\n",
        "        # return self.vq(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
        "        return x\n",
        "\n",
        "# split plateaus\n",
        "# conv 3<7\n",
        "# stride2 < pixel\n",
        "# convres\n",
        "\n",
        "batch=64\n",
        "in_ch=3\n",
        "z_dim=3\n",
        "h,w = 64,64\n",
        "model = VQVAE(in_ch, d_model=16, out_ch=z_dim, depth=4, num_res_blocks=1, n_head=-1, d_head=4).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "x = torch.rand((batch, in_ch, h, w), device=device)\n",
        "out = model.encode(x)\n",
        "# print(out.shape)\n",
        "# out = torch.rand((batch, in_ch, h, w), device=device)\n",
        "x_ = model.decode(out)\n",
        "# x = model.quantise(x)\n",
        "print(x_.shape)\n",
        "# # print(x)\n",
        "# x_ = model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-VeKdVamz2w"
      },
      "outputs": [],
      "source": [
        "# conv = nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2)\n",
        "# print(conv.weight.data.shape)\n",
        "print(model.encoder[0].block[-1].net[-1].gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WuXDKUIACl91"
      },
      "outputs": [],
      "source": [
        "# @title VQVAE me\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, depth=4, num_res_blocks=1, n_head=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch # z_channels z_dim\n",
        "        # n_head = d_model // d_head\n",
        "        # self.vq = VectorQuantizerEMA(num_emb=8192, emb_dim=out_ch, beta=0.5) # chat gpt\n",
        "        # self.vq = FSQ(levels = z_dim*[32])\n",
        "\n",
        "        mult = [1,1,1,1]\n",
        "        # mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            # nn.Conv2d(in_ch, ch_list[0], 3, 1, padding=3//2),\n",
        "\n",
        "            ResBlock(in_ch, ch_list[0]),\n",
        "            # ResBlock(ch_list[0], ch_list[1]),\n",
        "            # ResBlock(ch_list[1], ch_list[1]),\n",
        "            AttentionBlock(ch_list[0], d_head),\n",
        "\n",
        "            # nn.PixelUnshuffle(2),\n",
        "            # ResBlock(ch_list[0]*2**2, ch_list[1]),\n",
        "            # nn.AvgPool2d(2,2),\n",
        "            # nn.MaxPool2d(2,2),\n",
        "            UpDownBlock(3, ch_list[0], r=1/2),\n",
        "\n",
        "            ResBlock(ch_list[0], ch_list[1]),\n",
        "            # ResBlock(ch_list[2], ch_list[2]),\n",
        "            AttentionBlock(ch_list[1], d_head),\n",
        "            # ResBlock(ch_list[1], ch_list[2]),\n",
        "\n",
        "            # nn.PixelUnshuffle(2),\n",
        "            # ResBlock(ch_list[2]*2**2, ch_list[3]),\n",
        "            # nn.AvgPool2d(2,2),\n",
        "            UpDownBlock(ch_list[0], out_ch, r=1/2),\n",
        "\n",
        "            ResBlock(ch_list[2], ch_list[3]),\n",
        "            AttentionBlock(ch_list[3], d_head),\n",
        "            # ResBlock(ch_list[3], ch_list[3]),\n",
        "            ResBlock(ch_list[3], out_ch),\n",
        "\n",
        "            # # nn.GroupNorm(32, ch_list[-1]), nn.SiLU(), nn.Conv2d(ch_list[-1], out_ch, 3, 1, padding=3//2)\n",
        "            # nn.BatchNorm2d(ch_list[-1]), nn.SiLU(), nn.Conv2d(ch_list[-1], out_ch, 3, 1, padding=3//2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # nn.Conv2d(out_ch, ch_list[-1], 3, 1, padding=3//2),\n",
        "\n",
        "            ResBlock(out_ch, ch_list[3]),\n",
        "            # ResBlock(ch_list[3], ch_list[3]),\n",
        "            AttentionBlock(ch_list[3], d_head),\n",
        "            ResBlock(ch_list[3], ch_list[2]),\n",
        "            # nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            UpDownBlock(out_ch, ch_list[0], r=2),\n",
        "\n",
        "            # ResBlock(ch_list[3], ch_list[2]*2**2),\n",
        "            # # AttentionBlock(ch_list[2]*2**2, d_head),\n",
        "            # nn.PixelShuffle(2),\n",
        "\n",
        "            ResBlock(ch_list[2], ch_list[1]),\n",
        "            AttentionBlock(ch_list[2], d_head),\n",
        "            # ResBlock(ch_list[1], ch_list[0]),\n",
        "            # nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            UpDownBlock(ch_list[0], in_ch, r=2),\n",
        "            # ResBlock(ch_list[1], ch_list[0]*2**2),\n",
        "            # nn.PixelShuffle(2),\n",
        "\n",
        "            ResBlock(ch_list[1], ch_list[0]),\n",
        "            AttentionBlock(ch_list[0], d_head),\n",
        "            ResBlock(ch_list[0], in_ch),\n",
        "\n",
        "            # # nn.GroupNorm(32, ch_list[0]), nn.SiLU(), nn.Conv2d(ch_list[0], in_ch, 3, 1, padding=3//2)\n",
        "            # nn.BatchNorm2d(ch_list[0]), nn.SiLU(), zero_module(nn.Conv2d(ch_list[0], in_ch, 3, 1, padding=3//2)) # zero\n",
        "        )\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x = self.encoder(x)\n",
        "    #     # print(x.shape)\n",
        "    #     commitment_loss, x, _ = self.vq(x)\n",
        "    #     # print(x.shape)\n",
        "    #     x = self.decoder(x)\n",
        "    #     return x, commitment_loss\n",
        "\n",
        "    # def encode(self, x):\n",
        "    #     x = self.encoder(x)\n",
        "    #     _, x, _ = self.vq(x)\n",
        "    #     return x\n",
        "\n",
        "    # def decode(self, x):\n",
        "    #     _, x, _ = self.vq(x)\n",
        "    #     return self.decoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # x = self.quantise(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # return self.quantise(x)\n",
        "        return x\n",
        "    def decode(self, x):\n",
        "        # _, x, _ = self.vq(x)\n",
        "        # x = self.quantise(x)\n",
        "        return self.decoder(x)\n",
        "    def quantise(self, x): # [b,c,h,w]->[b,h,w,c]->[b,c,h,w]\n",
        "        return self.vq(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
        "\n",
        "\n",
        "\n",
        "batch=2\n",
        "in_ch=3\n",
        "z_dim=3\n",
        "h,w = 64,64\n",
        "model = VQVAE(in_ch, d_model=16, out_ch=z_dim, depth=4, num_res_blocks=1, n_head=-1, d_head=4).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "x = torch.rand((batch, in_ch, h, w), device=device)\n",
        "# out, _ = model(x)\n",
        "# print(out.shape)\n",
        "# x = model.quantise(x)\n",
        "# print(x.shape)\n",
        "# print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7CtfbCdIpCG",
        "outputId": "9ae91b19-bd81-423b-aea0-667f3dc450f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "89100\n",
            "torch.Size([2, 3, 8, 8])\n",
            "torch.Size([2, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # encoder mult=[1,2,4,4,8,8] # depth_list=[0,4,8,2,2,2]\n",
        "        # decoder mult=[1,2,4,4,8,8] # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, width_list[0], 3, 2, padding=3//2),\n",
        "\n",
        "            UpDownBlock(width_list[0], width_list[0], r=1/2),\n",
        "            ResBlock(width_list[0]),\n",
        "            UpDownBlock(width_list[0], width_list[-1], r=1/2),\n",
        "            ResBlock(width_list[-1]),\n",
        "            AttentionBlock(width_list[-1], d_head=4),\n",
        "            UpDownBlock(width_list[-1], out_ch, r=1),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            UpDownBlock(out_ch, width_list[-1], r=1),\n",
        "            AttentionBlock(width_list[-1], d_head=4),\n",
        "            ResBlock(width_list[-1]),\n",
        "            UpDownBlock(width_list[-1], width_list[0], r=2),\n",
        "            ResBlock(width_list[0]),\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpDownBlock(width_list[0], width_list[0], r=2),\n",
        "            nn.ConvTranspose2d(width_list[0], in_ch, 3, 2, padding=3//2, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=16, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "sx = model.encoder(x)\n",
        "print(sx.shape)\n",
        "out = model.decoder(sx)\n",
        "# out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rJQDMix9EpYZ",
        "cellView": "form",
        "outputId": "ed9f8177-8993-4438-a7e0-0ba8bde50ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0.3648523688316345\n",
            "0.007551356218755245\n",
            "0.006569647695869207\n",
            "0.005783926695585251\n",
            "0.00540410540997982\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.44149795..1.1510487].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.16275787..1.0677876].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0.005086201708763838\n",
            "0.004946894943714142\n",
            "0.004749026149511337\n",
            "0.0046072667464613914\n",
            "0.0042388709262013435\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARYAAAEWCAYAAACjTbhPAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANIRJREFUeJztnXt4VNW9978zyczkPiGB3CSBqGhQSkWuETy1GMtBa7XES33sK1re46sNyKW+2rzneMFjDa+eFmpP8HYotO+RYukpWvoeoZ5YsdZwi69VRFIQlGBIACGT61z3fv+g7r3Wmuw1e8/sSSbw+zzPPM9es9bstWbvPWvW77J+P4eqqioIgiBsxDncAyAI4tyDJhaCIGyHJhaCIGyHJhaCIGyHJhaCIGyHJhaCIGyHJhaCIGyHJhaCIGyHJhaCIGyHJhaCIGwnaRNLY2Mjxo8fj4yMDMycORO7d+9OVlcEQaQYjmTsFXrllVdw11134fnnn8fMmTOxZs0abN68Ga2trSgqKpJ+VlEUtLe3Izc3Fw6Hw+6hEQQRJ6qqoqenB2VlZXA6Y6xJ1CQwY8YMta6uTitHIhG1rKxMbWhoiPnZtrY2FQC96EWvFH21tbXF/B2nw2aCwSBaWlpQX1+vved0OlFTU4Pm5uao9oFAAIFAQCurf1tALdzVBndOnt3DM6TcnZzztgWTc95kcAFzDbzCH9J+/9COZTDYe+RN4+v29dvfhxXExfXRwODtAECFGl8nNsJ+TwfMSQb+3m48dWU5cnNzY7a1fWI5deoUIpEIiouLufeLi4tx4MCBqPYNDQ1YuXJl1PvunDy4c4duYslI0sTiHkETC3sNMoWJxe0a2rEMBjc+YWJxC2U7+rCCOLG4JedJhYklI46JRWtvQkUx7Fah+vp6+Hw+7dXW1jbcQyIIIkFsX7GMHj0aaWlp6Ozs5N7v7OxESUlJVHuPxwOPxxP1vqqoUJWhm9mTFe5KVZJz3mSgMGONCH9Ko4W/oJOh+PpI5DKz41OdxnWJEPf4hA/aNZ5kwT7vDrPf2sKPxPYVi9vtxtSpU9HU1KS9pygKmpqaUF1dbXd3BEGkILavWABgxYoVWLhwIaZNm4YZM2ZgzZo16Ovrwz333JOM7giCSDGSMrHcfvvtOHnyJB599FF0dHTgiiuuwLZt26IUujJUNXniyWDIVq6OBMYxhrnCncGhV9qdsNBnNqPE87h5Wag/zJ+nI6CXi9xD42+kqHo/4v1SbXpYVFXyXSRdeIVfUjmj7D4aGH5lrYiiMN/TpNxiRbpLysQCAIsXL8bixYuTdXqCIFKYYbcKEQRx7kETC0EQtpM0UShROgMRuFwRAECOU5cHs21yhBJRGPtquiBmpwnlkAWROYP9rE3m8zOCviNok2mTPa3QBcKCDoN1Bej0G3+vYhv1L+y/IKcjAGy7tpbMxMwQFMHJjNP5xND/WHGY67RJX5Ofpl9N8fl2M783N3PRreixaMVCEITt0MRCEITtpKwopEZUqJGzS6+eiL4E64nT41Pkq7m8TMWupLNFWUjAb0UWYrCylLS05I1zdZwjfM9MZjkfFkSCcEToMmKu044Bebsij3lRKYPZqi9KPmnMtY3z9vztvBZEN6YfcTxsWREqTwiyK/dYDJFlmhX5HMLyws/cW39Yfz9gwXWBViwEQdgOTSwEQdgOTSwEQdhO6upYkry7uS8iKBEY3Y1HFDoFuuMU4o/3R2I3GkJyhfCCbiejpxCGGhKul2LT9t2OAeO6MW7hPjDKCLH30YxZu33A/Niy0iVmYgGxppd5DtzCM9PF6FE6Uuy+A8Ao5pfvM/k8W/k90oqFIAjboYmFIAjboYmFIAjbSVkdixJRoIh6EBsJh439FZQY/iaROPULCemMkqBuCgl++xHGf0HsTgzhmskc95v0abFKlvC31yfuM2A47Y9Pp3Gp4M/kCxhfAxniM8M+I1H6qBSIosCqlmI971bbAbRiIQgiCdDEQhCE7aSsKJRsc3NEcu5Yko4qWWnLRmzWDT6ZqIo++LCLz+kRUiSikBBGLzNNL/eGkiOyhoUdzBFxRzPblhGbrYjQETEqtwzJ7VOEe8sF/k6B+y7CbjMQx27mM7GgFQtBELZDEwtBELZDEwtBELaTujoWJmxCMojSsTjYOrmMHokzC5ldbvCJEOnXkxyHPXwO3pCYpYxBvBOsqTpZ9ykSEU24Er0YYwq1opszq18YDAcXQU7UscQ3HtsQuiwXQi+y1WaHZ+Vr0IqFIAjboYmFIAjbSVlRSFGVpIoOimI8p8Za8nFmNyuB3lLA7Jjn0pfEYcGTNSwZn1jDtk3WUt+KKBTveKx4k0Y1ZaPECS4IEcbkXS64EH/WE8ZQ43IKERNZc7PJ35mV3yOtWAiCsB3LE8vbb7+NG2+8EWVlZXA4HHj11Ve5elVV8eijj6K0tBSZmZmoqanBwYMH7RovQRAjAMsTS19fH7761a+isbFx0Pqnn34azz77LJ5//nns2rUL2dnZmDdvHvx+f8KDJQhiZGBZxzJ//nzMnz9/0DpVVbFmzRr80z/9E2666SYAwC9/+UsUFxfj1VdfxXe+8x3zHbEu/TaJ8OU5+tcV5XcWWR0QLU+bxSVYc4PDoHNhXd9FnU+Q1bk45GPjXOit6MIsfOWIYP4OR4z/B8Ph+Fz6EzE3c+cR9Do5zM32BYSo/MNgfhb1U+wlMnv7rNxmW3UsR44cQUdHB2pqarT3vF4vZs6ciebm5kE/EwgE0N3dzb0IghjZ2DqxdHR0AACKi4u594uLi7U6kYaGBni9Xu1VXl5u55AIghgGht0qVF9fD5/Pp73a2tqGe0gEQSSIrX4sJSUlAIDOzk6UlpZq73d2duKKK64Y9DMejwcejyfq/WS49Mu2fbOJucMSHxfAoks/02VZJn/ew902pXWUbecPBbly0KEriNKE9IZBtih69wt9hELGuhq7EP1quLIwPk6HICawlwxPtrXDCmGhEw+TWF187qJ0LEOgcokK68BugTDryzNcEeQqKytRUlKCpqYm7b3u7m7s2rUL1dXVdnZFEEQKY3nF0tvbi0OHDmnlI0eO4P3330dBQQEqKiqwbNkyPPnkk5gwYQIqKyvxyCOPoKysDDfffLOd4yYIIoWxPLHs3bsXX//617XyihUrAAALFy7Ehg0b8NBDD6Gvrw/33nsvurq6MGfOHGzbtg0ZGRmW+lEU+136w5LzFWcypugY5kC7TJRD4eKvBHlRKJKmL9Ej/Xy2sKAr2/R5WbFEfIhCcZpTCz2823m0idQ4wh3rIlDg4hfipwaM/QNsMzdbcF9IZpB4IxRBfGfFn2S49FueWK655hqpTOZwOPDEE0/giSeesHpqgiDOEYbdKkQQxLkHTSwEQdhOyoZNSIa5OSJJeMVtn4/RLaOmQCiBMQ6Fa7fYR4gTY3mZORRmQ8vz5xEjpAXDxnoLC1ZJjqgo/IJIH5boJrixi6eR6AYi8Q5WICxcZwcXkTCGuXkIiArrwPwWFJNRHMy2A2jFQhBEEqCJhSAI26GJhSAI20ldHUsSMiHKQkrGCpXAksG4a/utZAEU9RZD4M8g9sGW0p28MiJo4bvY5f/BIguNCQh6DPH+ybZrCHXjvHqKe7u+R0C4di5GESf2MRwhSsVtBVzYBEoKTxDESIAmFoIgbCdlRSFFUc25EFtYVXLR08Q6K2IXF509tczNqui6Lekj0y13oZf2k4Sxs1HgBoPdUS3ewHRmme4Py0UPs/csdiu2T77G4dD/syOiODoMievEJHysi3+sBH1W2wG0YiEIIgnQxEIQhO3QxEIQhO2krI7FDpf+wkz+60kj80vc/WVYMVOLJENPwSZ9j9lW0GmoMXQcLMkwlYdj/M1x2yeES5fGfFYMjyHqNKzoCswimupZ9ZWou8p381/0jF+S9sGmR0TUK7FjioqsaNCnLAKjCK1YCIKwHZpYCIKwndQVhdTEPW+tLHmlnpticOY4+xBJhjghRgqTERJ3E/f3aodpnizpZ5MixgniqFP425OZo51cO/lu4nhdBGSfEkXijHTG81YYT5qDv+5D4YmbLUTVU2SikAHkeUsQxLBCEwtBELZDEwtBELaTsjoWJaIkrIMIR8xnnwpJIqLJSDmXfgvnDAsmUlb0V9Pl194uvQC7wToshIzzuvgtB8GQLIg7k3AuJN/WIHURiDNhmaiBiTBJ72TZIYChcfHPTOe/GKdjMXkvrTzrtGIhCMJ2aGIhCMJ2aGIhCMJ2UlbHYkcEOStu+hFJSAUZsfRAsnPlMX7fvgELIdDFMTDh063opcRE5ixpacJ5oiLf26QfYs4rqhpUlf/fC0XMub6HQkK7qATyceo0rIToYPQ8BZm8ruhkT4g/bRL8WEbnuLhylH6Eu+4mdSzJculvaGjA9OnTkZubi6KiItx8881obW3l2vj9ftTV1aGwsBA5OTmora1FZ2enlW4IghjhWJpYduzYgbq6OuzcuRNvvPEGQqEQvvGNb6Cvr09rs3z5cmzduhWbN2/Gjh070N7ejgULFtg+cIIgUhdLotC2bdu48oYNG1BUVISWlhb83d/9HXw+H9atW4eNGzdi7ty5AID169dj4sSJ2LlzJ2bNmmW6L6PdzaqF9agVc3OQFZusLHkTEAnYHbCJmBwj/oHYjQZBJgqpokgliilx9RiNzMgvmo05JLc21pI9GYHARTiTtjDWoLADXVV4scUO0oSyTPwzKxoO2e5mn88HACgoKAAAtLS0IBQKoaamRmtTVVWFiooKNDc3D3qOQCCA7u5u7kUQxMgm7olFURQsW7YMs2fPxqRJkwAAHR0dcLvdyM/P59oWFxejo6Nj0PM0NDTA6/Vqr/Ly8niHRBBEihD3xFJXV4d9+/Zh06ZNCQ2gvr4ePp9Pe7W1tSV0PoIghp+4zM2LFy/G73//e7z99tsYO3as9n5JSQmCwSC6urq4VUtnZydKSkoGPZfH44HH44l6X1GUhF2dxQhpLDkeIUJ9nNsHosy7EjFUtus8EZNjvJ+VfUrp7eXKaRnZcfWRCFayBrBfRnRTEPVysVzs4+lTvJihNGMlUEQwmyuKqBFJHPG3I4sSN+wu/aqqYvHixdiyZQvefPNNVFZWcvVTp06Fy+VCU1OT9l5rayuOHj2K6upqK10RBDGCsbRiqaurw8aNG/Haa68hNzdX05t4vV5kZmbC6/Vi0aJFWLFiBQoKCpCXl4clS5agurrakkWIIIiRjaWJ5bnnngMAXHPNNdz769evx9133w0AWL16NZxOJ2praxEIBDBv3jysXbvW8sBMe94yTdzC8lPmeZsmSF8hKzmY2T5sMl0m4mXMftaKOd4SoSBXdKTZ5LQtGW6sBGZG5xHF09wMXtRQrJw3TiJMdD4l4OfrxFzOFiKzme5fDJ4tRtVjRUezCdwsPOuWng4zFyAjIwONjY1obGy0cmqCIM4haBMiQRC2QxMLQRC2k7q7m00mLGN1CmGHhYhxEXvmVDFyuVRalNQlYlrnPhvjkikB3Z3cGSMSP0skwusJhsL8LOpKuK8m3mqJudntFHRvZnUFcUeT4zMFiK4MohuE26V3FBD1P3GqX8TMANFbT5jxUVJ4giBGAjSxEARhOzSxEARhO6mrY4nDpd+KS4klHwkLYm+8HgkJRRGzkmBAMTi2SDIyDIjIdCHS7RHC2Czda5sIhAJ6QRirqHNxMhkR7MqOGVaEbQOSa2DapZ+SwhMEMZzQxEIQhO2ksChkztzMErESXW4YlsdRSEyklk4jS2gvXBNOvExkl28SEtqLhMMSe6+4WZdJWCaKE1aW8HYJeKIowhLlXp/ObMmwScQUxZuoaIFM0az53cr2FVqxEARhOzSxEARhOzSxEARhO6mtY7Eob1qK4G9BxxL28/JyekYSIn4loLOwYpZnr2lkoI+rs+TiP8BuDcg0/TkrSKP6WTA3R0USTMBV37DPEJ+EzOFg+hQTpolbFZjx2ZW8THTpF8vceMilnyCIkQBNLARB2E7KikKKosBh1RxqYRUpWxqKONP5+Zf7rE3L6mSZm5WAmMxMYpq2NAb7TaQiUSKMyW5EcUI0vZoVmS3l8BYi7CnMjupQnxB9z8WL0mxiNnG3fLyIYkvIDs9bMjcTBDGc0MRCEITt0MRCEITtpKyORY3YZ3objHgTlCWLLDc/x/f5zW9ZlupYIha2Psd7TZJ0LcOqoMCSPQ5MU9GFPzwE91rU2bFD7+/lTdE5BfzPLsiML+YTb/In4RH0grLn3WzWAtWCiwatWAiCsB2aWAiCsB2aWAiCsJ2U1bFE/H34UnC24mrOIvorsBI7m6lORJQ5RT+WZCUb5MZgQS8ga2vFxyRef5Sk+bFItvrLEJvFuzUgFkpQ9BEanPQM/mcm+pTY5bvCkiFmBZXpWBRzfj5Jc+l/7rnnMHnyZOTl5SEvLw/V1dV4/fXXtXq/34+6ujoUFhYiJycHtbW16OzstNIFQRDnAJYmlrFjx2LVqlVoaWnB3r17MXfuXNx000346KOPAADLly/H1q1bsXnzZuzYsQPt7e1YsGBBUgZOEETq4lATzEhdUFCAZ555BrfccgvGjBmDjRs34pZbbgEAHDhwABMnTkRzczNmzZpl6nzd3d3wer2oemoH0jJyYra3IiZ53JJdycxl6P+in6vKKoxPFEuEgIUk9WwSMit1IvGKnFGfGwJRUYaYt87jMr7v0sc/xnYN6XVnxCSnW777O+jwyDsySQbzfBfnxzgn87X7g+ZcEsIDPfjTvVfA5/MhLy9P2jZu5W0kEsGmTZvQ19eH6upqtLS0IBQKoaamRmtTVVWFiooKNDc3G54nEAigu7ubexEEMbKxPLF8+OGHyMnJgcfjwX333YctW7bgsssuQ0dHB9xuN/Lz87n2xcXF6OjoMDxfQ0MDvF6v9iovL7f8JQiCSC0sTyyXXnop3n//fezatQv3338/Fi5ciP3798c9gPr6evh8Pu3V1tYW97kIgkgNLJub3W43Lr74YgDA1KlTsWfPHvz0pz/F7bffjmAwiK6uLm7V0tnZiZKSEsPzeTweeDzR8qDZCHJWTJ1mXZcdLiFMgl1bCyyEWJBtZ7CiNxHPIwsZoPT3aseW9C0ptj3C4xLd2a1kdDPmD/0TuXJN2nuGbXtP6vcouzhDel7Vac/z5WCjA8Z4ZlnVkvQ3xLYbyrAJiqIgEAhg6tSpcLlcaGpq0upaW1tx9OhRVFdXJ9oNQRAjCEsrlvr6esyfPx8VFRXo6enBxo0b8dZbb2H79u3wer1YtGgRVqxYgYKCAuTl5WHJkiWorq42bREiCOLcwNLEcuLECdx11104fvw4vF4vJk+ejO3bt+O6664DAKxevRpOpxO1tbUIBAKYN28e1q5dG9fAlIgCh5kltoVleFTSJgPS3PxlCYckS+kkBGYGor0hTYs/iujVGZ+YctbzWcfpNjYpR/rjD8qdDKwkrpPxvucy/g3B+yHSzUR+C/L3Rwnrz0wsL2orQeBlbUMRJmpd2Lz4J0bqM+ohbEEUsjSxrFu3TlqfkZGBxsZGNDY2WjktQRDnGLQJkSAI26GJhSAI20nZ3c1mzc1s0q1Ysr09RscYWBDvVcNCtKu5WVOfEvSbH4AFrJga7drtbEX3wBKGaGKPj8v7P+TK0XouRo8i6CnYqHV9Xfzn3FmCe4XbWAfjjAS4sppm7KqvMNtAIla2hJjUU6oW9Jm0YiEIwnZoYiEIwnZoYiEIwnZSWMdiXVaPpQeQuTn3n9Hl4KxRQ++HEUufYDbxu5UE8Zawcl67xhCnciSWO3u8iHoUJcTqswS/I1X/zw4N8Nq99Az+PA7Jc740n9828BOfsbMpu/UkbEHHEjH5O7NyXWnFQhCE7dDEQhCE7aSwKKRYX9YnsARnzbuBPt5k68p0x31euzAb6C+qnVhU9GW5wymJqCee10pQ7iQmmjPVv03nEd30o+ol5teMUdna8UCXkDBeuJZOpnzpnx/h6vZfeCFXXsHsMviX0zO4OnbLSjjM31vZNRHHY9iOEpYRBDGc0MRCEITt0MRCEITtpLCORXfpZ+V7h9M4TkEiruQZeXqUr4EzfCKqdI9L6CjubuLGrN4iVjslGDCsk0WTH4rEZ1HnGYoLLXTB6lXE7yE+e1JXeAeT6F3Q/Q2c4XV4maP1Z8/t5vV56enGP9Hxji+48meRAu04ZtgEddBDKUlLWEYQBGEGmlgIgrAdmlgIgrCd1NaxDKIvkOkQ7PKfEGXr4fDLEPULdrn0i/K+6c/K6qKysKdW1H4ryPRDET/v1xLo1vVVnjw+nEGECV+hqvz/txh+YOLOx7TjCZdfLh1fV1eXdnzLqE+4uqdPjNL7F59ZK4+wQVslTC79BEEMIzSxEARhOykrCn266ziQfnaX8fhZxgnPOMJhvuyMb96MipAvEx8EN2dnuk1ztbiSlSUwCw4Y1kWdVibWSeoiA7wYIDNNsxH+o6L7C/h9usiQ4ZUn9hoKZKJQlIjMuNDLPpeWzrvXh4Nhg5Zy8zIALre5mM74oaK92vHPunh3fzuwkviNViwEQdgOTSwEQdgOTSwEQdhOyupYyqcWwek5u/XcrIt4RNA1yLL3SRH6GzjNn5fVBbAmR7HOTqTXwIoLvSz8QhLc9mO18+TqZtpYbc2GjrBCrNAIXP+CmVhhzM9qlktsruF081sBph98lit/85vf1I47Ojr4PsVsDUz5888/5+rGjhunjy0JLhJWzpnQimXVqlVwOBxYtmyZ9p7f70ddXR0KCwuRk5OD2tpadHZ2JtINQRAjjLgnlj179uCFF17A5MmTufeXL1+OrVu3YvPmzdixYwfa29uxYMGChAdKEMTIIS5RqLe3F3feeSdeeuklPPnkk9r7Pp8P69atw8aNGzF37lwAwPr16zFx4kTs3LkTs2YZBwIWUSODe95aQfZ5fze/w5Td3SyajCNCUnh2yZ6emW5YZydWzKAcivHYo03a+tLe4TI2JwOQmqb57nnx4bOdn3HlcbPGYViRORTHMONzUopwnrw+/Xte2PF/uTpW9AGAL77QdymL5maZKCTWdZ44qR0vLfkLV/eTY18RBs8cGwcM4BCTx8uIa8VSV1eHG264ATU1Ndz7LS0tCIVC3PtVVVWoqKhAc3PzoOcKBALo7u7mXgRBjGwsr1g2bdqE9957D3v27Imq6+jogNvtjnLcKS4ujlJKfUlDQwNWrlxpdRgEQaQwllYsbW1tWLp0KV5++WVkZNhj/aivr4fP59NebW1ttpyXIIjhw9KKpaWlBSdOnMCVV16pvReJRPD222/jX//1X7F9+3YEg0F0dXVxq5bOzk6UlAzulu/xeODxRCe6NpsUXobUFV/itp/m4V2wQ4GwYVuHw0JUsQSQ7uqW6V9CxhHj5B3GuPZm743QrujSIr4bic7HrOyfCFHXjil+b/UPuKp/q/tnw89e8claro7Xo/A6FVHcZ/UqEydO5Oo++ugjvk/mvojPt4PZwuIQ7l9YeIZlW0+M7qyVrBmWJpZrr70WH374IffePffcg6qqKjz88MMoLy+Hy+VCU1MTamtrAQCtra04evQoqqurrXRFEMQIxtLEkpubi0mTJnHvZWdno7CwUHt/0aJFWLFiBQoKCpCXl4clS5agurrakkWIIIiRje2et6tXr4bT6URtbS0CgQDmzZuHtWvXxv4gQRDnDA41GX7SCdDd3Q2v14uyuq2aS3+8OCRb+wM9vO6BdS230lb0cUlzmc8uKCOWDwXXNmTcVlYnI5YfS0w/l79x7INTXLl0UilXTvcY/7d1d/C6iLySPO04kQj+/5EnD+VgxILPDnDluvGHtePCwkLDz/X3y7cNZGXp4+nt7eXqRL2GTMcCJrOlWDd69GiuvOqIPFLdYCiBPnz6L9fC5/MhLy9P2pY2IRIEYTs0sRAEYTspu7vZDnOzzCQaFQ0szrah/hBX58y1Z66Wjkd085aYuOPdFuFwyk2Lsvrf3spckwW5XN2sx49z5QumXGB4HtF0b8XcydJ0gXEfuZnyJT3Lo6N3ceUrTl2vF3iJD3u872jHLhe/81m8f4GALmqnpfGidJQ7g+QaqExTR5Spnu8zKti2cVO9bwqmTRDEcEITC0EQtkMTC0EQtpO6OhZV1WTRuE2LQT40giNdT7gtujTLXPGj3P+ZtrI6K8RrFgbkkdXi9SYQzd0OV4ZQr5tQ757Gm+r3f/Sedvzee+9xdb9ZdA1X/h9/1M20USZ/0dtepnNjqt6sGMtVffz5fq488YLLtOOeAd6kLdO5fBO38m8wFtxjp45yVdN9c7TjIxfxZurTp09zZVZvIt4vUcfClsU6rp1QPnniBFf+x4v0fp5onQgzKBSlnyCI4YQmFoIgbCdlRSFFUWzIAczv6IyE9HnUKXjIypfZEtO0rM4Csl22UW0tiE225Z12Gp/nIj9vhj0Q0aPCbU6fxtVdg1au/MVhPXpaySR+B7woFrT/pd2wLUvDu49z5fqrHh+0HWDN3Cwiij8s+0patOOBU/z9shIVzkpbVqSK8tiVlJWwEGXQYFu5mJxPBq1YCIKwHZpYCIKwHZpYCIKwnZTd3Vx4z2/gdJ/d3exMs2f+CwV0vYrDycuRrkzjhFPBvqBhnXjx3NnG5xFxpumu3BG/efk14hcStKcZmx3VkN+wTsZFhbwO6qeCa/6NG/TznvyU39bwm0XGeaS+//YErnzioHHw9Owx/O72jFzd5H3V8Z9ydaWlus6lvLycqxPNsk4m0tqxr/2Cq1v41iva8cGr/4Ore/cvH3Dl8l49xpDYh9fr1Y7FQPJTpkyBEbLdzGK92FZmthbbFhXxkfxYHt1XNfjYgn04+cL1tLuZIIjhgSYWgiBshyYWgiBsJ2X9WL440gukn5ULx1yUG6O1ORxMNPRIiJc50zi7viC7WlBDWfFjufeny7Tjtff92PTnRE9uvk8LfjWSqv85ndeTHGvjy89dqx+7XG6uLsSoXFh9BgAsncl3+o8H9WNRN/OLjX/gyuNHj9eOJ33zhkHHDQDHj/OhGcQx7C/9tXacW8m7ur/59uva8RdTfsnV/bWDz+I4fmC2djx2LL+NgB2DGO959+7dXPmyy/QtBjk5OVydGNE/Xj8WscwmlL9ACCvx8CUfa8cNH1+qn8OCXxmtWAiCsB2aWAiCsJ2UFYVYTn7Sox2PrsyRtJTjdOnzaCRgnCw97Oe3AlhBdKFnE0XlFbWKzQ0/Z6WPLz7Xv0tBcUhsHhfb/riTK28JLDBsG+rkl+s/X6ib5/v6+ri647v+jSuv/sZFTIk3E4uwqWc++4wXS1hxRxR9RJHhRK5uNp5QehlXl5uri93+HN7cnTueH4/zqN5PZycvxrHixQlhZ7FobmYTlsXy/giH9edJ5u4flczMQiS6bLfelv1dWBHzacVCEITt0MRCEITt0MRCEITtjAgdC0tCYQCcxvIie95EsgOIsiur17lx8W9Mf07G6c95/dCoUr0PK5HUZfymZ77wjhBRLl13r+8O87qIQECPkLb0T7wOA+DLp/+gb09YfNErXN2ll17KlT/55BO9f0FnIDPDZmfz45t0+B6mxId8ON6rJyE7uP9jrm7ydH7s2z59UDu+/thPuLqOjg4YIY7P79e3R0SEKG2yiHKsvgWQu/TLTNOHDx/m6vLz87XjH03R/QEGBgawDOawtGJ5/PHH4XA4uFdVlb6vwO/3o66uDoWFhcjJyUFtbW2UUosgiHMfy6LQ5ZdfjuPHj2uvd97R86csX74cW7duxebNm7Fjxw60t7djwQJjawJBEOcmlkWh9PR0lJRER+/y+XxYt24dNm7ciLlz5wIA1q9fj4kTJ2Lnzp1R3ofxEm/SKgBAQDd9qiq/CznQb7yD2QqiGHX6iC4WBLGPq2ttPWP4uegTm+uz6yTvBestDIjNDdm+RI8OPe9npyQtIU0GFwzqJu/+Lv66ZuYZ7/4+3cMH0y7OMhYLjh07xtXd1/5X7fiNq3kxTvRePdmmj6m/vZ2rO1K5RjvOF8SHAx/zQbGdqv7zCQb57ykLep2IxyxbFvNFs+JXrF3SrIk7FOJdFE6d0u89aypnE6vFwvKK5eDBgygrK8OFF16IO++8E0ePng3P19LSglAohJqaGq1tVVUVKioqoraNEwRxbmNpxTJz5kxs2LABl156KY4fP46VK1fi6quvxr59+9DR0QG3280pfgCguLhYqsgKBALcTCj+uxAEMfKwNLHMn68vMSdPnoyZM2di3Lhx+PWvf43MzMy4BtDQ0ICVK1fG9VmCIFKThMzN+fn5uOSSS3Do0CFcd911CAaD6Orq4lYtnZ2dg+pkvqS+vh4rVqzQyt3d3VERwFgSThT/N5zgZeJIxHzkNxmiOTx/bL52XHoBH7XrzEe6LFvz4I1c3X8+voUrD/TopkUnH9zNUkJ7Gfs+3Ksdr7s5n6v73m+9fGPmvFlClaz/M5/zZusbM/+Pduz38/fg9tpvcOV/WbNOO/5F+CRX1+3VTcpVH7zN1S3P4Xcefxu6efc3Z7q4upm1uom79QC/BaNqIh9ZzfHr27TjgIPXP4jbClisuOLL9DGiJMDWHTlyhKsTd1+zepVRo0ZxdayOhe1fNIXLSMhBrre3F5988glKS0sxdepUuFwuNDU1afWtra04evQoqqurDc/h8XiQl5fHvQiCGNlYWrE8+OCDuPHGGzFu3Di0t7fjscceQ1paGu644w54vV4sWrQIK1asQEFBAfLy8rBkyRJUV1fbZhEiCGJkYGliOXbsGO644w588cUXGDNmDObMmYOdO3dizJgxAIDVq1fD6XSitrYWgUAA8+bNw9q1a5MycIIgUhdLE8umTZuk9RkZGWhsbERjY2NCg5JhxaX/0+ybuPL4vtcM2yqhfsM6R7p5xXTUVgGJAwobOayjk5eX+7p434K8Il3/0H2Cr+PlcqekTg6rGxtbfrFwHsGDmkkK/7/n8ZkAcnN1fcfUk3xkvGnT+MyItd/S3etPnvycqzvWdogrszL+Z318n6M8xjqy1b28z8sBX59BS16v4ikwbAaA912RJWiPhdmocCIyHUtZWRlXJ+pH2LZiWAej8Vh5lmgTIkEQtkMTC0EQtjPidjcravwu/aemrNaO1VPCjs6DPxGb620tmGx97T6u7MowXqLv379fO/amXy09ryj+sKicSzbvFm9l7A8w+bmereXFkOsvH8OVj/3Xs9pxae4dhucULYKtrbwJ1+35e8PP/ucf+KDT7HJ+XHYGV9cdNm8KZbnscn7H8v6P9HuSVSr/7Pxr9J2/+c4s030eE7ZdNLXo9npx064solxFRQVXZncpVxbx5/n69MTdKXr7gnjpJXNtacVCEITt0MRCEITt0MRCEITtjDgdi2hu5sxhQnD9Mj+f1Lu/9BG94OGj/Udkwe1DvBs60tyDt0O0TiPIhGM4eYJ3Q2dP03GIj2QGLDHs4+Ce27gyaxb9oIPfSn/bN54xPI/IjdNZkyn/PW6byI89fdJd2nFYcvFu/XaN8A5fZk3KG17eLh3f9267RDs+XPYFV3dq5Qdic43Rj03mym9du0c7XvjefzPu8D3pcHBvV712vLpgobwxQ2XnP3Ll5RN0b/Ou0jNcXfji57iyM5u9D7x5HnN5/dpwQisWgiBshyYWgiBshyYWgiBsx6HGSr02xHR3d8Pr9QLXbADSo30DMnL5mAF/f4nurr3lwnfE5jztH5kaw76rp5tqZ4bntur6Ga+Xjy/gcum+BexWdQBY8EITVw75dZ3CJaO5KhTN0I99vPsJ+pnE8wDgGKW720ffetmjwNd19+r/Sf/vULbYWOOK8Z9y5QyXPSFAZbgz+Qv02RzeR+mKf75SO/b9uIWr62RUHGP+/AOuLvqXohocAyH/aZilf9dy7Thr5mpJy6Fhw3/px2wmy2AwhJ+/8Bp8Pl/MKAS0YiEIwnZoYiEIwnZS1tzcvHw7crKNzbqD0fXrzVw5v1RMlmWMneIPS3GJbgt2pgmJuRkX9ZIy3kw8Ppc3p+bp+dDR86lxf6LJvWLDGq7MmrgP3H0P31g1XtqLeJivMmuCtOmQExzgxcqvXimIcT/RxZ/y6/h7krZDb9s3ECNTgU3w4g8/nuxRfNK2QL/uqh8O8KbpePnsOL9TvLhUj/h47KjuSqCq5ndw04qFIAjboYmFIAjboYmFIAjbSVkdi1kyc8drx/lTb+XqDl3BR1W/+D95F3+WDCaSu1+IOGaFz0/x2/dnVvZqx6/v4RUggYBevr6aN61n8VEKEGbE4OIZfB2rQXAJll8xsNmBhYzreQIhKOIle1SVYV3fmQOGdYmQN44vh5jUVUqI17/kMm2N48zFZsEJ/d66y/O5ut9KQyyIZmtez+Nw6GsBVwavl2M/a8XcHQrzz0GOS3/YsnL0bQJpLtKxEAQxjNDEQhCE7YwIUcjhNI5+5e9jxBbBs/Zik562AHDx63qCsK7j+7m6Y7eZ37l6weg0w7r/Pl+sM96NmiF41/YxG1ldgtNjkFnau/hN21LTdLJwMP9XooWy70yr0Np+x++Nf+RFzsjE73Fl12T9eUpLE+6J7pSL6/18gPXsDHGsJkUDQR6tdfDm3V8N9Jg+p1lhhBWZYn1yQkUuV75Y1U3Mn2Tp16B/QBYCgIdWLARB2A5NLARB2A5NLARB2E7K6lheeScdHs/Z4X33GmPZ7rNjuoLh6AXjuTrFLeowjOXM2e/+Vjv+81ULuLpx0mRU9sjEIhlCsqzuT/TjCC+ig9n4DDcvLsMp7IpwOJNxy42/ZVSNhQvikDT+2W95d/bMLD2pnCJEGczM4v8/MzL0CP/sDnMACId1/czvBPeAnp4erswmLGPPCYDT1Vwu6JEeCwjPs5N5ThO5YJY+Ze68l4zXx9bbZ35nOq1YCIKwHcsTy+eff47vfve7KCwsRGZmJr7yla9g7969Wr2qqnj00UdRWlqKzMxM1NTU4ODBg5IzEgRxrmFpYjlz5gxmz54Nl8uF119/Hfv378ePf/xjjBo1Smvz9NNP49lnn8Xzzz+PXbt2ITs7G/PmzYPf75ecmSCIcwlLEeR++MMf4s9//jP+9Kc/DVqvqirKysrwgx/8AA8++CAAwOfzobi4GBs2bMB3vvOdmH18GUHufyy+FZ6/JfouHKXrBTJzR3Htb5gsRNA3TfxJvOWnsUcmdkR4N2tP3a/0wsv/K64+ouFvfSJbGYyx6ToDOHJMd7IPC9kQmg+Z/4/MZvQxHiFTJftzOHOG16nIyMzg9XlTxhr/kU4YJ4++JmPMnmcN607OWKodi0nq3ZnFcff5JT29fnz1ayvtjyD3u9/9DtOmTcOtt96KoqIiTJkyBS8xORePHDmCjo4O1NToKR68Xi9mzpyJ5ubmQc8ZCATQ3d3NvQiCGNlYmlgOHz6M5557DhMmTMD27dtx//3344EHHsAvfvELAEBHRwcAoLiYnx2Li4u1OpGGhgZ4vV7tVV5eHs/3IAgihbAkCrndbkybNg3vvvuu9t4DDzyAPXv2oLm5Ge+++y5mz56N9vZ2lJbqGbVvu+02OBwOvPLKK1HnDAQCCAQCWrm7uxvl5eVY/sM74fGctZWOLtCXXZcUdIlfQTuaME6wtcaNfct3/lTG53WJ9mXpXbHiBi9pKwkOLVZFQubFAjk2XluT5z1yjF8FRxSz108+1nFl+v4Jt8t4K4fV85rFmS6YuC3FxTfbVm/X0xfAldf+2H5RqLS0FJddxod7nDhxIo4ePQoAKCk5G9Kus5PPdN/Z2anViXg8HuTl5XEvgiBGNpYmltmzZ6O1ld9E9te//hXjxp0NYlFZWYmSkhI0NempK7q7u7Fr1y5UV1fbMFyCIEYCltwwly9fjquuugpPPfUUbrvtNuzevRsvvvgiXnzxRQBnNdHLli3Dk08+iQkTJqCyshKPPPIIysrKcPPNNydj/ARBpCCWJpbp06djy5YtqK+vxxNPPIHKykqsWbMGd955p9bmoYceQl9fH+699150dXVhzpw52LZtW7TLcwxmXZSGrKyzcmthvu5aXZg/2ugjGA753dpphPMwMnEk2As51mVioYsEzinHmWZXMvLk3L+Lxo2J3WiQIdjlFm8N/pyRsHEcu0jQit5LlZQkbZnDSDAQ3dSAlM2E+MpLdyEr66zytjBff3AL82UT1MidWGIz3BOLcdtUn1gsblCy8KnhnVgSeX7imVh6+wKYPv8FyoRIEMTwkLK7mydUjkFOtol/QsmfRnRV8v9hQgEhiLFqWIhB4v8wZs4bdzumOgzjJXlGzgUm+xuMFFjBmP6YQ1KS4+9tM65UxaJN949raq5tJES7mwmCGEZoYiEIwnZSThT6Upfc22dSA21pVZt8USgcEJaL54EoJFueh2DekjB0pJYoFJAFUEohUaivP/i35rHbp5xV6NixY7RfiCBSmLa2NowdO1baJuUmFkVR0N7eDlVVUVFRgba2NnLzH4Qv91TR9TGGrpEcq9dHVVX09PSgrKwMTqdci5JyopDT6cTYsWO18Am0f0gOXZ/Y0DWSY+X6eL1eU+1IeUsQhO3QxEIQhO2k7MTi8Xjw2GOPweOxy1383IKuT2zoGslJ5vVJOeUtQRAjn5RdsRAEMXKhiYUgCNuhiYUgCNuhiYUgCNtJ2YmlsbER48ePR0ZGBmbOnIndu3cP95CGhYaGBkyfPh25ubkoKirCzTffHBV32O/3o66uDoWFhcjJyUFtbW1UQPPzgVWrVmnhUb+Ers0wpUVWU5BNmzapbrdb/fnPf65+9NFH6j/8wz+o+fn5amdn53APbciZN2+eun79enXfvn3q+++/r15//fVqRUWF2tvbq7W577771PLycrWpqUndu3evOmvWLPWqq64axlEPPbt371bHjx+vTp48WV26dKn2/vl+bU6fPq2OGzdOvfvuu9Vdu3aphw8fVrdv364eOnRIa7Nq1SrV6/Wqr776qvqXv/xF/da3vqVWVlaqAwMDcfebkhPLjBkz1Lq6Oq0ciUTUsrIytaGhYRhHlRqcOHFCBaDu2LFDVVVV7erqUl0ul7p582atzccff6wCUJubm4drmENKT0+POmHCBPWNN95Qv/a1r2kTC10bVX344YfVOXPmGNYriqKWlJSozzzzjPZeV1eX6vF41F/96ldx95tyolAwGERLSwuXptXpdKKmpsYwTev5hM/nAwAUFJxNcNbS0oJQKMRdr6qqKlRUVJw316uurg433HADdw0AujZActIimyHlJpZTp04hEolYStN6vqAoCpYtW4bZs2dj0qRJAM6mtXW73cjPz+fani/Xa9OmTXjvvffQ0NAQVXe+XxsgOWmRzZByu5sJY+rq6rBv3z688847wz2UlKCtrQ1Lly7FG2+8YTm9zPmCoiiYNm0annrqKQDAlClTsG/fPjz//PNYuHBh0vpNuRXL6NGjkZaWZilN6/nA4sWL8fvf/x5//OMfuSA7JSUlCAaD6Orq4tqfD9erpaUFJ06cwJVXXon09HSkp6djx44dePbZZ5Geno7i4uLz9tp8STLSIpsh5SYWt9uNqVOncmlaFUVBU1PTeZmmVVVVLF68GFu2bMGbb76JyspKrn7q1KlwuVzc9WptbcXRo0fP+et17bXX4sMPP8T777+vvaZNm4Y777xTOz5fr82XDFta5LjVvklk06ZNqsfjUTds2KDu379fvffee9X8/Hy1o6NjuIc25Nx///2q1+tV33rrLfX48ePaq7+/X2tz3333qRUVFeqbb76p7t27V62urlarq6uHcdTDB2sVUlW6Nrt371bT09PVH/3oR+rBgwfVl19+Wc3KylL//d//XWuzatUqNT8/X33ttdfUDz74QL3pppvOTXOzqqrqz372M7WiokJ1u93qjBkz1J07dw73kIYFnI2KHPVav3691mZgYED9/ve/r44aNUrNyspSv/3tb6vHjx8fvkEPI+LEQtdGVbdu3apOmjRJ9Xg8alVVlfriiy9y9YqiqI888ohaXFysejwe9dprr1VbW1sT6pPCJhAEYTspp2MhCGLkQxMLQRC2QxMLQRC2QxMLQRC2QxMLQRC2QxMLQRC2QxMLQRC2QxMLQRC2QxMLQRC2QxMLQRC2QxMLQRC2QxMLQRC28/8B2pO0YvdlL1UAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.463676..1.2173462].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.1471297..1.0436165].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "0.004266697913408279\n",
            "0.003956572152674198\n",
            "0.003679466200992465\n",
            "0.0038613067008554935\n"
          ]
        }
      ],
      "source": [
        "# @title train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler(device)\n",
        "\n",
        "def train(model, optim, dataloader, scheduler=None):\n",
        "    model.train()\n",
        "    # for i, (x, _) in enumerate(dataloader):\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        # x1 = F.interpolate(x1, size=(16,16)).repeat(1,3,1,1)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x_, commitment_loss = model(x)\n",
        "            # loss = commitment_loss + F.mse_loss(x, x_)\n",
        "\n",
        "            x_ = model(x)\n",
        "            loss = F.mse_loss(x, x_)\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if i%10 == 0:\n",
        "            # with torch.no_grad():\n",
        "            #     state = buffer[12][40][0]\n",
        "                # transform = transforms.Compose([transforms.ToTensor()])\n",
        "            #     x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "            #     out = model(x).squeeze(0)\n",
        "            #     sx = model.encoder(x).squeeze(0)\n",
        "            #     out = model.decoder(sx).squeeze(0)\n",
        "            #     imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "            #     imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i % 100 == 0: print(loss.item())\n",
        "\n",
        "\n",
        "# for i in range(1):\n",
        "for i in range(10): # 10\n",
        "    print(i)\n",
        "    train(model, optim, train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # z = torch.randn(1,z_dim,8,8).to(device)\n",
        "        # _, z, _ = model.vq(z)\n",
        "        # z = model.quantise(z)\n",
        "\n",
        "        # out = model.decode(z)\n",
        "        # imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "        state = buffer[12][40][0]\n",
        "        transform = transforms.Compose([transforms.ToTensor()])\n",
        "        x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "        sx = model.encoder(x)\n",
        "        out = model.decoder(sx)\n",
        "        imshow(torchvision.utils.make_grid(x.cpu()))\n",
        "        imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "        imshow(torchvision.utils.make_grid(out.cpu()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "de396606-7f82-4a69-c1e9-c2968ffd9387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▄▄▄▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.00292</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">likely-butterfly-61</strong> at: <a href='https://wandb.ai/bobdole/vqvae/runs/ewii657n' target=\"_blank\">https://wandb.ai/bobdole/vqvae/runs/ewii657n</a><br> View project at: <a href='https://wandb.ai/bobdole/vqvae' target=\"_blank\">https://wandb.ai/bobdole/vqvae</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250324_140132-ewii657n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250324_142544-qoa2y9ze</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/vqvae/runs/qoa2y9ze' target=\"_blank\">worthy-shadow-62</a></strong> to <a href='https://wandb.ai/bobdole/vqvae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/vqvae' target=\"_blank\">https://wandb.ai/bobdole/vqvae</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/vqvae/runs/qoa2y9ze' target=\"_blank\">https://wandb.ai/bobdole/vqvae/runs/qoa2y9ze</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"vqvae\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXLAI-bVQ6G9"
      },
      "outputs": [],
      "source": [
        "# z = torch.randn(1,z_dim,16,16).to(device)\n",
        "# with torch.no_grad(): out = model.decode(z)\n",
        "# imshow(torchvision.utils.make_grid(out))\n",
        "\n",
        "with torch.no_grad():\n",
        "    state = buffer[12][40][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "    # out = model(x).squeeze(0)\n",
        "    sx = model.encoder(x)#.squeeze(0)\n",
        "    out = model.decoder(sx)#.squeeze(0)\n",
        "    imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "    imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "# out, _ = model(x)\n",
        "# imshow(torchvision.utils.make_grid(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI3uaeX-r73O"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yfLogBYztcuM"
      },
      "outputs": [],
      "source": [
        "# @title chatgpt quantizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_emb, emb_dim, beta=0.5, decay=0.99):\n",
        "        super().__init__()\n",
        "        self.num_emb, self.emb_dim = num_emb, emb_dim\n",
        "        self.beta, self.decay = beta, decay\n",
        "        self.epsilon = 1e-5\n",
        "\n",
        "        self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim))\n",
        "        # self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim).uniform_(-3**.5, 3**.5))\n",
        "        # self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim).uniform_(-1./num_emb, 1./num_emb))\n",
        "\n",
        "        # Register buffers for EMA updates.\n",
        "        self.register_buffer('ema_cluster_size', torch.zeros(num_emb))\n",
        "        self.register_buffer('ema_w', self.embeddings.data.clone())\n",
        "        # self.ema_cluster_size = nn.Parameter(torch.zeros(num_emb), requires_grad=False)\n",
        "        # self.ema_w = nn.Parameter(self.embeddings.data.clone(), requires_grad=False)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Save the original shape and flatten the input to (batch_size * ..., emb_dim)\n",
        "        input_shape = z.shape\n",
        "        # flat_z = z.view(-1, self.emb_dim)\n",
        "\n",
        "        # flat_z = z.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        flat_z = z.permute(0,2,3,1).flatten(end_dim=-2) # [b*h*w,c]\n",
        "        # print(flat_z.shape)\n",
        "\n",
        "        # distances = (torch.sum(flat_z ** 2, dim=1, keepdim=True) + torch.sum(self.embeddings ** 2, dim=1) - 2 * torch.matmul(flat_z, self.embeddings.t()))\n",
        "        distances = (torch.sum(flat_z**2, dim=1, keepdim=True) + torch.sum(self.embeddings**2, dim=1) - 2*torch.matmul(flat_z, self.embeddings.T))\n",
        "        enc_ind = torch.argmin(distances, dim=1)\n",
        "        encs = F.one_hot(enc_ind, self.num_emb).type(flat_z.dtype)\n",
        "        # Quantise the input by replacing with the nearest embedding.\n",
        "        z_q = torch.matmul(encs, self.embeddings).view(input_shape)\n",
        "\n",
        "        commitment_loss = self.beta * F.mse_loss(z_q.detach(), z) # commitment loss.\n",
        "        # loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q-z.detach())**2) # comitment, codebook\n",
        "\n",
        "        if self.training:\n",
        "            # EMA update for cluster size.\n",
        "            ema_cluster_size = torch.sum(encs, dim=0)\n",
        "            self.ema_cluster_size = self.ema_cluster_size * self.decay + (1 - self.decay) * ema_cluster_size\n",
        "            # Laplace smoothing to avoid zero counts.\n",
        "            n = torch.sum(self.ema_cluster_size)\n",
        "            self.ema_cluster_size = ((self.ema_cluster_size + self.epsilon) / (n + self.num_emb * self.epsilon)) * n\n",
        "\n",
        "            # EMA update for the embedding weights.\n",
        "            # dw = torch.matmul(encs.t(), flat_z)\n",
        "            dw = encs.T @ flat_z.detach()\n",
        "            # print('vq fwd', encs.T, flat_z)\n",
        "            self.ema_w = self.ema_w * self.decay + (1 - self.decay) * dw\n",
        "\n",
        "            # Update embeddings with the EMA values.\n",
        "            self.embeddings.data = self.ema_w / self.ema_cluster_size.unsqueeze(1)\n",
        "\n",
        "        # # Compute perplexity of the encs.\n",
        "        # avg_probs = torch.mean(encs, dim=0)\n",
        "        # perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + self.epsilon)))\n",
        "\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # print(z_q.shape)\n",
        "        # flat_z = z.permute(0,2,3,1).flatten(end_dim=-2) # [b*h*w,c]\n",
        "\n",
        "        # z_q = z_q.transpose(1,2).reshape(*bchw)\n",
        "        # return loss, z_q, perplexity, enc_ind\n",
        "        return commitment_loss, z_q, enc_ind\n",
        "\n",
        "emb_dim, num_emb = 4,20\n",
        "# x = torch.randn(2, 3, 4)\n",
        "x = torch.randn(2, emb_dim, 5, 7)\n",
        "vq = VectorQuantizerEMA(num_emb, emb_dim, beta=0.5) # chat gpt\n",
        "loss, z_q, enc_ind = vq(x)\n",
        "print(z_q.shape)\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4iRZMIxPsMTV"
      },
      "outputs": [],
      "source": [
        "# @title vqvae conv\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=32, z_dim=3):\n",
        "        super().__init__()\n",
        "        d_list=[16, 3] # 849126\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        kernel = 3\n",
        "        self.encoder = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, 16, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 14\n",
        "            nn.Conv2d(in_ch, d_model, kernel, 2, kernel//2), nn.BatchNorm2d(d_model), act,# nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(d_model, z_dim, kernel, 2, kernel//2), act\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, d_model, kernel, stride=2, padding=kernel//2, output_padding=1), nn.BatchNorm2d(d_model), act,\n",
        "            # nn.Upsample(scale_factor=2),\n",
        "            nn.ConvTranspose2d(d_model, in_ch, kernel, 2, padding=kernel//2, output_padding=1)\n",
        "        )\n",
        "        # self.vq = VectorQuantizerEMA(num_emb=20, emb_dim=z_dim, beta=0.5) # chat gpt\n",
        "        self.vq = FSQ(levels = z_dim*[8])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # print(x.shape)\n",
        "        commitment_loss, quantised, encoding_indices = self.vq(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(quantised)\n",
        "        return x, commitment_loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.quantise(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.quantise(x)\n",
        "    def decode(self, x):\n",
        "        # _, x, _ = self.vq(x)\n",
        "        x = self.quantise(x)\n",
        "        return self.decoder(x)\n",
        "    def quantise(self, x): # [b,c,h,w]->[b,h,w,c]->[b,c,h,w]\n",
        "        return self.vq(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
        "\n",
        "\n",
        "in_ch=3\n",
        "d_model=32\n",
        "z_dim=3\n",
        "model = VQVAE(in_ch, d_model, z_dim).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 16x16 conv 17651 ; pixel(3)(3)  ; (1)(1)  ; (3,7,15)(3,7)  ; (3,5,7)(3,5) 42706 ; 7,5 70226\n",
        "\n",
        "x = torch.randn((2, in_ch, 64, 64), device=device)\n",
        "# out, _ = model(x)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "psJQyxGNkOlE"
      },
      "outputs": [],
      "source": [
        "# @title vqvae from CompVis\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/models/autoencoder.py\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from contextlib import contextmanager\n",
        "\n",
        "from ldm.modules.diffusionmodules.model import Encoder, Decoder\n",
        "from ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "class VQModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 n_embed,\n",
        "                 embed_dim,\n",
        "                 remap=None,\n",
        "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "                 use_ema=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25, remap=remap, sane_index_shape=sane_index_shape)\n",
        "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "\n",
        "        self.use_ema = use_ema\n",
        "        if self.use_ema:\n",
        "            self.model_ema = LitEma(self)\n",
        "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
        "\n",
        "    @contextmanager\n",
        "    def ema_scope(self, context=None):\n",
        "        if self.use_ema:\n",
        "            self.model_ema.store(self.parameters())\n",
        "            self.model_ema.copy_to(self)\n",
        "            if context is not None:\n",
        "                print(f\"{context}: Switched to EMA weights\")\n",
        "        try:\n",
        "            yield None\n",
        "        finally:\n",
        "            if self.use_ema:\n",
        "                self.model_ema.restore(self.parameters())\n",
        "                if context is not None:\n",
        "                    print(f\"{context}: Restored training weights\")\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        if self.use_ema:\n",
        "            self.model_ema(self)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        # https://github.com/pytorch/pytorch/issues/37142\n",
        "        # try not to fool the heuristics\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # autoencode\n",
        "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
        "                                            predicted_indices=ind)\n",
        "\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # discriminator\n",
        "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        log_dict = self._validation_step(batch, batch_idx)\n",
        "        with self.ema_scope():\n",
        "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
        "        return log_dict\n",
        "\n",
        "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
        "                                        self.global_step,\n",
        "                                        last_layer=self.get_last_layer(),\n",
        "                                        split=\"val\"+suffix,\n",
        "                                        predicted_indices=ind\n",
        "                                        )\n",
        "\n",
        "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
        "                                            self.global_step,\n",
        "                                            last_layer=self.get_last_layer(),\n",
        "                                            split=\"val\"+suffix,\n",
        "                                            predicted_indices=ind\n",
        "                                            )\n",
        "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
        "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "\n",
        "class VQModelInterface(VQModel):\n",
        "    def __init__(self, embed_dim, *args, **kwargs):\n",
        "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, h, force_not_quantize=False):\n",
        "        # also go through quantization layer\n",
        "        if not force_not_quantize:\n",
        "            quant, emb_loss, info = self.quantize(h)\n",
        "        else:\n",
        "            quant = h\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "\n",
        "class AutoencoderKL(pl.LightningModule):\n",
        "    def __init__(self, ddconfig, lossconfig, embed_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        assert ddconfig[\"double_z\"]\n",
        "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, sample_posterior=True):\n",
        "        posterior = self.encode(input)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        return dec, posterior\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # train encoder+decoder+logvar\n",
        "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # train the discriminator\n",
        "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "\n",
        "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "\n",
        "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XgDIoz8Jm8B6"
      },
      "outputs": [],
      "source": [
        "# @title encoder from CompVis\n",
        "\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/cin-ldm-vq-f8.yaml\n",
        "# ddconfig:\n",
        "#     double_z: false\n",
        "#     z_channels: 4\n",
        "#     resolution: 256\n",
        "#     in_channels: 3\n",
        "#     out_ch: 3\n",
        "#     ch: 128\n",
        "#     ch_mult: 1,2,3,4\n",
        "#     num_res_blocks: 2\n",
        "#     attn_resolutions: 32\n",
        "\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/ffhq-ldm-vq-4.yaml\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/celebahq-ldm-vq-4.yaml\n",
        "# embed_dim: 3\n",
        "# n_embed: 8192 = 2^13 ~ 16^3\n",
        "# ddconfig:\n",
        "#   double_z: false\n",
        "#   z_channels: 3\n",
        "#   resolution: 256\n",
        "#   in_channels: 3\n",
        "#   out_ch: 3\n",
        "#   ch: 128\n",
        "#   ch_mult: 1,2,4\n",
        "#   num_res_blocks: 2\n",
        "#   attn_resolutions: []\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch=128, out_ch=3, ch_mult=(1,2,3,4), num_res_blocks=2,\n",
        "                 attn_resolutions=32, dropout=0.0, resamp_with_conv=True, in_channels=3,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult) # 1,1,2,3,4\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level] # 128 * 1,1,2,3,4\n",
        "            block_out = ch*ch_mult[i_level] # 128 * 1,2,3,4\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1: # downsample at all except last\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "encoder:\n",
        "convin k3s1\n",
        "res attn (),\n",
        "norm act convout k3s1\n",
        "\n",
        "self.conv_in = torch.nn.Conv2d(in_channels, self.ch, 3, 1, padding=3//2)\n",
        "\n",
        "res res down res res down\n",
        "res att res\n",
        "\n",
        "self.conv_out = nn.Sequential(\n",
        "    nn.GroupNorm(32, ch), nn.SiLU(), torch.nn.Conv2d(block_in, z_channels, 3, 1, padding=3//2)\n",
        ")\n",
        "\n",
        "me: down res res down res res\n",
        "down res att down res att = lvl lvl\n",
        "\n",
        "decoder:\n",
        "conv_in\n",
        "res att res\n",
        "res res up res res up\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nWb4L-uhmL4u"
      },
      "outputs": [],
      "source": [
        "# @title CompVis model.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L368\n",
        "\n",
        "# pytorch_diffusion + derived encoder decoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.modules.attention import LinearAttention\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
        "    From Fairseq.\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in tensor2tensor, but differs slightly\n",
        "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    # swish\n",
        "    return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def Normalize(in_channels, num_groups=32):\n",
        "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0,1,0,1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if temb_channels > 0:\n",
        "            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        if temb is not None:\n",
        "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class LinAttnBlock(LinearAttention):\n",
        "    \"\"\"to match AttnBlock usage\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = q.reshape(b,c,h*w)\n",
        "        q = q.permute(0,2,1)   # b,hw,c\n",
        "        k = k.reshape(b,c,h*w) # b,c,hw\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b,c,h*w)\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
        "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
        "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
        "    if attn_type == \"vanilla\":\n",
        "        return AttnBlock(in_channels)\n",
        "    elif attn_type == \"none\":\n",
        "        return nn.Identity(in_channels)\n",
        "    else:\n",
        "        return LinAttnBlock(in_channels)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.use_timestep = use_timestep\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            self.temb = nn.Module()\n",
        "            self.temb.dense = nn.ModuleList([\n",
        "                torch.nn.Linear(self.ch,\n",
        "                                self.temb_ch),\n",
        "                torch.nn.Linear(self.temb_ch,\n",
        "                                self.temb_ch),\n",
        "            ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, t=None, context=None):\n",
        "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
        "        if context is not None:\n",
        "            # assume aligned context, cat along channel axis\n",
        "            x = torch.cat((x, context), dim=1)\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            assert t is not None\n",
        "            temb = get_timestep_embedding(t, self.ch)\n",
        "            temb = self.temb.dense[0](temb)\n",
        "            temb = nonlinearity(temb)\n",
        "            temb = self.temb.dense[1](temb)\n",
        "        else:\n",
        "            temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.conv_out.weight\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
        "                 attn_type=\"vanilla\", **ignorekwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "        self.give_pre_end = give_pre_end\n",
        "        self.tanh_out = tanh_out\n",
        "\n",
        "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
        "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
        "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
        "        print(\"Working with z of shape {} = {} dimensions.\".format(self.z_shape, np.prod(self.z_shape)))\n",
        "\n",
        "        # z to block_in\n",
        "        self.conv_in = torch.nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #assert z.shape[1:] == self.z_shape[1:]\n",
        "        self.last_z_shape = z.shape\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # z to block_in\n",
        "        h = self.conv_in(z)\n",
        "\n",
        "        # middle\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](h, temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        if self.give_pre_end:\n",
        "            return h\n",
        "\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        if self.tanh_out:\n",
        "            h = torch.tanh(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
        "                                     ResnetBlock(in_channels=in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=2 * in_channels, out_channels=4 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=4 * in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
        "                                     Upsample(in_channels, with_conv=True)])\n",
        "        # end\n",
        "        self.norm_out = Normalize(in_channels)\n",
        "        self.conv_out = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.model):\n",
        "            if i in [1,2,3]:\n",
        "                x = layer(x, None)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        h = self.norm_out(x)\n",
        "        h = nonlinearity(h)\n",
        "        x = self.conv_out(h)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsampleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
        "                 ch_mult=(2,2), dropout=0.0):\n",
        "        super().__init__()\n",
        "        # upsampling\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        block_in = in_channels\n",
        "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            res_block = []\n",
        "            block_out = ch * ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                res_block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "            self.res_blocks.append(nn.ModuleList(res_block))\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                self.upsample_blocks.append(Upsample(block_in, True))\n",
        "                curr_res = curr_res * 2\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # upsampling\n",
        "        h = x\n",
        "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                h = self.res_blocks[i_level][i_block](h, None)\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                h = self.upsample_blocks[k](h)\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class LatentRescaler(nn.Module):\n",
        "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
        "        super().__init__()\n",
        "        # residual block, interpolate, residual block\n",
        "        self.factor = factor\n",
        "        self.conv_in = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "        self.attn = AttnBlock(mid_channels)\n",
        "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "\n",
        "        self.conv_out = nn.Conv2d(mid_channels, out_channels, kernel_size=1,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "        for block in self.res_block1:\n",
        "            x = block(x, None)\n",
        "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
        "        x = self.attn(x)\n",
        "        for block in self.res_block2:\n",
        "            x = block(x, None)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
        "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        intermediate_chn = ch * ch_mult[-1]\n",
        "        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
        "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
        "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
        "                               out_ch=None)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
        "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.rescaler(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleDecoder(nn.Module):\n",
        "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
        "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        tmp_chn = z_channels*ch_mult[-1]\n",
        "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
        "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
        "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
        "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampler(nn.Module):\n",
        "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
        "        super().__init__()\n",
        "        assert out_size >= in_size\n",
        "        num_blocks = int(np.log2(out_size//in_size))+1\n",
        "        factor_up = 1.+ (out_size % in_size)\n",
        "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
        "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
        "                                       out_channels=in_channels)\n",
        "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
        "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
        "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Resize(nn.Module):\n",
        "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
        "        super().__init__()\n",
        "        self.with_conv = learned\n",
        "        self.mode = mode\n",
        "        if self.with_conv:\n",
        "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
        "            raise NotImplementedError()\n",
        "            assert in_channels is not None\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, scale_factor=1.0):\n",
        "        if scale_factor==1.0:\n",
        "            return x\n",
        "        else:\n",
        "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
        "        return x\n",
        "\n",
        "class FirstStagePostProcessor(nn.Module):\n",
        "    def __init__(self, ch_mult:list, in_channels,\n",
        "                 pretrained_model:nn.Module=None,\n",
        "                 reshape=False,\n",
        "                 n_channels=None,\n",
        "                 dropout=0.,\n",
        "                 pretrained_config=None):\n",
        "        super().__init__()\n",
        "        if pretrained_config is None:\n",
        "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.pretrained_model = pretrained_model\n",
        "        else:\n",
        "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.instantiate_pretrained(pretrained_config)\n",
        "        self.do_reshape = reshape\n",
        "        if n_channels is None:\n",
        "            n_channels = self.pretrained_model.encoder.ch\n",
        "\n",
        "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
        "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3, stride=1,padding=1)\n",
        "        blocks = []\n",
        "        downs = []\n",
        "        ch_in = n_channels\n",
        "        for m in ch_mult:\n",
        "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
        "            ch_in = m * n_channels\n",
        "            downs.append(Downsample(ch_in, with_conv=False))\n",
        "        self.model = nn.ModuleList(blocks)\n",
        "        self.downsampler = nn.ModuleList(downs)\n",
        "\n",
        "\n",
        "    def instantiate_pretrained(self, config):\n",
        "        model = instantiate_from_config(config)\n",
        "        self.pretrained_model = model.eval()\n",
        "        # self.pretrained_model.train = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_with_pretrained(self,x):\n",
        "        c = self.pretrained_model.encode(x)\n",
        "        if isinstance(c, DiagonalGaussianDistribution):\n",
        "            c = c.mode()\n",
        "        return  c\n",
        "\n",
        "    def forward(self,x):\n",
        "        z_fs = self.encode_with_pretrained(x)\n",
        "        z = self.proj_norm(z_fs)\n",
        "        z = self.proj(z)\n",
        "        z = nonlinearity(z)\n",
        "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
        "            z = submodel(z,temb=None)\n",
        "            z = downmodel(z)\n",
        "        if self.do_reshape:\n",
        "            z = rearrange(z,'b c h w -> b (h w) c')\n",
        "        return z\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5bYiW-RnBdAi"
      },
      "outputs": [],
      "source": [
        "# @title quantizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# airalcorn2\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "        self.decay = decay\n",
        "        # Small constant to avoid numerical instability in embedding updates.\n",
        "        self.epsilon = 1e-5\n",
        "\n",
        "        # Dictionary embeddings.\n",
        "        limit = 3 ** 0.5\n",
        "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_( -limit, limit)\n",
        "        if use_ema: self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else: self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        # Exponential moving average of the cluster counts.\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
        "        distances = ((flat_x ** 2).sum(1, keepdim=True) - 2 * flat_x @ self.e_i_ts + (self.e_i_ts ** 2).sum(0, keepdim=True))\n",
        "        encoding_indices = distances.argmin(1)\n",
        "        quantized_x = F.embedding(encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        # See second term of Equation (3).\n",
        "        if not self.use_ema: dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else: dictionary_loss = None\n",
        "\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
        "                # Cluster counts.\n",
        "                encoding_one_hots = F.one_hot(encoding_indices, self.num_embeddings).type(flat_x.dtype)\n",
        "                n_i_ts = encoding_one_hots.sum(0)\n",
        "                # Updated exponential moving average of the cluster counts.\n",
        "                # See Equation (6).\n",
        "                self.N_i_ts(n_i_ts)\n",
        "\n",
        "                # Exponential moving average of the embeddings. See Equation (7).\n",
        "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
        "                self.m_i_ts(embed_sums)\n",
        "\n",
        "                # This is kind of weird.\n",
        "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
        "                # and Equation (8).\n",
        "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
        "                N_i_ts_stable = ((self.N_i_ts.average + self.epsilon) / (N_i_ts_sum + self.num_embeddings * self.epsilon) * N_i_ts_sum)\n",
        "\n",
        "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
        "\n",
        "        return (quantized_x, dictionary_loss, commitment_loss, encoding_indices.view(x.shape[0], -1),)\n",
        "\n",
        "\n",
        "# rosinality\n",
        "class Quantize(nn.Module):\n",
        "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.n_embed = n_embed\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        embed = torch.randn(dim, n_embed)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, input):\n",
        "        flatten = input.reshape(-1, self.dim)\n",
        "        dist = (flatten.pow(2).sum(1, keepdim=True) - 2 * flatten @ self.embed + self.embed.pow(2).sum(0, keepdim=True))\n",
        "        _, embed_ind = (-dist).max(1)\n",
        "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
        "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
        "        quantize = self.embed_code(embed_ind)\n",
        "\n",
        "        if self.training:\n",
        "            embed_onehot_sum = embed_onehot.sum(0)\n",
        "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
        "            dist_fn.all_reduce(embed_onehot_sum)\n",
        "            dist_fn.all_reduce(embed_sum)\n",
        "            self.cluster_size.data.mul_(self.decay).add_(embed_onehot_sum, alpha=1 - self.decay)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = ((self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n)\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "        diff = (quantize.detach() - input).pow(2).mean()\n",
        "        quantize = input + (quantize - input).detach()\n",
        "        return quantize, diff, embed_ind\n",
        "\n",
        "    def embed_code(self, embed_id):\n",
        "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
        "\n",
        "# CompVis\n",
        "from einops import rearrange\n",
        "class VectorQuantizer2(nn.Module): # https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py#L213\n",
        "    def __init__(self, n_e, e_dim, beta, sane_index_shape=False): # sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + torch.sum(self.embedding.weight**2, dim=1) - 2 * torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        return z_q, loss, min_encoding_indices\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "\n",
        "emb_dim, num_emb = 4,20\n",
        "# x = torch.randn(2, 3, 4)\n",
        "# x = torch.randn(2, 3, 4, 4)\n",
        "# vq = VectorQuantizer(emb_dim, num_emb, use_ema, decay)\n",
        "# vq = Quantize(emb_dim, num_emb, decay=0.99, eps=1e-5)\n",
        "vq = VectorQuantizer2(num_emb, emb_dim, beta=0.5, sane_index_shape=False) # CompVis\n",
        "out = vq(x)\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6gGjNCKog8za"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5)\n",
        "        shift = torch.tan(offset / half_l)\n",
        "        return torch.tanh(z + shift) * half_l - offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "print(fsq.codebook)\n",
        "\n",
        "batch_size, seq_len = 1, 1\n",
        "x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "lact = fsq.codes_to_indexes(la)\n",
        "print(lact)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bhCqoI0_fX1B"
      },
      "outputs": [],
      "source": [
        "# @title tried no round, LogitNormalCDF\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def logit(x): return torch.log(x/(1-x)) # x in (0,1)\n",
        "def LogitNormalCDF(x, mu=0, std=.5): # _/- for std<1.8; /-/ for std>1.8\n",
        "    cdf = 1/2 * (1 + torch.erf((logit(x)-mu)/(2**.5*std)))\n",
        "    return cdf\n",
        "\n",
        "# class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "#     def __init__(self, levels):\n",
        "#         super().__init__()\n",
        "#         self.eps = eps\n",
        "#         self.levels = torch.tensor(levels, device=device)\n",
        "#         # level in levels\n",
        "\n",
        "    # linear, normal,\n",
        "    # center = LogitNormalCDF(torch.linspace(0,1,level), mu=0, std=3)\n",
        "    # def forward(self, z):\n",
        "    #     z = F.sigmoid(z)\n",
        "    #     ind = torch.argmin((z-center).abs())\n",
        "\n",
        "\n",
        "    # threshold = LogitNormalCDF(torch.linspace(0,1,level+1), mu=0, std=3)[1:-1]\n",
        "\n",
        "    # def forward(self, z):\n",
        "    #     z = F.sigmoid(z)\n",
        "    #     center[ind]\n",
        "\n",
        "\n",
        "# linear, normal,\n",
        "\n",
        "center = [LogitNormalCDF(torch.linspace(0,1,level), mu=0, std=3) for level in levels]\n",
        "# print(center)\n",
        "# def forward(self, z):\n",
        "z = torch.linspace(-2,2,7).repeat(3,1).T\n",
        "\n",
        "z = F.sigmoid(z)\n",
        "# ind = [torch.argmin((z-c).abs()) for c in center]\n",
        "# print(ind)\n",
        "\n",
        "threshold = [LogitNormalCDF(torch.linspace(0,1,level+1), mu=0, std=1)[1:-1] for level in levels]\n",
        "print(threshold)\n",
        "\n",
        "def get_vjp(v):\n",
        "    return torch.autograd.grad(y, x, v)\n",
        "out = torch.vmap(get_vjp)(I_N)\n",
        "\n",
        "\n",
        "# def forward(self, z):\n",
        "#     z = F.sigmoid(z)\n",
        "#     center[ind]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "axZFsNiThSmT"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains vector_quantize_pytorch.py GroupedResidualVQ\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/vector_quantize_pytorch.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "from math import ceil\n",
        "from functools import partial, cache\n",
        "from itertools import zip_longest\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import Module, ModuleList\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from vector_quantize_pytorch.vector_quantize_pytorch import VectorQuantize\n",
        "\n",
        "from einops import rearrange, repeat, reduce, pack, unpack\n",
        "\n",
        "from einx import get_at\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def first(it):\n",
        "    return it[0]\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cast_tuple(t, length = 1):\n",
        "    return t if isinstance(t, tuple) else ((t,) * length)\n",
        "\n",
        "def unique(arr):\n",
        "    return list({*arr})\n",
        "\n",
        "def round_up_multiple(num, mult):\n",
        "    return ceil(num / mult) * mult\n",
        "\n",
        "# distributed helpers\n",
        "\n",
        "def is_distributed():\n",
        "    return dist.is_initialized() and dist.get_world_size() > 1\n",
        "\n",
        "def get_maybe_sync_seed(device, max_size = 10_000):\n",
        "    rand_int = torch.randint(0, max_size, (), device = device)\n",
        "\n",
        "    if is_distributed():\n",
        "        dist.all_reduce(rand_int)\n",
        "\n",
        "    return rand_int.item()\n",
        "\n",
        "# the mlp for generating the neural implicit codebook\n",
        "# from Huijben et al. https://arxiv.org/abs/2401.14732\n",
        "\n",
        "class MLP(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_hidden = None,\n",
        "        depth = 4,             # they used 4 layers in the paper\n",
        "        l2norm_output = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim_hidden = default(dim_hidden, dim)\n",
        "\n",
        "        self.proj_in = nn.Linear(2 * dim, dim)\n",
        "\n",
        "        layers = ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            layers.append(nn.Sequential(\n",
        "                nn.Linear(dim, dim_hidden),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(dim_hidden, dim)\n",
        "            ))\n",
        "\n",
        "        self.layers = layers\n",
        "        self.l2norm_output = l2norm_output\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        codes,\n",
        "        *,\n",
        "        condition\n",
        "    ):\n",
        "        one_headed = codes.ndim == 2\n",
        "\n",
        "        if one_headed:\n",
        "            codes = rearrange(codes, 'c d -> 1 c d')\n",
        "\n",
        "        heads, num_codes, batch, seq_len = codes.shape[0], codes.shape[-2], condition.shape[0], condition.shape[-2]\n",
        "\n",
        "        codes = repeat(codes, 'h c d -> h b n c d', n = seq_len, b = batch)\n",
        "        condition = repeat(condition, 'b n d -> h b n c d', c = num_codes, h = heads)\n",
        "\n",
        "        x = torch.cat((condition, codes), dim = -1)\n",
        "        x = self.proj_in(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x) + x\n",
        "\n",
        "        if self.l2norm_output:\n",
        "            x = F.normalize(x, dim = -1)\n",
        "\n",
        "        if not one_headed:\n",
        "            return x\n",
        "\n",
        "        return rearrange(x, '1 ... -> ...')\n",
        "\n",
        "# main class\n",
        "\n",
        "class ResidualVQ(Module):\n",
        "    \"\"\" Follows Algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        num_quantizers: int | None = None,\n",
        "        codebook_size: int | tuple[int, ...],\n",
        "        codebook_dim = None,\n",
        "        shared_codebook = False,\n",
        "        heads = 1,\n",
        "        quantize_dropout = False,\n",
        "        quantize_dropout_cutoff_index = 0,\n",
        "        quantize_dropout_multiple_of = 1,\n",
        "        accept_image_fmap = False,\n",
        "        implicit_neural_codebook = False, # QINCo from https://arxiv.org/abs/2401.14732\n",
        "        mlp_kwargs: dict = dict(),\n",
        "        **vq_kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert heads == 1, 'residual vq is not compatible with multi-headed codes'\n",
        "        assert exists(num_quantizers) or isinstance(codebook_size, tuple)\n",
        "\n",
        "        codebook_dim = default(codebook_dim, dim)\n",
        "        codebook_input_dim = codebook_dim * heads\n",
        "\n",
        "        requires_projection = codebook_input_dim != dim\n",
        "        self.project_in = nn.Linear(dim, codebook_input_dim) if requires_projection else nn.Identity()\n",
        "        self.project_out = nn.Linear(codebook_input_dim, dim) if requires_projection else nn.Identity()\n",
        "        self.has_projections = requires_projection\n",
        "\n",
        "        self.accept_image_fmap = accept_image_fmap\n",
        "\n",
        "        self.implicit_neural_codebook = implicit_neural_codebook\n",
        "\n",
        "        if implicit_neural_codebook:\n",
        "            vq_kwargs.update(\n",
        "                learnable_codebook = True,\n",
        "                ema_update = False\n",
        "            )\n",
        "\n",
        "        if shared_codebook:\n",
        "            vq_kwargs.update(\n",
        "                manual_ema_update = True,\n",
        "                manual_in_place_optimizer_update = True\n",
        "            )\n",
        "\n",
        "        # take care of maybe different codebook sizes across depth\n",
        "\n",
        "        codebook_sizes = cast_tuple(codebook_size, num_quantizers)\n",
        "\n",
        "        num_quantizers = default(num_quantizers, len(codebook_sizes))\n",
        "        assert len(codebook_sizes) == num_quantizers\n",
        "\n",
        "        self.num_quantizers = num_quantizers\n",
        "\n",
        "        self.codebook_sizes = codebook_sizes\n",
        "        self.uniform_codebook_size = len(unique(codebook_sizes)) == 1\n",
        "\n",
        "        # define vq across layers\n",
        "\n",
        "        self.layers = ModuleList([VectorQuantize(dim = codebook_dim, codebook_size = layer_codebook_size, codebook_dim = codebook_dim, accept_image_fmap = accept_image_fmap, **vq_kwargs) for layer_codebook_size in codebook_sizes])\n",
        "\n",
        "        assert all([not vq.has_projections for vq in self.layers])\n",
        "\n",
        "        self.quantize_dropout = quantize_dropout and num_quantizers > 1\n",
        "\n",
        "        assert quantize_dropout_cutoff_index >= 0\n",
        "\n",
        "        self.quantize_dropout_cutoff_index = quantize_dropout_cutoff_index\n",
        "        self.quantize_dropout_multiple_of = quantize_dropout_multiple_of  # encodec paper proposes structured dropout, believe this was set to 4\n",
        "\n",
        "        # setting up the MLPs for implicit neural codebooks\n",
        "\n",
        "        self.mlps = None\n",
        "\n",
        "        if implicit_neural_codebook:\n",
        "            self.mlps = ModuleList([MLP(dim = codebook_dim, l2norm_output = first(self.layers).use_cosine_sim, **mlp_kwargs) for _ in range(num_quantizers - 1)])\n",
        "        else:\n",
        "            self.mlps = (None,) * (num_quantizers - 1)\n",
        "\n",
        "        # sharing codebook logic\n",
        "\n",
        "        self.shared_codebook = shared_codebook\n",
        "\n",
        "        if not shared_codebook:\n",
        "            return\n",
        "\n",
        "        assert self.uniform_codebook_size\n",
        "\n",
        "        first_vq, *rest_vq = self.layers\n",
        "        codebook = first_vq._codebook\n",
        "\n",
        "        for vq in rest_vq:\n",
        "            vq._codebook = codebook\n",
        "\n",
        "    @property\n",
        "    def codebook_size(self):\n",
        "        return self.layers[0].codebook_size\n",
        "\n",
        "    @property\n",
        "    def codebook_dim(self):\n",
        "        return self.layers[0].codebook_dim\n",
        "\n",
        "    @property\n",
        "    def codebooks(self):\n",
        "        codebooks = [layer._codebook.embed for layer in self.layers]\n",
        "\n",
        "        codebooks = tuple(rearrange(codebook, '1 ... -> ...') for codebook in codebooks)\n",
        "\n",
        "        if not self.uniform_codebook_size:\n",
        "            return codebooks\n",
        "\n",
        "        codebooks = torch.stack(codebooks)\n",
        "        return codebooks\n",
        "\n",
        "    def get_codes_from_indices(self, indices):\n",
        "\n",
        "        batch, quantize_dim = indices.shape[0], indices.shape[-1]\n",
        "\n",
        "        # may also receive indices in the shape of 'b h w q' (accept_image_fmap)\n",
        "\n",
        "        indices, ps = pack([indices], 'b * q')\n",
        "\n",
        "        # because of quantize dropout, one can pass in indices that are coarse\n",
        "        # and the network should be able to reconstruct\n",
        "\n",
        "        if quantize_dim < self.num_quantizers:\n",
        "            assert self.quantize_dropout > 0., 'quantize dropout must be greater than 0 if you wish to reconstruct from a signal with less fine quantizations'\n",
        "            indices = F.pad(indices, (0, self.num_quantizers - quantize_dim), value = -1)\n",
        "\n",
        "        # take care of quantizer dropout\n",
        "\n",
        "        mask = indices == -1.\n",
        "        indices = indices.masked_fill(mask, 0) # have it fetch a dummy code to be masked out later\n",
        "\n",
        "        if not self.implicit_neural_codebook and self.uniform_codebook_size:\n",
        "\n",
        "            all_codes = get_at('q [c] d, b n q -> q b n d', self.codebooks, indices)\n",
        "\n",
        "        else:\n",
        "            # else if using implicit neural codebook, or non uniform codebook sizes, codes will need to be derived layer by layer\n",
        "\n",
        "            code_transform_mlps = (None, *self.mlps)\n",
        "\n",
        "            all_codes = []\n",
        "            quantized_out = 0.\n",
        "\n",
        "            for codes, indices, maybe_transform_mlp in zip(self.codebooks, indices.unbind(dim = -1), code_transform_mlps):\n",
        "\n",
        "                if exists(maybe_transform_mlp):\n",
        "                    codes = maybe_transform_mlp(codes, condition = quantized_out)\n",
        "                    layer_codes = get_at('b n [c] d, b n -> b n d', codes, indices)\n",
        "                else:\n",
        "                    layer_codes = get_at('[c] d, b n -> b n d', codes, indices)\n",
        "\n",
        "                all_codes.append(layer_codes)\n",
        "                quantized_out += layer_codes\n",
        "\n",
        "            all_codes = torch.stack(all_codes)\n",
        "\n",
        "        # mask out any codes that were dropout-ed\n",
        "\n",
        "        all_codes = all_codes.masked_fill(rearrange(mask, 'b n q -> q b n 1'), 0.)\n",
        "\n",
        "        # if (accept_image_fmap = True) then return shape (quantize, batch, height, width, dimension)\n",
        "\n",
        "        all_codes, = unpack(all_codes, ps, 'q b * d')\n",
        "\n",
        "        return all_codes\n",
        "\n",
        "    def get_output_from_indices(self, indices):\n",
        "        codes = self.get_codes_from_indices(indices)\n",
        "        codes_summed = reduce(codes, 'q ... -> ...', 'sum')\n",
        "        return self.project_out(codes_summed)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        mask = None,\n",
        "        indices: Tensor | list[Tensor] | None = None,\n",
        "        return_all_codes = False,\n",
        "        sample_codebook_temp = None,\n",
        "        freeze_codebook = False,\n",
        "        rand_quantize_dropout_fixed_seed = None\n",
        "    ):\n",
        "        num_quant, quant_dropout_multiple_of, return_loss, device = self.num_quantizers, self.quantize_dropout_multiple_of, exists(indices), x.device\n",
        "\n",
        "        x = self.project_in(x)\n",
        "\n",
        "        assert not (self.accept_image_fmap and exists(indices))\n",
        "\n",
        "        quantized_out = 0.\n",
        "        residual = x\n",
        "\n",
        "        all_losses = []\n",
        "        all_indices = []\n",
        "\n",
        "        if isinstance(indices, list):\n",
        "            indices = torch.stack(indices)\n",
        "\n",
        "        if return_loss:\n",
        "            assert not torch.any(indices == -1), 'some of the residual vq indices were dropped out. please use indices derived when the module is in eval mode to derive cross entropy loss'\n",
        "            ce_losses = []\n",
        "\n",
        "        should_quantize_dropout = self.training and self.quantize_dropout and not return_loss\n",
        "\n",
        "        # sample a layer index at which to dropout further residual quantization\n",
        "        # also prepare null indices and loss\n",
        "\n",
        "        if should_quantize_dropout:\n",
        "\n",
        "            # check if seed is manually passed in\n",
        "\n",
        "            if not exists(rand_quantize_dropout_fixed_seed):\n",
        "                rand_quantize_dropout_fixed_seed = get_maybe_sync_seed(device)\n",
        "\n",
        "            rand = random.Random(rand_quantize_dropout_fixed_seed)\n",
        "\n",
        "            rand_quantize_dropout_index = rand.randrange(self.quantize_dropout_cutoff_index, num_quant)\n",
        "\n",
        "            if quant_dropout_multiple_of != 1:\n",
        "                rand_quantize_dropout_index = round_up_multiple(rand_quantize_dropout_index + 1, quant_dropout_multiple_of) - 1\n",
        "\n",
        "            null_indices_shape = (x.shape[0], *x.shape[-2:]) if self.accept_image_fmap else tuple(x.shape[:2])\n",
        "            null_indices = torch.full(null_indices_shape, -1., device = device, dtype = torch.long)\n",
        "            null_loss = torch.full((1,), 0., device = device, dtype = x.dtype)\n",
        "\n",
        "        # setup the mlps for implicit neural codebook\n",
        "\n",
        "        maybe_code_transforms = (None,) * len(self.layers)\n",
        "\n",
        "        if self.implicit_neural_codebook:\n",
        "            maybe_code_transforms = (None, *self.mlps)\n",
        "\n",
        "        # save all inputs across layers, for use during expiration at end under shared codebook setting\n",
        "\n",
        "        all_residuals = []\n",
        "\n",
        "        # go through the layers\n",
        "\n",
        "        for quantizer_index, (vq, maybe_mlp) in enumerate(zip(self.layers, maybe_code_transforms)):\n",
        "\n",
        "            if should_quantize_dropout and quantizer_index > rand_quantize_dropout_index:\n",
        "                all_indices.append(null_indices)\n",
        "                all_losses.append(null_loss)\n",
        "                continue\n",
        "\n",
        "            layer_indices = None\n",
        "            if return_loss:\n",
        "                layer_indices = indices[..., quantizer_index]\n",
        "\n",
        "            # setup the transform code function to be passed into VectorQuantize forward\n",
        "\n",
        "            if exists(maybe_mlp):\n",
        "                maybe_mlp = partial(maybe_mlp, condition = quantized_out)\n",
        "\n",
        "            # save for expiration\n",
        "\n",
        "            all_residuals.append(residual)\n",
        "\n",
        "            # vector quantize forward\n",
        "\n",
        "            quantized, *rest = vq(\n",
        "                residual,\n",
        "                mask = mask,\n",
        "                indices = layer_indices,\n",
        "                sample_codebook_temp = sample_codebook_temp,\n",
        "                freeze_codebook = freeze_codebook,\n",
        "                codebook_transform_fn = maybe_mlp\n",
        "            )\n",
        "\n",
        "            residual = residual - quantized.detach()\n",
        "            quantized_out = quantized_out + quantized\n",
        "\n",
        "            if return_loss:\n",
        "                ce_loss = rest[0]\n",
        "                ce_losses.append(ce_loss)\n",
        "                continue\n",
        "\n",
        "            embed_indices, loss = rest\n",
        "\n",
        "            all_indices.append(embed_indices)\n",
        "            all_losses.append(loss)\n",
        "\n",
        "        # if shared codebook, update ema only at end\n",
        "\n",
        "        if self.training and self.shared_codebook:\n",
        "            shared_layer = first(self.layers)\n",
        "            shared_layer._codebook.update_ema()\n",
        "            shared_layer.update_in_place_optimizer()\n",
        "            shared_layer.expire_codes_(torch.cat(all_residuals, dim = -2))\n",
        "\n",
        "        # project out, if needed\n",
        "\n",
        "        quantized_out = self.project_out(quantized_out)\n",
        "\n",
        "        # whether to early return the cross entropy loss\n",
        "\n",
        "        if return_loss:\n",
        "            return quantized_out, sum(ce_losses)\n",
        "\n",
        "        # stack all losses and indices\n",
        "\n",
        "        all_losses, all_indices = map(partial(torch.stack, dim = -1), (all_losses, all_indices))\n",
        "\n",
        "        ret = (quantized_out, all_indices, all_losses)\n",
        "\n",
        "        if return_all_codes:\n",
        "            # whether to return all codes from all codebooks across layers\n",
        "            all_codes = self.get_codes_from_indices(all_indices)\n",
        "\n",
        "            # will return all codes in shape (quantizer, batch, sequence length, codebook dimension)\n",
        "            ret = (*ret, all_codes)\n",
        "\n",
        "        return ret\n",
        "\n",
        "# grouped residual vq\n",
        "\n",
        "class GroupedResidualVQ(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        groups = 1,\n",
        "        accept_image_fmap = False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.groups = groups\n",
        "        assert (dim % groups) == 0\n",
        "        dim_per_group = dim // groups\n",
        "\n",
        "        self.accept_image_fmap = accept_image_fmap\n",
        "\n",
        "        self.rvqs = ModuleList([])\n",
        "\n",
        "        for _ in range(groups):\n",
        "            self.rvqs.append(ResidualVQ(\n",
        "                dim = dim_per_group,\n",
        "                accept_image_fmap = accept_image_fmap,\n",
        "                **kwargs\n",
        "            ))\n",
        "\n",
        "    @property\n",
        "    def codebooks(self):\n",
        "        return torch.stack(tuple(rvq.codebooks for rvq in self.rvqs))\n",
        "\n",
        "    @property\n",
        "    def split_dim(self):\n",
        "        return 1 if self.accept_image_fmap else -1\n",
        "\n",
        "    def get_codes_from_indices(self, indices):\n",
        "        codes = tuple(rvq.get_codes_from_indices(chunk_indices) for rvq, chunk_indices in zip(self.rvqs, indices))\n",
        "        return torch.stack(codes)\n",
        "\n",
        "    def get_output_from_indices(self, indices):\n",
        "        outputs = tuple(rvq.get_output_from_indices(chunk_indices) for rvq, chunk_indices in zip(self.rvqs, indices))\n",
        "        return torch.cat(outputs, dim = self.split_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        indices = None,\n",
        "        return_all_codes = False,\n",
        "        sample_codebook_temp = None,\n",
        "        freeze_codebook = False,\n",
        "        mask = None,\n",
        "    ):\n",
        "        shape, split_dim, device = x.shape, self.split_dim, x.device\n",
        "        assert shape[split_dim] == self.dim\n",
        "\n",
        "        # split the feature dimension into groups\n",
        "\n",
        "        x = x.chunk(self.groups, dim = split_dim)\n",
        "\n",
        "        indices = default(indices, tuple())\n",
        "        return_ce_loss = len(indices) > 0\n",
        "        assert len(indices) == 0 or len(indices) == self.groups\n",
        "\n",
        "        forward_kwargs = dict(\n",
        "            return_all_codes = return_all_codes,\n",
        "            sample_codebook_temp = sample_codebook_temp,\n",
        "            mask = mask,\n",
        "            freeze_codebook = freeze_codebook,\n",
        "            rand_quantize_dropout_fixed_seed = get_maybe_sync_seed(device) if self.training else None\n",
        "        )\n",
        "\n",
        "        # invoke residual vq on each group\n",
        "\n",
        "        out = tuple(rvq(chunk, indices = chunk_indices, **forward_kwargs) for rvq, chunk, chunk_indices in zip_longest(self.rvqs, x, indices))\n",
        "        out = tuple(zip(*out))\n",
        "\n",
        "        # if returning cross entropy loss to rvq codebooks\n",
        "\n",
        "        if return_ce_loss:\n",
        "            quantized, ce_losses = out\n",
        "            return torch.cat(quantized, dim = split_dim), sum(ce_losses)\n",
        "\n",
        "        # otherwise, get all the zipped outputs and combine them\n",
        "\n",
        "        quantized, all_indices, commit_losses, *maybe_all_codes = out\n",
        "\n",
        "        quantized = torch.cat(quantized, dim = split_dim)\n",
        "        all_indices = torch.stack(all_indices)\n",
        "        commit_losses = torch.stack(commit_losses)\n",
        "\n",
        "        ret = (quantized, all_indices, commit_losses, *maybe_all_codes)\n",
        "        return ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dr2bajYVyjbN"
      },
      "outputs": [],
      "source": [
        "# @title CompVis vqvae\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VectorQuantizer2(nn.Module): # https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py#L213\n",
        "    def __init__(self, n_e, e_dim, beta, sane_index_shape=False): # sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        return z_q, loss, min_encoding_indices\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "\n",
        "\n",
        "# class VQModel(pl.LightningModule):\n",
        "class VQModel(nn.Module):\n",
        "    def __init__(self, n_embed, embed_dim,):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        z_channels = 4 # 4?\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25)\n",
        "        self.quant_conv = torch.nn.Conv2d(z_channels, embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, z_channels, 1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cS-HV67C8S_m"
      },
      "outputs": [],
      "source": [
        "# @title airalcorn2 vqvae\n",
        "# https://github.com/airalcorn2/vqvae-pytorch/blob/master/vqvae.py\n",
        "# Ported from: https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb.\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        layers = []\n",
        "        for i in range(num_residual_layers):\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_hiddens,\n",
        "                        out_channels=num_residual_hiddens,\n",
        "                        kernel_size=3,\n",
        "                        padding=1,\n",
        "                    ),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_residual_hiddens,\n",
        "                        out_channels=num_hiddens,\n",
        "                        kernel_size=1,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for layer in self.layers:\n",
        "            h = h + layer(h)\n",
        "\n",
        "        # ResNet V1-style.\n",
        "        return torch.relu(h)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        # The last ReLU from the Sonnet example is omitted because ResidualStack starts\n",
        "        # off with a ReLU.\n",
        "        conv = nn.Sequential()\n",
        "        for downsampling_layer in range(num_downsampling_layers):\n",
        "            if downsampling_layer == 0:\n",
        "                out_channels = num_hiddens // 2\n",
        "            elif downsampling_layer == 1:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, num_hiddens)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            conv.add_module(\n",
        "                f\"down{downsampling_layer}\",\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            conv.add_module(f\"relu{downsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        conv.add_module(\n",
        "            \"final_conv\",\n",
        "            nn.Conv2d(\n",
        "                in_channels=num_hiddens,\n",
        "                out_channels=num_hiddens,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "        )\n",
        "        self.conv = conv\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        return self.residual_stack(h)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        num_hiddens,\n",
        "        num_upsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=embedding_dim,\n",
        "            out_channels=num_hiddens,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "        upconv = nn.Sequential()\n",
        "        for upsampling_layer in range(num_upsampling_layers):\n",
        "            if upsampling_layer < num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            elif upsampling_layer == num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens // 2)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, 3)\n",
        "\n",
        "            upconv.add_module(\n",
        "                f\"up{upsampling_layer}\",\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            if upsampling_layer < num_upsampling_layers - 1:\n",
        "                upconv.add_module(f\"relu{upsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        self.upconv = upconv\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h = self.residual_stack(h)\n",
        "        x_recon = self.upconv(h)\n",
        "        return x_recon\n",
        "\n",
        "\n",
        "class SonnetExponentialMovingAverage(nn.Module):\n",
        "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
        "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
        "    # of \"Neural Discrete Representation Learning\".\n",
        "    def __init__(self, decay, shape):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.counter = 0\n",
        "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
        "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
        "\n",
        "    def update(self, value):\n",
        "        self.counter += 1\n",
        "        with torch.no_grad():\n",
        "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
        "            self.average = self.hidden / (1 - self.decay ** self.counter)\n",
        "\n",
        "    def __call__(self, value):\n",
        "        self.update(value)\n",
        "        return self.average\n",
        "\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay, epsilon):\n",
        "        super().__init__()\n",
        "        # See Section 3 of \"Neural Discrete Representation Learning\" and:\n",
        "        # https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L142.\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "        # Weight for the exponential moving average.\n",
        "        self.decay = decay\n",
        "        # Small constant to avoid numerical instability in embedding updates.\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Dictionary embeddings.\n",
        "        limit = 3 ** 0.5\n",
        "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_( -limit, limit)\n",
        "        if use_ema: self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else: self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        # Exponential moving average of the cluster counts.\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
        "        distances = ((flat_x ** 2).sum(1, keepdim=True) - 2 * flat_x @ self.e_i_ts + (self.e_i_ts ** 2).sum(0, keepdim=True))\n",
        "        encoding_indices = distances.argmin(1)\n",
        "        quantized_x = F.embedding(encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        # See second term of Equation (3).\n",
        "        if not self.use_ema:\n",
        "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else:\n",
        "            dictionary_loss = None\n",
        "\n",
        "        # See third term of Equation (3).\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        # Straight-through gradient. See Section 3.2.\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
        "                # Cluster counts.\n",
        "                encoding_one_hots = F.one_hot(encoding_indices, self.num_embeddings).type(flat_x.dtype)\n",
        "                n_i_ts = encoding_one_hots.sum(0)\n",
        "                # Updated exponential moving average of the cluster counts.\n",
        "                # See Equation (6).\n",
        "                self.N_i_ts(n_i_ts)\n",
        "\n",
        "                # Exponential moving average of the embeddings. See Equation (7).\n",
        "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
        "                self.m_i_ts(embed_sums)\n",
        "\n",
        "                # This is kind of weird.\n",
        "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
        "                # and Equation (8).\n",
        "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
        "                N_i_ts_stable = ((self.N_i_ts.average + self.epsilon) / (N_i_ts_sum + self.num_embeddings * self.epsilon) * N_i_ts_sum)\n",
        "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
        "        return (quantized_x, dictionary_loss, commitment_loss, encoding_indices.view(x.shape[0], -1),)\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "        embedding_dim,\n",
        "        num_embeddings,\n",
        "        use_ema,\n",
        "        decay,\n",
        "        epsilon,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens,\n",
        "        )\n",
        "        self.pre_vq_conv = nn.Conv2d(\n",
        "            in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1\n",
        "        )\n",
        "        self.vq = VectorQuantizer(\n",
        "            embedding_dim, num_embeddings, use_ema, decay, epsilon\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            embedding_dim,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens,\n",
        "        )\n",
        "\n",
        "    def quantize(self, x):\n",
        "        z = self.pre_vq_conv(self.encoder(x))\n",
        "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = self.vq(z)\n",
        "        return (z_quantized, dictionary_loss, commitment_loss, encoding_indices)\n",
        "\n",
        "    def forward(self, x):\n",
        "        (z_quantized, dictionary_loss, commitment_loss, _) = self.quantize(x)\n",
        "        x_recon = self.decoder(z_quantized)\n",
        "        return {\n",
        "            \"dictionary_loss\": dictionary_loss,\n",
        "            \"commitment_loss\": commitment_loss,\n",
        "            \"x_recon\": x_recon,\n",
        "        }\n",
        "\n",
        "# https://github.com/airalcorn2/vqvae-pytorch/blob/master/train_vqvae.py\n",
        "        # out = model(imgs)\n",
        "        # recon_error = criterion(out[\"x_recon\"], imgs) / train_data_variance\n",
        "        # total_recon_error += recon_error.item()\n",
        "        # loss = recon_error + beta * out[\"commitment_loss\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "15v0h5A74X2Z"
      },
      "outputs": [],
      "source": [
        "# @title rosinality from sonet\n",
        "# https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import distributed as dist_fn\n",
        "\n",
        "\n",
        "# Copyright 2018 The Sonnet Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Borrowed from https://github.com/deepmind/sonnet and ported it to PyTorch\n",
        "\n",
        "\n",
        "class Quantize(nn.Module):\n",
        "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.n_embed = n_embed\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        embed = torch.randn(dim, n_embed)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, input):\n",
        "        flatten = input.reshape(-1, self.dim)\n",
        "        dist = (flatten.pow(2).sum(1, keepdim=True) - 2 * flatten @ self.embed + self.embed.pow(2).sum(0, keepdim=True))\n",
        "        _, embed_ind = (-dist).max(1)\n",
        "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
        "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
        "        quantize = self.embed_code(embed_ind)\n",
        "\n",
        "        if self.training:\n",
        "            embed_onehot_sum = embed_onehot.sum(0)\n",
        "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
        "            dist_fn.all_reduce(embed_onehot_sum)\n",
        "            dist_fn.all_reduce(embed_sum)\n",
        "            self.cluster_size.data.mul_(self.decay).add_(embed_onehot_sum, alpha=1 - self.decay)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = ((self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n)\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "        diff = (quantize.detach() - input).pow(2).mean()\n",
        "        quantize = input + (quantize - input).detach()\n",
        "        return quantize, diff, embed_ind\n",
        "\n",
        "    def embed_code(self, embed_id):\n",
        "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channel, channel, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel, in_channel, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv(input)\n",
        "        out += input\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channel, channel, n_res_block, n_res_channel, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        if stride == 4:\n",
        "            blocks = [\n",
        "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 2, channel, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel, channel, 3, padding=1),\n",
        "            ]\n",
        "\n",
        "        elif stride == 2:\n",
        "            blocks = [\n",
        "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 2, channel, 3, padding=1),\n",
        "            ]\n",
        "\n",
        "        for i in range(n_res_block):\n",
        "            blocks.append(ResBlock(channel, n_res_channel))\n",
        "\n",
        "        blocks.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.blocks(input)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channel, out_channel, channel, n_res_block, n_res_channel, stride\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        blocks = [nn.Conv2d(in_channel, channel, 3, padding=1)]\n",
        "\n",
        "        for i in range(n_res_block):\n",
        "            blocks.append(ResBlock(channel, n_res_channel))\n",
        "\n",
        "        blocks.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        if stride == 4:\n",
        "            blocks.extend(\n",
        "                [\n",
        "                    nn.ConvTranspose2d(channel, channel // 2, 4, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.ConvTranspose2d(\n",
        "                        channel // 2, out_channel, 4, stride=2, padding=1\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        elif stride == 2:\n",
        "            blocks.append(\n",
        "                nn.ConvTranspose2d(channel, out_channel, 4, stride=2, padding=1)\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.blocks(input)\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel=3,\n",
        "        channel=128,\n",
        "        n_res_block=2,\n",
        "        n_res_channel=32,\n",
        "        embed_dim=64,\n",
        "        n_embed=512,\n",
        "        decay=0.99,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_b = Encoder(in_channel, channel, n_res_block, n_res_channel, stride=4)\n",
        "        self.enc_t = Encoder(channel, channel, n_res_block, n_res_channel, stride=2)\n",
        "        self.quantize_conv_t = nn.Conv2d(channel, embed_dim, 1)\n",
        "        self.quantize_t = Quantize(embed_dim, n_embed)\n",
        "        self.dec_t = Decoder(\n",
        "            embed_dim, embed_dim, channel, n_res_block, n_res_channel, stride=2\n",
        "        )\n",
        "        self.quantize_conv_b = nn.Conv2d(embed_dim + channel, embed_dim, 1)\n",
        "        self.quantize_b = Quantize(embed_dim, n_embed)\n",
        "        self.upsample_t = nn.ConvTranspose2d(\n",
        "            embed_dim, embed_dim, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.dec = Decoder(\n",
        "            embed_dim + embed_dim,\n",
        "            in_channel,\n",
        "            channel,\n",
        "            n_res_block,\n",
        "            n_res_channel,\n",
        "            stride=4,\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        quant_t, quant_b, diff, _, _ = self.encode(input)\n",
        "        dec = self.decode(quant_t, quant_b)\n",
        "\n",
        "        return dec, diff\n",
        "\n",
        "    def encode(self, input):\n",
        "        enc_b = self.enc_b(input)\n",
        "        enc_t = self.enc_t(enc_b)\n",
        "\n",
        "        quant_t = self.quantize_conv_t(enc_t).permute(0, 2, 3, 1)\n",
        "        quant_t, diff_t, id_t = self.quantize_t(quant_t)\n",
        "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
        "        diff_t = diff_t.unsqueeze(0)\n",
        "\n",
        "        dec_t = self.dec_t(quant_t)\n",
        "        enc_b = torch.cat([dec_t, enc_b], 1)\n",
        "\n",
        "        quant_b = self.quantize_conv_b(enc_b).permute(0, 2, 3, 1)\n",
        "        quant_b, diff_b, id_b = self.quantize_b(quant_b)\n",
        "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
        "        diff_b = diff_b.unsqueeze(0)\n",
        "\n",
        "        return quant_t, quant_b, diff_t + diff_b, id_t, id_b\n",
        "\n",
        "    def decode(self, quant_t, quant_b):\n",
        "        upsample_t = self.upsample_t(quant_t)\n",
        "        quant = torch.cat([upsample_t, quant_b], 1)\n",
        "        dec = self.dec(quant)\n",
        "\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_t, code_b):\n",
        "        quant_t = self.quantize_t.embed_code(code_t)\n",
        "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
        "        quant_b = self.quantize_b.embed_code(code_b)\n",
        "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
        "\n",
        "        dec = self.decode(quant_t, quant_b)\n",
        "\n",
        "        return dec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GRVHtJAs0bXp"
      },
      "outputs": [],
      "source": [
        "# @title CompVis stable-diffusion model.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L368\n",
        "\n",
        "# pytorch_diffusion + derived encoder decoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.modules.attention import LinearAttention\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
        "    From Fairseq.\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in tensor2tensor, but differs slightly\n",
        "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    # swish\n",
        "    return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def Normalize(in_channels, num_groups=32):\n",
        "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0,1,0,1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if temb_channels > 0:\n",
        "            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        if temb is not None:\n",
        "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class LinAttnBlock(LinearAttention):\n",
        "    \"\"\"to match AttnBlock usage\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = q.reshape(b,c,h*w)\n",
        "        q = q.permute(0,2,1)   # b,hw,c\n",
        "        k = k.reshape(b,c,h*w) # b,c,hw\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b,c,h*w)\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
        "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
        "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
        "    if attn_type == \"vanilla\":\n",
        "        return AttnBlock(in_channels)\n",
        "    elif attn_type == \"none\":\n",
        "        return nn.Identity(in_channels)\n",
        "    else:\n",
        "        return LinAttnBlock(in_channels)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.use_timestep = use_timestep\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            self.temb = nn.Module()\n",
        "            self.temb.dense = nn.ModuleList([\n",
        "                torch.nn.Linear(self.ch,\n",
        "                                self.temb_ch),\n",
        "                torch.nn.Linear(self.temb_ch,\n",
        "                                self.temb_ch),\n",
        "            ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, t=None, context=None):\n",
        "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
        "        if context is not None:\n",
        "            # assume aligned context, cat along channel axis\n",
        "            x = torch.cat((x, context), dim=1)\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            assert t is not None\n",
        "            temb = get_timestep_embedding(t, self.ch)\n",
        "            temb = self.temb.dense[0](temb)\n",
        "            temb = nonlinearity(temb)\n",
        "            temb = self.temb.dense[1](temb)\n",
        "        else:\n",
        "            temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.conv_out.weight\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
        "                 attn_type=\"vanilla\", **ignorekwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "        self.give_pre_end = give_pre_end\n",
        "        self.tanh_out = tanh_out\n",
        "\n",
        "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
        "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
        "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
        "        print(\"Working with z of shape {} = {} dimensions.\".format(self.z_shape, np.prod(self.z_shape)))\n",
        "\n",
        "        # z to block_in\n",
        "        self.conv_in = torch.nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #assert z.shape[1:] == self.z_shape[1:]\n",
        "        self.last_z_shape = z.shape\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # z to block_in\n",
        "        h = self.conv_in(z)\n",
        "\n",
        "        # middle\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](h, temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        if self.give_pre_end:\n",
        "            return h\n",
        "\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        if self.tanh_out:\n",
        "            h = torch.tanh(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
        "                                     ResnetBlock(in_channels=in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=2 * in_channels, out_channels=4 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=4 * in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
        "                                     Upsample(in_channels, with_conv=True)])\n",
        "        # end\n",
        "        self.norm_out = Normalize(in_channels)\n",
        "        self.conv_out = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.model):\n",
        "            if i in [1,2,3]:\n",
        "                x = layer(x, None)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        h = self.norm_out(x)\n",
        "        h = nonlinearity(h)\n",
        "        x = self.conv_out(h)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsampleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
        "                 ch_mult=(2,2), dropout=0.0):\n",
        "        super().__init__()\n",
        "        # upsampling\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        block_in = in_channels\n",
        "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            res_block = []\n",
        "            block_out = ch * ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                res_block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "            self.res_blocks.append(nn.ModuleList(res_block))\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                self.upsample_blocks.append(Upsample(block_in, True))\n",
        "                curr_res = curr_res * 2\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # upsampling\n",
        "        h = x\n",
        "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                h = self.res_blocks[i_level][i_block](h, None)\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                h = self.upsample_blocks[k](h)\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class LatentRescaler(nn.Module):\n",
        "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
        "        super().__init__()\n",
        "        # residual block, interpolate, residual block\n",
        "        self.factor = factor\n",
        "        self.conv_in = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "        self.attn = AttnBlock(mid_channels)\n",
        "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "\n",
        "        self.conv_out = nn.Conv2d(mid_channels, out_channels, kernel_size=1,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "        for block in self.res_block1:\n",
        "            x = block(x, None)\n",
        "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
        "        x = self.attn(x)\n",
        "        for block in self.res_block2:\n",
        "            x = block(x, None)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
        "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        intermediate_chn = ch * ch_mult[-1]\n",
        "        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
        "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
        "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
        "                               out_ch=None)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
        "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.rescaler(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleDecoder(nn.Module):\n",
        "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
        "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        tmp_chn = z_channels*ch_mult[-1]\n",
        "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
        "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
        "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
        "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampler(nn.Module):\n",
        "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
        "        super().__init__()\n",
        "        assert out_size >= in_size\n",
        "        num_blocks = int(np.log2(out_size//in_size))+1\n",
        "        factor_up = 1.+ (out_size % in_size)\n",
        "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
        "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
        "                                       out_channels=in_channels)\n",
        "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
        "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
        "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Resize(nn.Module):\n",
        "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
        "        super().__init__()\n",
        "        self.with_conv = learned\n",
        "        self.mode = mode\n",
        "        if self.with_conv:\n",
        "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
        "            raise NotImplementedError()\n",
        "            assert in_channels is not None\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, scale_factor=1.0):\n",
        "        if scale_factor==1.0:\n",
        "            return x\n",
        "        else:\n",
        "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
        "        return x\n",
        "\n",
        "class FirstStagePostProcessor(nn.Module):\n",
        "    def __init__(self, ch_mult:list, in_channels,\n",
        "                 pretrained_model:nn.Module=None,\n",
        "                 reshape=False,\n",
        "                 n_channels=None,\n",
        "                 dropout=0.,\n",
        "                 pretrained_config=None):\n",
        "        super().__init__()\n",
        "        if pretrained_config is None:\n",
        "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.pretrained_model = pretrained_model\n",
        "        else:\n",
        "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.instantiate_pretrained(pretrained_config)\n",
        "        self.do_reshape = reshape\n",
        "        if n_channels is None:\n",
        "            n_channels = self.pretrained_model.encoder.ch\n",
        "\n",
        "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
        "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3, stride=1,padding=1)\n",
        "        blocks = []\n",
        "        downs = []\n",
        "        ch_in = n_channels\n",
        "        for m in ch_mult:\n",
        "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
        "            ch_in = m * n_channels\n",
        "            downs.append(Downsample(ch_in, with_conv=False))\n",
        "        self.model = nn.ModuleList(blocks)\n",
        "        self.downsampler = nn.ModuleList(downs)\n",
        "\n",
        "\n",
        "    def instantiate_pretrained(self, config):\n",
        "        model = instantiate_from_config(config)\n",
        "        self.pretrained_model = model.eval()\n",
        "        # self.pretrained_model.train = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_with_pretrained(self,x):\n",
        "        c = self.pretrained_model.encode(x)\n",
        "        if isinstance(c, DiagonalGaussianDistribution):\n",
        "            c = c.mode()\n",
        "        return  c\n",
        "\n",
        "    def forward(self,x):\n",
        "        z_fs = self.encode_with_pretrained(x)\n",
        "        z = self.proj_norm(z_fs)\n",
        "        z = self.proj(z)\n",
        "        z = nonlinearity(z)\n",
        "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
        "            z = submodel(z,temb=None)\n",
        "            z = downmodel(z)\n",
        "        if self.do_reshape:\n",
        "            z = rearrange(z,'b c h w -> b (h w) c')\n",
        "        return z\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BwCLJCY_dFD6"
      },
      "outputs": [],
      "source": [
        "# @title CompVis taming-transformers vqvae quantize.py\n",
        "# https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import einsum\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    \"\"\"\n",
        "    see https://github.com/MishaLaskin/vqvae/blob/d761a999e2267766400dc646d82d3ac3657771d4/models/quantizer.py\n",
        "    ____________________________________________\n",
        "    Discretization bottleneck part of the VQ-VAE.\n",
        "    Inputs:\n",
        "    - n_e : number of embeddings\n",
        "    - e_dim : dimension of embedding\n",
        "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
        "    _____________________________________________\n",
        "    \"\"\"\n",
        "\n",
        "    # NOTE: this class contains a bug regarding beta; see VectorQuantizer2 for\n",
        "    # a fix and use legacy=False to apply that fix. VectorQuantizer2 can be\n",
        "    # used wherever VectorQuantizer has been used before and is additionally\n",
        "    # more efficient.\n",
        "    def __init__(self, n_e, e_dim, beta):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Inputs the output of the encoder network z and maps it to a discrete\n",
        "        one-hot vector that is the index of the closest embedding vector e_j\n",
        "        z (continuous) -> z_q (discrete)\n",
        "        z.shape = (batch, channel, height, width)\n",
        "        quantization pipeline:\n",
        "            1. get encoder input (B,C,H,W)\n",
        "            2. flatten input to (B*H*W,C)\n",
        "        \"\"\"\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = z.permute(0, 2, 3, 1).contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.matmul(z_flattened, self.embedding.weight.t())\n",
        "\n",
        "        ## could possible replace this here\n",
        "        # #\\start...\n",
        "        # find closest encodings\n",
        "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
        "\n",
        "        min_encodings = torch.zeros(\n",
        "            min_encoding_indices.shape[0], self.n_e).to(z)\n",
        "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
        "\n",
        "        # dtype min encodings: torch.float32\n",
        "        # min_encodings shape: torch.Size([2048, 512])\n",
        "        # min_encoding_indices.shape: torch.Size([2048, 1])\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
        "        #.........\\end\n",
        "\n",
        "        # with:\n",
        "        # .........\\start\n",
        "        #min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        #z_q = self.embedding(min_encoding_indices)\n",
        "        # ......\\end......... (TODO)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
        "            torch.mean((z_q - z.detach()) ** 2)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # perplexity\n",
        "        e_mean = torch.mean(min_encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # shape specifying (batch, height, width, channel)\n",
        "        # TODO: check for more easy handling with nn.Embedding\n",
        "        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n",
        "        min_encodings.scatter_(1, indices[:,None], 1)\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n",
        "\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q\n",
        "\n",
        "\n",
        "class GumbelQuantize(nn.Module):\n",
        "    \"\"\"\n",
        "    credit to @karpathy: https://github.com/karpathy/deep-vector-quantization/blob/main/model.py (thanks!)\n",
        "    Gumbel Softmax trick quantizer\n",
        "    Categorical Reparameterization with Gumbel-Softmax, Jang et al. 2016\n",
        "    https://arxiv.org/abs/1611.01144\n",
        "    \"\"\"\n",
        "    def __init__(self, num_hiddens, embedding_dim, n_embed, straight_through=True, kl_weight=5e-4, temp_init=1.0, use_vqinterface=True, remap=None, unknown_index=\"random\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_embed = n_embed\n",
        "\n",
        "        self.straight_through = straight_through\n",
        "        self.temperature = temp_init\n",
        "        self.kl_weight = kl_weight\n",
        "\n",
        "        self.proj = nn.Conv2d(num_hiddens, n_embed, 1)\n",
        "        self.embed = nn.Embedding(n_embed, embedding_dim)\n",
        "\n",
        "        self.use_vqinterface = use_vqinterface\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_embed} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_embed\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z, temp=None, return_logits=False):\n",
        "        # force hard = True when we are in eval mode, as we must quantize. actually, always true seems to work\n",
        "        hard = self.straight_through if self.training else True\n",
        "        temp = self.temperature if temp is None else temp\n",
        "\n",
        "        logits = self.proj(z)\n",
        "        if self.remap is not None:\n",
        "            # continue only with used logits\n",
        "            full_zeros = torch.zeros_like(logits)\n",
        "            logits = logits[:,self.used,...]\n",
        "\n",
        "        soft_one_hot = F.gumbel_softmax(logits, tau=temp, dim=1, hard=hard)\n",
        "        if self.remap is not None:\n",
        "            # go back to all entries but unused set to zero\n",
        "            full_zeros[:,self.used,...] = soft_one_hot\n",
        "            soft_one_hot = full_zeros\n",
        "        z_q = einsum('b n h w, n d -> b d h w', soft_one_hot, self.embed.weight)\n",
        "\n",
        "        # + kl divergence to the prior loss\n",
        "        qy = F.softmax(logits, dim=1)\n",
        "        diff = self.kl_weight * torch.sum(qy * torch.log(qy * self.n_embed + 1e-10), dim=1).mean()\n",
        "\n",
        "        ind = soft_one_hot.argmax(dim=1)\n",
        "        if self.remap is not None:\n",
        "            ind = self.remap_to_used(ind)\n",
        "        if self.use_vqinterface:\n",
        "            if return_logits:\n",
        "                return z_q, diff, (None, None, ind), logits\n",
        "            return z_q, diff, (None, None, ind)\n",
        "        return z_q, diff, ind\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        b, h, w, c = shape\n",
        "        assert b*h*w == indices.shape[0]\n",
        "        indices = rearrange(indices, '(b h w) -> b h w', b=b, h=h, w=w)\n",
        "        if self.remap is not None:\n",
        "            indices = self.unmap_to_all(indices)\n",
        "        one_hot = F.one_hot(indices, num_classes=self.n_embed).permute(0, 3, 1, 2).float()\n",
        "        z_q = einsum('b n h w, n d -> b d h w', one_hot, self.embed.weight)\n",
        "        return z_q\n",
        "\n",
        "\n",
        "class VectorQuantizer2(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly\n",
        "    avoids costly matrix multiplications and allows for post-hoc remapping of indices.\n",
        "    \"\"\"\n",
        "    # NOTE: due to a bug the beta term was applied to the wrong term. for\n",
        "    # backwards compatibility we use the buggy version by default, but you can\n",
        "    # specify legacy=False to fix it.\n",
        "    def __init__(self, n_e, e_dim, beta, remap=None, unknown_index=\"random\", sane_index_shape=False, legacy=True):\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "        self.legacy = legacy\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_e\n",
        "\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z, temp=None, rescale_logits=False, return_logits=False):\n",
        "        assert temp is None or temp==1.0, \"Only for interface compatible with Gumbel\"\n",
        "        assert rescale_logits==False, \"Only for interface compatible with Gumbel\"\n",
        "        assert return_logits==False, \"Only for interface compatible with Gumbel\"\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "        perplexity = None\n",
        "        min_encodings = None\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        # loss = torch.mean((z_q.detach()-z)**2) + self.beta * torch.mean((z_q - z.detach()) ** 2) # legacy\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        if self.remap is not None:\n",
        "            min_encoding_indices = min_encoding_indices.reshape(z.shape[0],-1) # add batch axis\n",
        "            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n",
        "            min_encoding_indices = min_encoding_indices.reshape(-1,1) # flatten\n",
        "        if self.sane_index_shape:\n",
        "            min_encoding_indices = min_encoding_indices.reshape(\n",
        "                z_q.shape[0], z_q.shape[2], z_q.shape[3])\n",
        "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # shape specifying (batch, height, width, channel)\n",
        "        if self.remap is not None:\n",
        "            indices = indices.reshape(shape[0],-1) # add batch axis\n",
        "            indices = self.unmap_to_all(indices)\n",
        "            indices = indices.reshape(-1) # flatten again\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "class EmbeddingEMA(nn.Module):\n",
        "    def __init__(self, num_tokens, codebook_dim, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        weight = torch.randn(num_tokens, codebook_dim)\n",
        "        self.weight = nn.Parameter(weight, requires_grad = False)\n",
        "        self.cluster_size = nn.Parameter(torch.zeros(num_tokens), requires_grad = False)\n",
        "        self.embed_avg = nn.Parameter(weight.clone(), requires_grad = False)\n",
        "        self.update = True\n",
        "\n",
        "    def forward(self, embed_id):\n",
        "        return F.embedding(embed_id, self.weight)\n",
        "\n",
        "    def cluster_size_ema_update(self, new_cluster_size):\n",
        "        self.cluster_size.data.mul_(self.decay).add_(new_cluster_size, alpha=1 - self.decay)\n",
        "\n",
        "    def embed_avg_ema_update(self, new_embed_avg):\n",
        "        self.embed_avg.data.mul_(self.decay).add_(new_embed_avg, alpha=1 - self.decay)\n",
        "\n",
        "    def weight_update(self, num_tokens):\n",
        "        n = self.cluster_size.sum()\n",
        "        smoothed_cluster_size = (\n",
        "                (self.cluster_size + self.eps) / (n + num_tokens * self.eps) * n\n",
        "            )\n",
        "        #normalize embedding average with smoothed cluster size\n",
        "        embed_normalized = self.embed_avg / smoothed_cluster_size.unsqueeze(1)\n",
        "        self.weight.data.copy_(embed_normalized)\n",
        "\n",
        "\n",
        "class EMAVectorQuantizer(nn.Module):\n",
        "    def __init__(self, n_embed, embedding_dim, beta, decay=0.99, eps=1e-5,\n",
        "                remap=None, unknown_index=\"random\"):\n",
        "        super().__init__()\n",
        "        self.codebook_dim = codebook_dim\n",
        "        self.num_tokens = num_tokens\n",
        "        self.beta = beta\n",
        "        self.embedding = EmbeddingEMA(self.num_tokens, self.codebook_dim, decay, eps)\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_embed} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_embed\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        #z, 'b c h w -> b h w c'\n",
        "        z = rearrange(z, 'b c h w -> b h w c')\n",
        "        z_flattened = z.reshape(-1, self.codebook_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = z_flattened.pow(2).sum(dim=1, keepdim=True) + \\\n",
        "            self.embedding.weight.pow(2).sum(dim=1) - 2 * \\\n",
        "            torch.einsum('bd,nd->bn', z_flattened, self.embedding.weight) # 'n d -> d n'\n",
        "\n",
        "\n",
        "        encoding_indices = torch.argmin(d, dim=1)\n",
        "\n",
        "        z_q = self.embedding(encoding_indices).view(z.shape)\n",
        "        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        if self.training and self.embedding.update:\n",
        "            #EMA cluster size\n",
        "            encodings_sum = encodings.sum(0)\n",
        "            self.embedding.cluster_size_ema_update(encodings_sum)\n",
        "            #EMA embedding average\n",
        "            embed_sum = encodings.transpose(0,1) @ z_flattened\n",
        "            self.embedding.embed_avg_ema_update(embed_sum)\n",
        "            #normalize embed_avg and update weight\n",
        "            self.embedding.weight_update(self.num_tokens)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * F.mse_loss(z_q.detach(), z)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        #z_q, 'b h w c -> b c h w'\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w')\n",
        "        return z_q, loss, (perplexity, encodings, encoding_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7CSqxzfG_2rb"
      },
      "outputs": [],
      "source": [
        "# @title CompVis stable-diffusion autoencoder.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/models/autoencoder.py\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# from taming.modules.vqvae.quantize import VectorQuantizer2 as VectorQuantizer\n",
        "\n",
        "from ldm.modules.diffusionmodules.model import Encoder, Decoder\n",
        "from ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "class VQModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 n_embed,\n",
        "                 embed_dim,\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 image_key=\"image\",\n",
        "                 colorize_nlabels=None,\n",
        "                 monitor=None,\n",
        "                 batch_resize_range=None,\n",
        "                 scheduler_config=None,\n",
        "                 lr_g_factor=1.0,\n",
        "                 remap=None,\n",
        "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "                 use_ema=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.image_key = image_key\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n",
        "                                        remap=remap,\n",
        "                                        sane_index_shape=sane_index_shape)\n",
        "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        if colorize_nlabels is not None:\n",
        "            assert type(colorize_nlabels)==int\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        self.batch_resize_range = batch_resize_range\n",
        "        if self.batch_resize_range is not None:\n",
        "            print(f\"{self.__class__.__name__}: Using per-batch resizing in range {batch_resize_range}.\")\n",
        "\n",
        "        self.use_ema = use_ema\n",
        "        if self.use_ema:\n",
        "            self.model_ema = LitEma(self)\n",
        "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
        "\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
        "        self.scheduler_config = scheduler_config\n",
        "        self.lr_g_factor = lr_g_factor\n",
        "\n",
        "    @contextmanager\n",
        "    def ema_scope(self, context=None):\n",
        "        if self.use_ema:\n",
        "            self.model_ema.store(self.parameters())\n",
        "            self.model_ema.copy_to(self)\n",
        "            if context is not None:\n",
        "                print(f\"{context}: Switched to EMA weights\")\n",
        "        try:\n",
        "            yield None\n",
        "        finally:\n",
        "            if self.use_ema:\n",
        "                self.model_ema.restore(self.parameters())\n",
        "                if context is not None:\n",
        "                    print(f\"{context}: Restored training weights\")\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
        "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        missing, unexpected = self.load_state_dict(sd, strict=False)\n",
        "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
        "        if len(missing) > 0:\n",
        "            print(f\"Missing Keys: {missing}\")\n",
        "            print(f\"Unexpected Keys: {unexpected}\")\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        if self.use_ema:\n",
        "            self.model_ema(self)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        if self.batch_resize_range is not None:\n",
        "            lower_size = self.batch_resize_range[0]\n",
        "            upper_size = self.batch_resize_range[1]\n",
        "            if self.global_step <= 4:\n",
        "                # do the first few batches with max size to avoid later oom\n",
        "                new_resize = upper_size\n",
        "            else:\n",
        "                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n",
        "            if new_resize != x.shape[2]:\n",
        "                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n",
        "            x = x.detach()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        # https://github.com/pytorch/pytorch/issues/37142\n",
        "        # try not to fool the heuristics\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # autoencode\n",
        "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
        "                                            predicted_indices=ind)\n",
        "\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # discriminator\n",
        "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        log_dict = self._validation_step(batch, batch_idx)\n",
        "        with self.ema_scope():\n",
        "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
        "        return log_dict\n",
        "\n",
        "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
        "                                        self.global_step,\n",
        "                                        last_layer=self.get_last_layer(),\n",
        "                                        split=\"val\"+suffix,\n",
        "                                        predicted_indices=ind\n",
        "                                        )\n",
        "\n",
        "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
        "                                            self.global_step,\n",
        "                                            last_layer=self.get_last_layer(),\n",
        "                                            split=\"val\"+suffix,\n",
        "                                            predicted_indices=ind\n",
        "                                            )\n",
        "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
        "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr_d = self.learning_rate\n",
        "        lr_g = self.lr_g_factor*self.learning_rate\n",
        "        print(\"lr_d\", lr_d)\n",
        "        print(\"lr_g\", lr_g)\n",
        "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
        "                                  list(self.decoder.parameters())+\n",
        "                                  list(self.quantize.parameters())+\n",
        "                                  list(self.quant_conv.parameters())+\n",
        "                                  list(self.post_quant_conv.parameters()),\n",
        "                                  lr=lr_g, betas=(0.5, 0.9))\n",
        "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
        "                                    lr=lr_d, betas=(0.5, 0.9))\n",
        "\n",
        "        if self.scheduler_config is not None:\n",
        "            scheduler = instantiate_from_config(self.scheduler_config)\n",
        "\n",
        "            print(\"Setting up LambdaLR scheduler...\")\n",
        "            scheduler = [\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                },\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                },\n",
        "            ]\n",
        "            return [opt_ae, opt_disc], scheduler\n",
        "        return [opt_ae, opt_disc], []\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "    def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n",
        "        log = dict()\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        x = x.to(self.device)\n",
        "        if only_inputs:\n",
        "            log[\"inputs\"] = x\n",
        "            return log\n",
        "        xrec, _ = self(x)\n",
        "        if x.shape[1] > 3:\n",
        "            # colorize with random projection\n",
        "            assert xrec.shape[1] > 3\n",
        "            x = self.to_rgb(x)\n",
        "            xrec = self.to_rgb(xrec)\n",
        "        log[\"inputs\"] = x\n",
        "        log[\"reconstructions\"] = xrec\n",
        "        if plot_ema:\n",
        "            with self.ema_scope():\n",
        "                xrec_ema, _ = self(x)\n",
        "                if x.shape[1] > 3: xrec_ema = self.to_rgb(xrec_ema)\n",
        "                log[\"reconstructions_ema\"] = xrec_ema\n",
        "        return log\n",
        "\n",
        "    def to_rgb(self, x):\n",
        "        assert self.image_key == \"segmentation\"\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
        "        x = F.conv2d(x, weight=self.colorize)\n",
        "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
        "        return x\n",
        "\n",
        "\n",
        "class VQModelInterface(VQModel):\n",
        "    def __init__(self, embed_dim, *args, **kwargs):\n",
        "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, h, force_not_quantize=False):\n",
        "        # also go through quantization layer\n",
        "        if not force_not_quantize:\n",
        "            quant, emb_loss, info = self.quantize(h)\n",
        "        else:\n",
        "            quant = h\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "\n",
        "class AutoencoderKL(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 embed_dim,\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 image_key=\"image\",\n",
        "                 colorize_nlabels=None,\n",
        "                 monitor=None,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.image_key = image_key\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        assert ddconfig[\"double_z\"]\n",
        "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        self.embed_dim = embed_dim\n",
        "        if colorize_nlabels is not None:\n",
        "            assert type(colorize_nlabels)==int\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
        "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        self.load_state_dict(sd, strict=False)\n",
        "        print(f\"Restored from {path}\")\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, sample_posterior=True):\n",
        "        posterior = self.encode(input)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        return dec, posterior\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # train encoder+decoder+logvar\n",
        "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # train the discriminator\n",
        "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "\n",
        "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "\n",
        "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.learning_rate\n",
        "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
        "                                  list(self.decoder.parameters())+\n",
        "                                  list(self.quant_conv.parameters())+\n",
        "                                  list(self.post_quant_conv.parameters()),\n",
        "                                  lr=lr, betas=(0.5, 0.9))\n",
        "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "        return [opt_ae, opt_disc], []\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_images(self, batch, only_inputs=False, **kwargs):\n",
        "        log = dict()\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        x = x.to(self.device)\n",
        "        if not only_inputs:\n",
        "            xrec, posterior = self(x)\n",
        "            if x.shape[1] > 3:\n",
        "                # colorize with random projection\n",
        "                assert xrec.shape[1] > 3\n",
        "                x = self.to_rgb(x)\n",
        "                xrec = self.to_rgb(xrec)\n",
        "            log[\"samples\"] = self.decode(torch.randn_like(posterior.sample()))\n",
        "            log[\"reconstructions\"] = xrec\n",
        "        log[\"inputs\"] = x\n",
        "        return log\n",
        "\n",
        "    def to_rgb(self, x):\n",
        "        assert self.image_key == \"segmentation\"\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
        "        x = F.conv2d(x, weight=self.colorize)\n",
        "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XOuGUdMZaxB6"
      },
      "outputs": [],
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ConvLayer\n",
        "# nn.Sequential(\n",
        "#     nn.Dropout2d(dropout), nn.Conv2d(in_ch, out_ch, 3, 1, 3//2, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU()\n",
        "# )\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.repeats = out_ch//in_ch\n",
        "        if out_ch//in_ch > 1:\n",
        "            self.func = lambda x: x.repeat_interleave(out_ch//in_ch, dim=1) # [b,i,h,w] -> [b,o,h,w]\n",
        "        elif in_ch//out_ch > 1:\n",
        "            self.func = lambda x: torch.unflatten(x, 1, (out_ch, in_ch//out_ch)).mean(dim=2) # [b,i,h,w] -> [b,o,i/o,h,w] -> [b,o,h,w]\n",
        "        else: print('err SameCh', in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        return self.func(x)\n",
        "\n",
        "class PixelShortcut(nn.Module): # up shortcut [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(SameCh(in_ch, out_ch*r**2), nn.PixelShuffle(r)) #\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SameCh(in_ch*r**2, out_ch)) #\n",
        "        else: self.net = SameCh(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): #\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module): # down main [b,i,2h,2w] -> [b,o,h,w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch//r**2, kernel_size, 1, kernel_size//2)\n",
        "\n",
        "    def forward(self, x): # [b,i,2h,2w] -> [b,o/4,2h,2w] -> [b,o,h,w]\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module): # up main [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch*r**2, kernel_size, 1, kernel_size//2)\n",
        "        # self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, 1, kernel_size//2) # InterpolateConvUpSampleLayer\n",
        "\n",
        "    def forward(self, x): # [b,i,h,w] -> [b,4o,h,w] -> [b,o,2h,2w]\n",
        "        # x = torch.nn.functional.interpolate(x, scale_r=self.r, mode=\"nearest\")\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "        # if self.r>1: self.net = nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), out_r=r), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), in_r=r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        else: self.net = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2)\n",
        "        # self.net.apply(self.init_conv_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# # https://arxiv.org/pdf/1707.02937\n",
        "# nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), 'out'), nn.PixelShuffle(r))\n",
        "# nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), 'in'))\n",
        "\n",
        "# https://github.com/fastai/fastai/blob/main/fastai/layers.py#L368\n",
        "def icnr_init(x, scale=2, init=nn.init.kaiming_normal_):\n",
        "    \"ICNR init of `x`, with `scale` and `init` function\"\n",
        "    ni,nf,h,w = x.shape\n",
        "    ni2 = int(ni/(scale**2))\n",
        "    k = init(x.new_zeros([ni2,nf,h,w])).transpose(0, 1)\n",
        "    k = k.contiguous().view(ni2, nf, -1)\n",
        "    k = k.repeat(1, 1, scale**2)\n",
        "    return k.contiguous().view([nf,ni,h,w]).transpose(0, 1)\n",
        "\n",
        "class PixelShuffle_ICNR(nn.Sequential):\n",
        "    \"Upsample by `scale` from `ni` filters to `nf` (default `ni`), using `nn.PixelShuffle`.\"\n",
        "    def __init__(self, ni, nf=None, scale=2, blur=False, norm_type=NormType.Weight, act_cls=defaults.activation):\n",
        "        super().__init__()\n",
        "        nf = ifnone(nf, ni)\n",
        "        layers = [ConvLayer(ni, nf*(scale**2), ks=1, norm_type=norm_type, act_cls=act_cls, bias_std=0),\n",
        "                  nn.PixelShuffle(scale)]\n",
        "        if norm_type == NormType.Weight:\n",
        "            layers[0][0].weight_v.data.copy_(icnr_init(layers[0][0].weight_v.data))\n",
        "            layers[0][0].weight_g.data.copy_(((layers[0][0].weight_v.data**2).sum(dim=[1,2,3])**0.5)[:,None,None,None])\n",
        "        else:\n",
        "            layers[0][0].weight.data.copy_(icnr_init(layers[0][0].weight.data))\n",
        "        if blur: layers += [nn.ReplicationPad2d((1,0,1,0)), nn.AvgPool2d(2, stride=1)]\n",
        "        super().__init__(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# block = EfficientViTBlock(in_ch, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU\n",
        "# self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio,\n",
        "#     use_bias=(True, True, False), norm=(None, None, norm), act_func=(act_func, act_func, None))\n",
        "class GLUMBConv(nn.Module):\n",
        "    # def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4, use_bias=False, norm=(None, None, \"ln2d\"), act_func=(\"silu\", \"silu\", None)):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4):\n",
        "        super().__init__()\n",
        "        mid_channels = round(in_ch * expand_ratio) if mid_channels is None else mid_channels\n",
        "        # self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        # self.inverted_conv = ConvLayer(in_ch, mid_channels * 2, 1, use_bias=use_bias[0], norm=norm[0], act_func=act_func[0],)\n",
        "        # self.depth_conv = ConvLayer(mid_channels * 2, mid_channels * 2, kernel_size, stride=stride, groups=mid_channels * 2, use_bias=use_bias[1], norm=norm[1], act_func=None,)\n",
        "        self.inverted_depth_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_channels*2, 1, 1, 0), nn.SiLU(),\n",
        "            nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2),\n",
        "        )\n",
        "        # self.point_conv = ConvLayer(mid_channels, out_ch, 1, use_bias=use_bias[2], norm=norm[2], act_func=act_func[2],)\n",
        "        self.point_conv = nn.Sequential(\n",
        "            nn.Conv2d(mid_channels, out_ch, 1, 1, 0, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.inverted_conv(x)\n",
        "        # x = self.depth_conv(x)\n",
        "        x = self.inverted_depth_conv(x)\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        x = x * nn.SiLU()(gate)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# main_block = ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel_size=3, stride=1, d_model=None,\n",
        "        use_bias=False, norm=(\"bn2d\", \"bn2d\"), act_func=(\"relu6\", None)):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_ch\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, d_model, kernel_size, stride, kernel_size//2), nn.SiLU(),\n",
        "            nn.Conv2d(d_model, out_ch, kernel_size, 1, kernel_size//2, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(self, in_ch, heads_ratio = 1.0, dim=32, expand_ratio=1, # expand_ratio=4\n",
        "        # scales: tuple[int, ...] = (5,), # (5,): sana\n",
        "        # act_func = \"hswish\", # nn.Hardswish()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # self.context_module = LiteMLA(in_ch, in_ch, heads_ratio=heads_ratio, dim=dim, norm=(None, norm), scales=scales,)\n",
        "        self.context_module = AttentionBlock(in_ch, d_head=8)\n",
        "        # self.local_module = MBConv(\n",
        "        self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.context_module(x)\n",
        "        x = x + self.local_module(x)\n",
        "        return x\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "    # def forward(self, x):\n",
        "    #     res = self.forward_main(self.pre_norm(x)) + self.shortcut(x)\n",
        "    #     res = self.post_act(res)\n",
        "    #     return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "w_FyFDkua0lA"
      },
      "outputs": [],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_block(block_type, d_model, norm=None, act=None):\n",
        "    if block_type == \"ResBlock\": return ResBlock(d_model) # ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "    # ResBlock: bn2d, relu ; EViT_GLU: trms2d, silu\n",
        "    elif block_type == \"EViT_GLU\": return EfficientViTBlock(d_model) # EfficientViTBlock(d_model, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU:scales=() ; EViTS5_GLU sana:scales=(5,)\n",
        "\n",
        "class LevelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, depth, block_type, norm=None, act=None, updown=None):\n",
        "        super().__init__()\n",
        "        stage = []\n",
        "        if updown=='up': stage.append(UpsampleBlock(in_ch, out_ch))\n",
        "        for d in range(depth):\n",
        "            # block = build_block(block_type=block_type, in_ch=d_model if d > 0 else in_ch, out_ch=d_model, norm=norm, act=act,)\n",
        "            block = build_block(block_type, out_ch if updown=='up' else in_ch, norm=norm, act=act,)\n",
        "            stage.append(block)\n",
        "        if updown=='down': stage.append(DownsampleBlock(in_ch, out_ch))\n",
        "        self.block = nn.Sequential(*stage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# stage = build_stage_main(width, depth, block_type)\n",
        "# downsample_block = DownsampleBlock(width, width_list[stage_id + 1])\n",
        "\n",
        "# upsample_block = UpsampleBlock(width_list[stage_id + 1], width)\n",
        "# stage.extend(build_stage_main(width, depth, block_type, \"bn2d\", \"silu\", input_width=width))\n",
        "\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = nn.Conv2d(in_ch, out_ch, 3, 2, 3//2)\n",
        "        # self.block = ConvPixelUnshuffleDownSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=3, r=1/2)\n",
        "        # self.shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(in_ch, out_ch, r=2)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=1/2)\n",
        "    def forward(self, x):\n",
        "        # print(\"DownsampleBlock fwd\", x.shape, self.block(x).shape + self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,4,8,2,2,2]\n",
        "\n",
        "        # # self.project_in = nn.Conv2d(in_ch, width_list[0], 3, 1, 3//2) # if depth_list[0] > 0:\n",
        "        self.project_in = DownsampleBlock(in_ch, width_list[0]) # shortcut=None # self.project_in = ConvPixelUnshuffleDownSampleLayer(in_ch, width_list[0], kernel_size=3, r=2)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[0], width_list[-1], depth=depth_list[0], block_type='ResBlock', updown='down'),\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "        )\n",
        "\n",
        "        self.out_block = nn.Conv2d(width_list[-1], out_ch, 3, 1, 3//2)\n",
        "        # self.out_shortcut = PixelUnshuffleChannelAveragingDownSampleLayer(width_list[-1], out_ch, r=1)\n",
        "        self.out_shortcut = PixelShortcut(width_list[-1], out_ch, r=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x = self.stages(x)\n",
        "        # print(\"Encoder fwd\", x.shape, self.out_block, self.out_shortcut(x).shape)\n",
        "        x = self.out_block(x) + self.out_shortcut(x)\n",
        "        return x\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = ConvPixelShuffleUpSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=3, r=2)\n",
        "        # self.block = InterpolateConvUpSampleLayer(in_ch=in_ch, out_ch=out_ch, kernel_size=3, r=2)\n",
        "        # self.shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, out_ch, r=2)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=2)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,o,2h,2w]\n",
        "        # print(\"UpsampleBlock fwd\", x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.in_block = nn.Conv2d(in_ch, width_list[-1], 3, 1, 3//2)\n",
        "        # self.in_shortcut = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, width_list[-1], r=1)\n",
        "        self.in_shortcut = PixelShortcut(in_ch, width_list[-1], r=1)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "            LevelBlock(width_list[-1], width_list[0], depth=depth_list[0], block_type='ResBlock', updown='up'),\n",
        "        )\n",
        "\n",
        "        # if depth_list[0] > 0:\n",
        "        # self.project_out = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(width_list[0]), nn.ReLU(), nn.Conv2d(width_list[0], out_ch, 3, 1, 3//2) # norm=\"trms2d\"\n",
        "        #     )\n",
        "        # else:\n",
        "        self.project_out = nn.Sequential(\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpsampleBlock(width_list[0], out_ch) # shortcut=None ; norm=\"trms2d\"\n",
        "            # nn.BatchNorm2d(width_list[0]), nn.ReLU(), ConvPixelShuffleUpSampleLayer(width_list[0], out_ch, kernel_size=3, r=2) # shortcut=None ; norm=\"trms2d\"\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.in_block(x) + self.in_shortcut(x)\n",
        "        x = self.stages(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(in_ch, out_ch, d_model, mult, depth_list)\n",
        "        self.decoder = Decoder(out_ch, in_ch, d_model, mult, depth_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# https://discuss.pytorch.org/t/is-there-a-layer-normalization-for-conv2d/7595/5\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "class LayerNorm2d(nn.LayerNorm):\n",
        "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
        "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=24, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# model = Encoder(in_ch, out_ch, d_model=32, mult=[1,1], depth_list=[2,2])\n",
        "# print(sum(p.numel() for p in model.project_in.parameters() if p.requires_grad)) # 896\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "# print(sum(p.numel() for p in model.out_shortcut.parameters() if p.requires_grad)) # 0\n",
        "# print(sum(p.numel() for p in model.out_block.parameters() if p.requires_grad)) # 18436\n",
        "# model = Decoder(out_ch, in_ch)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rkrEVsXxSs_C"
      },
      "outputs": [],
      "source": [
        "# @title efficientvit nn/ops.py\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from efficientvit.models.nn.act import build_act\n",
        "from efficientvit.models.nn.norm import build_norm\n",
        "from efficientvit.models.utils import get_same_padding, list_sum, resize, val2list, val2tuple\n",
        "\n",
        "__all__ = [\n",
        "    \"ConvLayer\",\n",
        "    \"UpSampleLayer\",\n",
        "    \"ConvPixelUnshuffleDownSampleLayer\",\n",
        "    \"PixelUnshuffleChannelAveragingDownSampleLayer\",\n",
        "    \"ConvPixelShuffleUpSampleLayer\",\n",
        "    \"ChannelDuplicatingPixelUnshuffleUpSampleLayer\",\n",
        "    \"LinearLayer\",\n",
        "    \"IdentityLayer\",\n",
        "    \"DSConv\",\n",
        "    \"MBConv\",\n",
        "    \"FusedMBConv\",\n",
        "    \"ResBlock\",\n",
        "    \"LiteMLA\",\n",
        "    \"EfficientViTBlock\",\n",
        "    \"ResidualBlock\",\n",
        "    \"DAGBlock\",\n",
        "    \"OpSequential\",\n",
        "]\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Basic Layers                                      #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        dilation=1,\n",
        "        groups=1,\n",
        "        use_bias=False,\n",
        "        dropout=0,\n",
        "        norm=\"bn2d\",\n",
        "        act_func=\"relu\",\n",
        "    ):\n",
        "        super(ConvLayer, self).__init__()\n",
        "\n",
        "        padding = get_same_padding(kernel_size)\n",
        "        padding *= dilation\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropout, inplace=False) if dropout > 0 else None\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=(kernel_size, kernel_size),\n",
        "            stride=(stride, stride),\n",
        "            padding=padding,\n",
        "            dilation=(dilation, dilation),\n",
        "            groups=groups,\n",
        "            bias=use_bias,\n",
        "        )\n",
        "        self.norm = build_norm(norm, num_features=out_channels)\n",
        "        self.act = build_act(act_func)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = self.conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.act:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mode=\"bicubic\",\n",
        "        size: Optional[int | tuple[int, int] | list[int]] = None,\n",
        "        factor=2,\n",
        "        align_corners=False,\n",
        "    ):\n",
        "        super(UpSampleLayer, self).__init__()\n",
        "        self.mode = mode\n",
        "        self.size = val2list(size, 2) if size is not None else None\n",
        "        self.factor = None if self.size is not None else factor\n",
        "        self.align_corners = align_corners\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if (self.size is not None and tuple(x.shape[-2:]) == self.size) or self.factor == 1:\n",
        "            return x\n",
        "        if x.dtype in [torch.float16, torch.bfloat16]:\n",
        "            x = x.float()\n",
        "        return resize(x, self.size, self.factor, self.mode, self.align_corners)\n",
        "\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        out_ratio = factor**2\n",
        "        assert out_channels % out_ratio == 0\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels // out_ratio,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.factor = factor\n",
        "        assert in_channels * factor**2 % out_channels == 0\n",
        "        self.group_size = in_channels * factor**2 // out_channels\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.pixel_unshuffle(x, self.factor)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, self.out_channels, self.group_size, H, W)\n",
        "        x = x.mean(dim=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        out_ratio = factor**2\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels * out_ratio,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InterpolateConvUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "        mode: str = \"nearest\",\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        self.mode = mode\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=self.factor, mode=self.mode)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ChannelDuplicatingPixelUnshuffleUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.factor = factor\n",
        "        assert out_channels * factor**2 % in_channels == 0\n",
        "        self.repeats = out_channels * factor**2 // in_channels\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.repeat_interleave(self.repeats, dim=1)\n",
        "        x = F.pixel_shuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LinearLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        use_bias=True,\n",
        "        dropout=0,\n",
        "        norm=None,\n",
        "        act_func=None,\n",
        "    ):\n",
        "        super(LinearLayer, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout, inplace=False) if dropout > 0 else None\n",
        "        self.linear = nn.Linear(in_features, out_features, use_bias)\n",
        "        self.norm = build_norm(norm, num_features=out_features)\n",
        "        self.act = build_act(act_func)\n",
        "\n",
        "    def _try_squeeze(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() > 2:\n",
        "            x = torch.flatten(x, start_dim=1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self._try_squeeze(x)\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.act:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Basic Blocks                                      #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class DSConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super(DSConv, self).__init__()\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        self.depth_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            in_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            groups=in_channels,\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "            use_bias=use_bias[0],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "            use_bias=use_bias[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.depth_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", \"relu6\", None),\n",
        "    ):\n",
        "        super(MBConv, self).__init__()\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 3)\n",
        "        norm = val2tuple(norm, 3)\n",
        "        act_func = val2tuple(act_func, 3)\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.inverted_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            1,\n",
        "            stride=1,\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "            use_bias=use_bias[0],\n",
        "        )\n",
        "        self.depth_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            groups=mid_channels,\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "            use_bias=use_bias[1],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            norm=norm[2],\n",
        "            act_func=act_func[2],\n",
        "            use_bias=use_bias[2],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.inverted_conv(x)\n",
        "        x = self.depth_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FusedMBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        groups=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.spatial_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            groups=groups,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.spatial_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GLUMBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        use_bias=False,\n",
        "        norm=(None, None, \"ln2d\"),\n",
        "        act_func=(\"silu\", \"silu\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 3)\n",
        "        norm = val2tuple(norm, 3)\n",
        "        act_func = val2tuple(act_func, 3)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        self.inverted_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels * 2,\n",
        "            1,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.depth_conv = ConvLayer(\n",
        "            mid_channels * 2,\n",
        "            mid_channels * 2,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            groups=mid_channels * 2,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=None,\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[2],\n",
        "            norm=norm[2],\n",
        "            act_func=act_func[2],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.inverted_conv(x)\n",
        "        x = self.depth_conv(x)\n",
        "\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        gate = self.glu_act(gate)\n",
        "        x = x * gate\n",
        "\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.conv1 = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.conv2 = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LiteMLA(nn.Module):\n",
        "    r\"\"\"Lightweight multi-scale linear attention\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        heads: Optional[int] = None,\n",
        "        heads_ratio: float = 1.0,\n",
        "        dim=8,\n",
        "        use_bias=False,\n",
        "        norm=(None, \"bn2d\"),\n",
        "        act_func=(None, None),\n",
        "        kernel_func=\"relu\",\n",
        "        scales: tuple[int, ...] = (5,),\n",
        "        eps=1.0e-15,\n",
        "    ):\n",
        "        super(LiteMLA, self).__init__()\n",
        "        self.eps = eps\n",
        "        heads = int(in_channels // dim * heads_ratio) if heads is None else heads\n",
        "\n",
        "        total_dim = heads * dim\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.qkv = ConvLayer(\n",
        "            in_channels,\n",
        "            3 * total_dim,\n",
        "            1,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.aggreg = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        3 * total_dim,\n",
        "                        3 * total_dim,\n",
        "                        scale,\n",
        "                        padding=get_same_padding(scale),\n",
        "                        groups=3 * total_dim,\n",
        "                        bias=use_bias[0],\n",
        "                    ),\n",
        "                    nn.Conv2d(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0]),\n",
        "                )\n",
        "                for scale in scales\n",
        "            ]\n",
        "        )\n",
        "        self.kernel_func = build_act(kernel_func, inplace=False)\n",
        "\n",
        "        self.proj = ConvLayer(\n",
        "            total_dim * (1 + len(scales)),\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def relu_linear_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
        "        B, _, H, W = list(qkv.size())\n",
        "\n",
        "        if qkv.dtype == torch.float16:\n",
        "            qkv = qkv.float()\n",
        "\n",
        "        qkv = torch.reshape(\n",
        "            qkv,\n",
        "            (\n",
        "                B,\n",
        "                -1,\n",
        "                3 * self.dim,\n",
        "                H * W,\n",
        "            ),\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[:, :, 0 : self.dim],\n",
        "            qkv[:, :, self.dim : 2 * self.dim],\n",
        "            qkv[:, :, 2 * self.dim :],\n",
        "        )\n",
        "\n",
        "        # lightweight linear attention\n",
        "        q = self.kernel_func(q)\n",
        "        k = self.kernel_func(k)\n",
        "\n",
        "        # linear matmul\n",
        "        trans_k = k.transpose(-1, -2)\n",
        "\n",
        "        v = F.pad(v, (0, 0, 0, 1), mode=\"constant\", value=1)\n",
        "        vk = torch.matmul(v, trans_k)\n",
        "        out = torch.matmul(vk, q)\n",
        "        if out.dtype == torch.bfloat16:\n",
        "            out = out.float()\n",
        "        out = out[:, :, :-1] / (out[:, :, -1:] + self.eps)\n",
        "\n",
        "        out = torch.reshape(out, (B, -1, H, W))\n",
        "        return out\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def relu_quadratic_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
        "        B, _, H, W = list(qkv.size())\n",
        "\n",
        "        qkv = torch.reshape(\n",
        "            qkv,\n",
        "            (\n",
        "                B,\n",
        "                -1,\n",
        "                3 * self.dim,\n",
        "                H * W,\n",
        "            ),\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[:, :, 0 : self.dim],\n",
        "            qkv[:, :, self.dim : 2 * self.dim],\n",
        "            qkv[:, :, 2 * self.dim :],\n",
        "        )\n",
        "\n",
        "        q = self.kernel_func(q)\n",
        "        k = self.kernel_func(k)\n",
        "\n",
        "        att_map = torch.matmul(k.transpose(-1, -2), q)  # b h n n\n",
        "        original_dtype = att_map.dtype\n",
        "        if original_dtype in [torch.float16, torch.bfloat16]:\n",
        "            att_map = att_map.float()\n",
        "        att_map = att_map / (torch.sum(att_map, dim=2, keepdim=True) + self.eps)  # b h n n\n",
        "        att_map = att_map.to(original_dtype)\n",
        "        out = torch.matmul(v, att_map)  # b h d n\n",
        "\n",
        "        out = torch.reshape(out, (B, -1, H, W))\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # generate multi-scale q, k, v\n",
        "        qkv = self.qkv(x)\n",
        "        multi_scale_qkv = [qkv]\n",
        "        for op in self.aggreg:\n",
        "            multi_scale_qkv.append(op(qkv))\n",
        "        qkv = torch.cat(multi_scale_qkv, dim=1)\n",
        "\n",
        "        H, W = list(qkv.size())[-2:]\n",
        "        if H * W > self.dim:\n",
        "            out = self.relu_linear_att(qkv).to(qkv.dtype)\n",
        "        else:\n",
        "            out = self.relu_quadratic_att(qkv)\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        heads_ratio: float = 1.0,\n",
        "        dim=32,\n",
        "        expand_ratio: float = 4,\n",
        "        scales: tuple[int, ...] = (5,),\n",
        "        norm: str = \"bn2d\",\n",
        "        act_func: str = \"hswish\",\n",
        "        context_module: str = \"LiteMLA\",\n",
        "        local_module: str = \"MBConv\",\n",
        "    ):\n",
        "        super(EfficientViTBlock, self).__init__()\n",
        "        if context_module == \"LiteMLA\":\n",
        "            self.context_module = ResidualBlock(\n",
        "                LiteMLA(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    heads_ratio=heads_ratio,\n",
        "                    dim=dim,\n",
        "                    norm=(None, norm),\n",
        "                    scales=scales,\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"context_module {context_module} is not supported\")\n",
        "        if local_module == \"MBConv\":\n",
        "            self.local_module = ResidualBlock(\n",
        "                MBConv(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    expand_ratio=expand_ratio,\n",
        "                    use_bias=(True, True, False),\n",
        "                    norm=(None, None, norm),\n",
        "                    act_func=(act_func, act_func, None),\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        elif local_module == \"GLUMBConv\":\n",
        "            self.local_module = ResidualBlock(\n",
        "                GLUMBConv(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    expand_ratio=expand_ratio,\n",
        "                    use_bias=(True, True, False),\n",
        "                    norm=(None, None, norm),\n",
        "                    act_func=(act_func, act_func, None),\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(f\"local_module {local_module} is not supported\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.context_module(x)\n",
        "        x = self.local_module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Functional Blocks                                 #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        main: Optional[nn.Module],\n",
        "        shortcut: Optional[nn.Module],\n",
        "        post_act=None,\n",
        "        pre_norm: Optional[nn.Module] = None,\n",
        "    ):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        self.main = main\n",
        "        self.shortcut = shortcut\n",
        "        self.post_act = build_act(post_act)\n",
        "\n",
        "    def forward_main(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.pre_norm is None:\n",
        "            return self.main(x)\n",
        "        else:\n",
        "            return self.main(self.pre_norm(x))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.main is None:\n",
        "            res = x\n",
        "        elif self.shortcut is None:\n",
        "            res = self.forward_main(x)\n",
        "        else:\n",
        "            res = self.forward_main(x) + self.shortcut(x)\n",
        "            if self.post_act:\n",
        "                res = self.post_act(res)\n",
        "        return res\n",
        "\n",
        "\n",
        "class DAGBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inputs: dict[str, nn.Module],\n",
        "        merge: str,\n",
        "        post_input: Optional[nn.Module],\n",
        "        middle: nn.Module,\n",
        "        outputs: dict[str, nn.Module],\n",
        "    ):\n",
        "        super(DAGBlock, self).__init__()\n",
        "\n",
        "        self.input_keys = list(inputs.keys())\n",
        "        self.input_ops = nn.ModuleList(list(inputs.values()))\n",
        "        self.merge = merge\n",
        "        self.post_input = post_input\n",
        "\n",
        "        self.middle = middle\n",
        "\n",
        "        self.output_keys = list(outputs.keys())\n",
        "        self.output_ops = nn.ModuleList(list(outputs.values()))\n",
        "\n",
        "    def forward(self, feature_dict: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
        "        feat = [op(feature_dict[key]) for key, op in zip(self.input_keys, self.input_ops)]\n",
        "        if self.merge == \"add\":\n",
        "            feat = list_sum(feat)\n",
        "        elif self.merge == \"cat\":\n",
        "            feat = torch.concat(feat, dim=1)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        if self.post_input is not None:\n",
        "            feat = self.post_input(feat)\n",
        "        feat = self.middle(feat)\n",
        "        for key, op in zip(self.output_keys, self.output_ops):\n",
        "            feature_dict[key] = op(feat)\n",
        "        return feature_dict\n",
        "\n",
        "\n",
        "class OpSequential(nn.Module):\n",
        "    def __init__(self, op_list: list[Optional[nn.Module]]):\n",
        "        super(OpSequential, self).__init__()\n",
        "        valid_op_list = []\n",
        "        for op in op_list:\n",
        "            if op is not None:\n",
        "                valid_op_list.append(op)\n",
        "        self.op_list = nn.ModuleList(valid_op_list)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for op in self.op_list:\n",
        "            x = op(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EfX78Q2-Sqaz"
      },
      "outputs": [],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from omegaconf import MISSING, OmegaConf\n",
        "\n",
        "from efficientvit.models.nn.act import build_act\n",
        "from efficientvit.models.nn.norm import build_norm\n",
        "from efficientvit.models.nn.ops import (\n",
        "    ChannelDuplicatingPixelUnshuffleUpSampleLayer,\n",
        "    ConvLayer,\n",
        "    ConvPixelShuffleUpSampleLayer,\n",
        "    ConvPixelUnshuffleDownSampleLayer,\n",
        "    EfficientViTBlock,\n",
        "    IdentityLayer,\n",
        "    InterpolateConvUpSampleLayer,\n",
        "    OpSequential,\n",
        "    PixelUnshuffleChannelAveragingDownSampleLayer,\n",
        "    ResBlock,\n",
        "    ResidualBlock,\n",
        ")\n",
        "\n",
        "__all__ = [\"DCAE\", \"dc_ae_f32c32\", \"dc_ae_f64c128\", \"dc_ae_f128c512\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EncoderConfig:\n",
        "    in_channels: int = MISSING\n",
        "    latent_channels: int = MISSING\n",
        "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
        "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
        "    block_type: Any = \"ResBlock\"\n",
        "    norm: str = \"trms2d\"\n",
        "    act: str = \"silu\"\n",
        "    downsample_block_type: str = \"ConvPixelUnshuffle\"\n",
        "    downsample_match_channel: bool = True\n",
        "    downsample_shortcut: Optional[str] = \"averaging\"\n",
        "    out_norm: Optional[str] = None\n",
        "    out_act: Optional[str] = None\n",
        "    out_shortcut: Optional[str] = \"averaging\"\n",
        "    double_latent: bool = False\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DecoderConfig:\n",
        "    in_channels: int = MISSING\n",
        "    latent_channels: int = MISSING\n",
        "    in_shortcut: Optional[str] = \"duplicating\"\n",
        "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
        "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
        "    block_type: Any = \"ResBlock\"\n",
        "    norm: Any = \"trms2d\"\n",
        "    act: Any = \"silu\"\n",
        "    upsample_block_type: str = \"ConvPixelShuffle\"\n",
        "    upsample_match_channel: bool = True\n",
        "    upsample_shortcut: str = \"duplicating\"\n",
        "    out_norm: str = \"trms2d\"\n",
        "    out_act: str = \"relu\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DCAEConfig:\n",
        "    in_channels: int = 3\n",
        "    latent_channels: int = 32\n",
        "    encoder: EncoderConfig = field(\n",
        "        default_factory=lambda: EncoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
        "    )\n",
        "    decoder: DecoderConfig = field(\n",
        "        default_factory=lambda: DecoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
        "    )\n",
        "    use_quant_conv: bool = False\n",
        "\n",
        "    pretrained_path: Optional[str] = None\n",
        "    pretrained_source: str = \"dc-ae\"\n",
        "\n",
        "    scaling_factor: Optional[float] = None\n",
        "\n",
        "\n",
        "def build_block(\n",
        "    block_type: str, in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str]\n",
        ") -> nn.Module:\n",
        "    if block_type == \"ResBlock\":\n",
        "        assert in_channels == out_channels\n",
        "        main_block = ResBlock(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            use_bias=(True, False),\n",
        "            norm=(None, norm),\n",
        "            act_func=(act, None),\n",
        "        )\n",
        "        block = ResidualBlock(main_block, IdentityLayer())\n",
        "    elif block_type == \"EViT_GLU\":\n",
        "        assert in_channels == out_channels\n",
        "        block = EfficientViTBlock(in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=())\n",
        "    elif block_type == \"EViTS5_GLU\":\n",
        "        assert in_channels == out_channels\n",
        "        block = EfficientViTBlock(in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(5,))\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_stage_main(\n",
        "    width: int, depth: int, block_type: str | list[str], norm: str, act: str, input_width: int\n",
        ") -> list[nn.Module]:\n",
        "    assert isinstance(block_type, str) or (isinstance(block_type, list) and depth == len(block_type))\n",
        "    stage = []\n",
        "    for d in range(depth):\n",
        "        current_block_type = block_type[d] if isinstance(block_type, list) else block_type\n",
        "        block = build_block(\n",
        "            block_type=current_block_type,\n",
        "            in_channels=width if d > 0 else input_width,\n",
        "            out_channels=width,\n",
        "            norm=norm,\n",
        "            act=act,\n",
        "        )\n",
        "        stage.append(block)\n",
        "    return stage\n",
        "\n",
        "\n",
        "def build_downsample_block(block_type: str, in_channels: int, out_channels: int, shortcut: Optional[str]) -> nn.Module:\n",
        "    if block_type == \"Conv\":\n",
        "        block = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "    elif block_type == \"ConvPixelUnshuffle\":\n",
        "        block = ConvPixelUnshuffleDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported for downsampling\")\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"averaging\":\n",
        "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=2\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for downsample\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_upsample_block(block_type: str, in_channels: int, out_channels: int, shortcut: Optional[str]) -> nn.Module:\n",
        "    if block_type == \"ConvPixelShuffle\":\n",
        "        block = ConvPixelShuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    elif block_type == \"InterpolateConv\":\n",
        "        block = InterpolateConvUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported for upsampling\")\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"duplicating\":\n",
        "        shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=2\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for upsample\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_encoder_project_in_block(in_channels: int, out_channels: int, factor: int, downsample_block_type: str):\n",
        "    if factor == 1:\n",
        "        block = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "    elif factor == 2:\n",
        "        block = build_downsample_block(\n",
        "            block_type=downsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"downsample factor {factor} is not supported for encoder project in\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_encoder_project_out_block(\n",
        "    in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str], shortcut: Optional[str]\n",
        "):\n",
        "    block = OpSequential(\n",
        "        [\n",
        "            build_norm(norm),\n",
        "            build_act(act),\n",
        "            ConvLayer(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                use_bias=True,\n",
        "                norm=None,\n",
        "                act_func=None,\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"averaging\":\n",
        "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for encoder project out\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_decoder_project_in_block(in_channels: int, out_channels: int, shortcut: Optional[str]):\n",
        "    block = ConvLayer(\n",
        "        in_channels=in_channels,\n",
        "        out_channels=out_channels,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        use_bias=True,\n",
        "        norm=None,\n",
        "        act_func=None,\n",
        "    )\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"duplicating\":\n",
        "        shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for decoder project in\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_decoder_project_out_block(\n",
        "    in_channels: int, out_channels: int, factor: int, upsample_block_type: str, norm: Optional[str], act: Optional[str]\n",
        "):\n",
        "    layers: list[nn.Module] = [\n",
        "        build_norm(norm, in_channels),\n",
        "        build_act(act),\n",
        "    ]\n",
        "    if factor == 1:\n",
        "        layers.append(\n",
        "            ConvLayer(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                use_bias=True,\n",
        "                norm=None,\n",
        "                act_func=None,\n",
        "            )\n",
        "        )\n",
        "    elif factor == 2:\n",
        "        layers.append(\n",
        "            build_upsample_block(\n",
        "                block_type=upsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"upsample factor {factor} is not supported for decoder project out\")\n",
        "    return OpSequential(layers)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, cfg: EncoderConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        num_stages = len(cfg.width_list)\n",
        "        self.num_stages = num_stages\n",
        "        assert len(cfg.depth_list) == num_stages\n",
        "        assert len(cfg.width_list) == num_stages\n",
        "        assert isinstance(cfg.block_type, str) or (\n",
        "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
        "        )\n",
        "\n",
        "        self.project_in = build_encoder_project_in_block(\n",
        "            in_channels=cfg.in_channels,\n",
        "            out_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
        "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
        "            downsample_block_type=cfg.downsample_block_type,\n",
        "        )\n",
        "\n",
        "        self.stages: list[OpSequential] = []\n",
        "        for stage_id, (width, depth) in enumerate(zip(cfg.width_list, cfg.depth_list)):\n",
        "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
        "            stage = build_stage_main(\n",
        "                width=width, depth=depth, block_type=block_type, norm=cfg.norm, act=cfg.act, input_width=width\n",
        "            )\n",
        "\n",
        "            if stage_id < num_stages - 1 and depth > 0:\n",
        "                downsample_block = build_downsample_block(\n",
        "                    block_type=cfg.downsample_block_type,\n",
        "                    in_channels=width,\n",
        "                    out_channels=cfg.width_list[stage_id + 1] if cfg.downsample_match_channel else width,\n",
        "                    shortcut=cfg.downsample_shortcut,\n",
        "                )\n",
        "                stage.append(downsample_block)\n",
        "            self.stages.append(OpSequential(stage))\n",
        "        self.stages = nn.ModuleList(self.stages)\n",
        "\n",
        "        self.project_out = build_encoder_project_out_block(\n",
        "            in_channels=cfg.width_list[-1],\n",
        "            out_channels=2 * cfg.latent_channels if cfg.double_latent else cfg.latent_channels,\n",
        "            norm=cfg.out_norm,\n",
        "            act=cfg.out_act,\n",
        "            shortcut=cfg.out_shortcut,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.project_in(x)\n",
        "        for stage in self.stages:\n",
        "            if len(stage.op_list) == 0:\n",
        "                continue\n",
        "            x = stage(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, cfg: DecoderConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        num_stages = len(cfg.width_list)\n",
        "        self.num_stages = num_stages\n",
        "        assert len(cfg.depth_list) == num_stages\n",
        "        assert len(cfg.width_list) == num_stages\n",
        "        assert isinstance(cfg.block_type, str) or (\n",
        "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
        "        )\n",
        "        assert isinstance(cfg.norm, str) or (isinstance(cfg.norm, list) and len(cfg.norm) == num_stages)\n",
        "        assert isinstance(cfg.act, str) or (isinstance(cfg.act, list) and len(cfg.act) == num_stages)\n",
        "\n",
        "        self.project_in = build_decoder_project_in_block(\n",
        "            in_channels=cfg.latent_channels,\n",
        "            out_channels=cfg.width_list[-1],\n",
        "            shortcut=cfg.in_shortcut,\n",
        "        )\n",
        "\n",
        "        self.stages: list[OpSequential] = []\n",
        "        for stage_id, (width, depth) in reversed(list(enumerate(zip(cfg.width_list, cfg.depth_list)))):\n",
        "            stage = []\n",
        "            if stage_id < num_stages - 1 and depth > 0:\n",
        "                upsample_block = build_upsample_block(\n",
        "                    block_type=cfg.upsample_block_type,\n",
        "                    in_channels=cfg.width_list[stage_id + 1],\n",
        "                    out_channels=width if cfg.upsample_match_channel else cfg.width_list[stage_id + 1],\n",
        "                    shortcut=cfg.upsample_shortcut,\n",
        "                )\n",
        "                stage.append(upsample_block)\n",
        "\n",
        "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
        "            norm = cfg.norm[stage_id] if isinstance(cfg.norm, list) else cfg.norm\n",
        "            act = cfg.act[stage_id] if isinstance(cfg.act, list) else cfg.act\n",
        "            stage.extend(\n",
        "                build_stage_main(\n",
        "                    width=width,\n",
        "                    depth=depth,\n",
        "                    block_type=block_type,\n",
        "                    norm=norm,\n",
        "                    act=act,\n",
        "                    input_width=(\n",
        "                        width if cfg.upsample_match_channel else cfg.width_list[min(stage_id + 1, num_stages - 1)]\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "            self.stages.insert(0, OpSequential(stage))\n",
        "        self.stages = nn.ModuleList(self.stages)\n",
        "\n",
        "        self.project_out = build_decoder_project_out_block(\n",
        "            in_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
        "            out_channels=cfg.in_channels,\n",
        "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
        "            upsample_block_type=cfg.upsample_block_type,\n",
        "            norm=cfg.out_norm,\n",
        "            act=cfg.out_act,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.project_in(x)\n",
        "        for stage in reversed(self.stages):\n",
        "            if len(stage.op_list) == 0:\n",
        "                continue\n",
        "            x = stage(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, cfg: DCAEConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.encoder = Encoder(cfg.encoder)\n",
        "        self.decoder = Decoder(cfg.decoder)\n",
        "\n",
        "        if self.cfg.pretrained_path is not None:\n",
        "            self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.cfg.pretrained_source == \"dc-ae\":\n",
        "            state_dict = torch.load(self.cfg.pretrained_path, map_location=\"cpu\", weights_only=True)[\"state_dict\"]\n",
        "            self.load_state_dict(state_dict)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def spatial_compression_ratio(self) -> int:\n",
        "        return 2 ** (self.decoder.num_stages - 1)\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.encoder(x)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor, global_step: int) -> torch.Tensor:\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x, torch.tensor(0), {}\n",
        "\n",
        "\n",
        "def dc_ae_f32c32(name: str, pretrained_path: str) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f32c32-in-1.0\", \"dc-ae-f32c32-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=32 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[0,4,8,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[0,5,10,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu]\"\n",
        "        )\n",
        "    elif name in [\"dc-ae-f32c32-sana-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=32 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[2,2,2,3,3,3] \"\n",
        "            \"encoder.downsample_block_type=Conv \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[3,3,3,3,3,3] \"\n",
        "            \"decoder.upsample_block_type=InterpolateConv \"\n",
        "            \"decoder.norm=trms2d decoder.act=silu \"\n",
        "            \"scaling_factor=0.41407\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def dc_ae_f64c128(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f64c128-in-1.0\", \"dc-ae-f64c128-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=128 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024,2048] encoder.depth_list=[0,4,8,2,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024,2048] decoder.depth_list=[0,5,10,2,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu,silu]\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def dc_ae_f128c512(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f128c512-in-1.0\", \"dc-ae-f128c512-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=512 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024,2048,2048] encoder.depth_list=[0,4,8,2,2,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024,2048,2048] decoder.depth_list=[0,5,10,2,2,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu,silu,silu]\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsgTY2id2HLR"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYMQDoL578HQ",
        "outputId": "c53aa10d-d4e5-4a46-ad6f-9a2cafbdfea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12, 16, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        if in_ch==out_ch: self.func = lambda x: x\n",
        "        if in_ch > out_ch:\n",
        "            # self.func = lambda x: F.interpolate(x.flatten(2).transpose(1,2), size=out_ch, mode='nearest-exact').transpose(1,2).unflatten(-1,(x.shape[-2:])) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "            # self.func = lambda x: F.adaptive_avg_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "            self.func = lambda x: F.adaptive_max_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        elif in_ch < out_ch:\n",
        "            # self.func = lambda x: F.interpolate(x.flatten(2).transpose(1,2), size=out_ch, mode='nearest-exact').transpose(1,2).unflatten(-1,(x.shape[-2:])) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "            self.func = lambda x: F.adaptive_avg_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "            # self.func = lambda x: F.adaptive_max_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "\n",
        "    def forward(self, x): return self.func(x) # [b,c,h,w] -> [b,o,h,w]\n",
        "\n",
        "class PixelShortcut(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(SameCh(in_ch, out_ch*r**2), nn.PixelShuffle(r)) #\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SameCh(in_ch*r**2, out_ch)) #\n",
        "        else: self.net = SameCh(in_ch, out_ch)\n",
        "    def forward(self, x): return self.net(x) # [b,c,h,w] -> [b,o,r*h,r*w]\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.net = nn.Sequential(SeparableConv2d(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.Conv2d(in_ch, out_ch*r**2, 1, bias=False), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(MultiConv(in_ch, out_ch*r**2, (3,7)), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # if self.r>1: self.net = nn.Sequential(MultiConv(in_ch, in_ch, (3,7), groups=in_ch, bias=False), nn.Conv2d(in_ch, out_ch*r**2, 1, bias=False), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, 7, 1, 7//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        if self.r>1: self.net = nn.Sequential(UIB(in_ch, out_ch*r**2, r=r), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(UIB(in_ch, in_ch), UIB(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SeparableConv2d(in_ch*r**2, out_ch)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(SeparableConv2d(in_ch, out_ch//r**2), nn.PixelUnshuffle(r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, 1, bias=False), nn.Conv2d(out_ch, out_ch, 3, 1, 3//2, groups=out_ch, bias=False))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), MultiConv(in_ch*r**2, out_ch, (3,7))) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, 1, bias=False), MultiConv(out_ch, out_ch, (3,7), groups=out_ch, bias=False))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, 7, 1, 7//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), UIB(in_ch*r**2, out_ch, r=r))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), UIB(in_ch*r**2, in_ch), UIB(in_ch, out_ch))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch))\n",
        "\n",
        "\n",
        "\n",
        "        # if self.r>1: self.net = nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), out_r=r), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), in_r=r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        else: self.net = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2)\n",
        "        # self.net.apply(self.init_conv_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1, kernel=3):\n",
        "        super().__init__()\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=r)\n",
        "    def forward(self, x):\n",
        "        # print('UpDownBlock', x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        # print(self.block, self.shortcut_block)\n",
        "        # return self.block(x) #+ self.shortcut_block(x)\n",
        "        return self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1, kernel=3):\n",
        "        super().__init__()\n",
        "        # self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        act = nn.SiLU()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # print('UpDownBlock', x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        # print(self.block, self.shortcut_block)\n",
        "        out = self.block(x)\n",
        "        # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        return out + shortcut\n",
        "        # return shortcut\n",
        "\n",
        "# if out>in, inter=max=near. ave=ave\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "# model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}