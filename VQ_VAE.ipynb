{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/Autoencoder/blob/main/VQ_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghQ8RSExs_A4",
        "outputId": "ff40c080-1afb-4443-88c7-a6ff004493d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY\n",
            "From (redirected): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY&confirm=t&uuid=56fcaa8f-6b05-4cd7-b044-547f98cf8ffa\n",
            "To: /content/buffer512.pkl\n",
            "100% 706M/706M [00:05<00:00, 121MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title gdown\n",
        "import pickle\n",
        "!gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UKkkuorG_b9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        # self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        state, action, reward = self.data[idx]\n",
        "        state = self.transform(state)\n",
        "        return state\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 # 128 512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mje-yFj88WlY",
        "outputId": "efe259a6-9702-46e9-9039-e1e6acfa9d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-1.0000, -1.0000, -1.0000, -1.0000],\n",
            "        [-0.5000, -0.3333, -1.0000, -1.0000],\n",
            "        [-0.5000, -0.3333,  0.0000, -1.0000],\n",
            "        [ 0.0000, -0.3333,  0.0000, -1.0000],\n",
            "        [ 0.5000,  0.3333,  0.0000,  1.0000],\n",
            "        [ 0.5000,  0.3333,  1.0000,  1.0000],\n",
            "        [ 1.0000,  1.0000,  1.0000,  1.0000]], device='cuda:0')\n",
            "tensor([  0.,  30.,  32.,  56.,  87.,  89., 119.], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "fsq = FSQ(levels = [5,4,3,2])\n",
        "# print(fsq.codebook)\n",
        "batch_size, seq_len = 2, 4\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "x = torch.linspace(-2,2,7, device=device).repeat(4,1).T\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "lact = fsq.codes_to_indexes(la)\n",
        "print(lact)\n",
        "# la = fsq.indexes_to_codes(lact)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxGF8bYeGDA2",
        "outputId": "5dffa40b-e0f9-4e81-b050-bc2f744abe62",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 16, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title UIB\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class UIB(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, mult=4):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential( # norm,act,conv\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, in_ch, kernel, 1, kernel//2, groups=in_ch, bias=False),\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, mult*in_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, nn.Conv2d(mult*in_ch, mult*in_ch, kernel, 1, kernel//2, groups=mult*in_ch, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, zero_module(nn.Conv2d(mult*in_ch, out_ch, 1, bias=False)),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UIB(in_ch, out_ch)\n",
        "x = torch.rand(128, in_ch, 64, 64)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nu4Dzma_cD5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.res_conv = zero_module(nn.Conv2d(in_ch, out_ch, 1)) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        return self.block(x) + self.res_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjvZZswH1_KR",
        "outputId": "e470abf0-fba8-4f34-fca1-3a8d137095de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 16, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # if self.r>1: self.net = nn.Sequential(UIB(in_ch, out_ch*r**2, kernel), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), UIB(in_ch*r**2, out_ch, kernel))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Interpolate(nn.Module):\n",
        "    def __init__(self, scale_factor=2, mode=\"nearest-exact\", **kwargs):\n",
        "        super().__init__()\n",
        "        self.kwargs = kwargs\n",
        "    def forward(self, x):\n",
        "        return F.interpolate(x, scale_factor=2, mode=\"nearest-exact\", **self.kwargs)\n",
        "        # return F.adaptive_avg_pool2d(x, (x.shape[2]*2, x.shape[3]*2))\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # self.block = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        # )\n",
        "        if self.r>1: self.res_conv = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.res_conv = nn.Sequential(Interpolate() nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity())\n",
        "\n",
        "        elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.MaxPool2d(2,2))\n",
        "        # elif self.r<1: self.res_conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity(), nn.AvgPool2d(2,2))\n",
        "\n",
        "        else: self.res_conv = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        out = self.block(x)\n",
        "        # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        # return out #+ shortcut\n",
        "        return self.res_conv(x)\n",
        "\n",
        "# if out>in, inter=max=near. ave=ave\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# stride2\n",
        "# interconv/convpool\n",
        "# pixelconv\n",
        "# pixeluib\n",
        "# pixelres\n",
        "# shortcut\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "# model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CtmsyKatYT7h"
      },
      "outputs": [],
      "source": [
        "# @title AttentionBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        # self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        # self.lin = nn.Sequential(nn.Dropout(dropout), zero_module(nn.Linear(d_model, d_model)))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, mult=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0) # 16448\n",
        "        # self.self = Pooling()\n",
        "        act = nn.GELU() # ReLU GELU\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     *[nn.BatchNorm2d(d_model), act, SeparableConv2d(d_model, d_model),]*3\n",
        "        #     )\n",
        "        # self.ff = ResBlock(d_model) # 74112\n",
        "        self.ff = UIB(d_model, mult=4) # uib m4 36992, m2 18944\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # print('attnblk fwd',x.shape)\n",
        "\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.drop(self.self(self.norm(x)))\n",
        "        x = x.transpose(1,2).reshape(*bchw)\n",
        "        x = x + self.ff(x)\n",
        "        # x = self.ff(x)\n",
        "        # x = x + self.drop(self.norm2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "# pos_emb rope < learn < learned\n",
        "# conv > pixel?\n",
        "# droppath not required\n",
        "\n",
        "# norm,act,conv < conv,norm,act\n",
        "# 2*s1 < uib < resblock\n",
        "# gatedadaln 3 < 2 = 1 < ffmult4 = 2*gatedadaln\n",
        "# MaxPool2d(2,2) < MaxPool2d(3,2,3//2)\n",
        "\n",
        "\n",
        "# dim = 64\n",
        "# dim_head = 8\n",
        "# heads = dim // dim_head\n",
        "# model = AttentionBlock(d_model, d_head)\n",
        "# # print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 59850\n",
        "# # print(images.shape) # [batch, 3, 32, 32]\n",
        "# x = torch.rand(64, 3, 32, 32, device=device)\n",
        "# # x = torch.rand(64, 3, 28,28, device=device)\n",
        "# logits = model(x)\n",
        "# print(logits.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O9NnnDvZI37",
        "outputId": "07793067-71ae-41e5-a097-c7a2bf66b532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43919\n",
            "torch.Size([64, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title ae\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class Res(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "    def forward(self, x): return x + self.model(x)\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, depth=4, num_res_blocks=1, n_head=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch # z_channels z_dim\n",
        "        # n_head = d_model // d_head\n",
        "        mult = [1,1,1,1] # [1,2,3,4] [1,1,1,1] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "        self.encoder = nn.Sequential(\n",
        "            # ResBlock(in_ch),\n",
        "            UpDownBlock(in_ch, ch_list[0], r=1/2),\n",
        "            # AttentionBlock(ch_list[0], d_head=d_head),\n",
        "            # AttentionBlock(ch_list[0], d_head=d_head),\n",
        "            # ResBlock(ch_list[0]),\n",
        "            # Res(UIB(ch_list[0], mult=4)),\n",
        "            UpDownBlock(ch_list[0], out_ch, r=1/2),\n",
        "            # ResBlock(out_ch),\n",
        "            # Res(UIB(out_ch, mult=4)),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            # ResBlock(out_ch),\n",
        "            # Res(UIB(out_ch, mult=4)),\n",
        "            UpDownBlock(out_ch, ch_list[0], r=2),\n",
        "            # AttentionBlock(ch_list[0], d_head=d_head),\n",
        "            # AttentionBlock(ch_list[0], d_head=d_head),\n",
        "            # ResBlock(ch_list[0]),\n",
        "            # Res(UIB(ch_list[0], mult=4)),\n",
        "            UpDownBlock(ch_list[0], in_ch, r=2),\n",
        "            # ResBlock(in_ch),\n",
        "        )\n",
        "        # self.vq = FSQ(levels = out_ch*[1024])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.quantise(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.quantise(x)\n",
        "    def decode(self, x):\n",
        "        x = self.quantise(x)\n",
        "        return self.decoder(x)\n",
        "    def quantise(self, x): # [b,c,h,w]->[b,h,w,c]->[b,c,h,w]\n",
        "        # return self.vq(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
        "        return x\n",
        "\n",
        "# split plateaus\n",
        "# conv 3<7\n",
        "# stride2 < pixel\n",
        "# convres\n",
        "\n",
        "batch=64\n",
        "in_ch=3\n",
        "z_dim=3\n",
        "h,w = 64,64\n",
        "model = VQVAE(in_ch, d_model=16, out_ch=z_dim, depth=4, num_res_blocks=1, n_head=-1, d_head=4).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "x = torch.rand((batch, in_ch, h, w), device=device)\n",
        "out = model.encode(x)\n",
        "# print(out.shape)\n",
        "# out = torch.rand((batch, in_ch, h, w), device=device)\n",
        "x_ = model.decode(out)\n",
        "# x = model.quantise(x)\n",
        "print(x_.shape)\n",
        "# # print(x)\n",
        "# x_ = model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-VeKdVamz2w"
      },
      "outputs": [],
      "source": [
        "# conv = nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2)\n",
        "# print(conv.weight.data.shape)\n",
        "print(model.encoder[0].block[-1].net[-1].gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WuXDKUIACl91"
      },
      "outputs": [],
      "source": [
        "# @title VQVAE me\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, depth=4, num_res_blocks=1, n_head=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch # z_channels z_dim\n",
        "        # n_head = d_model // d_head\n",
        "        # self.vq = VectorQuantizerEMA(num_emb=8192, emb_dim=out_ch, beta=0.5) # chat gpt\n",
        "        # self.vq = FSQ(levels = z_dim*[32])\n",
        "\n",
        "        mult = [1,1,1,1]\n",
        "        # mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            # nn.Conv2d(in_ch, ch_list[0], 3, 1, padding=3//2),\n",
        "\n",
        "            ResBlock(in_ch, ch_list[0]),\n",
        "            # ResBlock(ch_list[0], ch_list[1]),\n",
        "            # ResBlock(ch_list[1], ch_list[1]),\n",
        "            AttentionBlock(ch_list[0], d_head),\n",
        "\n",
        "            # nn.PixelUnshuffle(2),\n",
        "            # ResBlock(ch_list[0]*2**2, ch_list[1]),\n",
        "            # nn.AvgPool2d(2,2),\n",
        "            # nn.MaxPool2d(2,2),\n",
        "            UpDownBlock(3, ch_list[0], r=1/2),\n",
        "\n",
        "            ResBlock(ch_list[0], ch_list[1]),\n",
        "            # ResBlock(ch_list[2], ch_list[2]),\n",
        "            AttentionBlock(ch_list[1], d_head),\n",
        "            # ResBlock(ch_list[1], ch_list[2]),\n",
        "\n",
        "            # nn.PixelUnshuffle(2),\n",
        "            # ResBlock(ch_list[2]*2**2, ch_list[3]),\n",
        "            # nn.AvgPool2d(2,2),\n",
        "            UpDownBlock(ch_list[0], out_ch, r=1/2),\n",
        "\n",
        "            ResBlock(ch_list[2], ch_list[3]),\n",
        "            AttentionBlock(ch_list[3], d_head),\n",
        "            # ResBlock(ch_list[3], ch_list[3]),\n",
        "            ResBlock(ch_list[3], out_ch),\n",
        "\n",
        "            # # nn.GroupNorm(32, ch_list[-1]), nn.SiLU(), nn.Conv2d(ch_list[-1], out_ch, 3, 1, padding=3//2)\n",
        "            # nn.BatchNorm2d(ch_list[-1]), nn.SiLU(), nn.Conv2d(ch_list[-1], out_ch, 3, 1, padding=3//2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # nn.Conv2d(out_ch, ch_list[-1], 3, 1, padding=3//2),\n",
        "\n",
        "            ResBlock(out_ch, ch_list[3]),\n",
        "            # ResBlock(ch_list[3], ch_list[3]),\n",
        "            AttentionBlock(ch_list[3], d_head),\n",
        "            ResBlock(ch_list[3], ch_list[2]),\n",
        "            # nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            UpDownBlock(out_ch, ch_list[0], r=2),\n",
        "\n",
        "            # ResBlock(ch_list[3], ch_list[2]*2**2),\n",
        "            # # AttentionBlock(ch_list[2]*2**2, d_head),\n",
        "            # nn.PixelShuffle(2),\n",
        "\n",
        "            ResBlock(ch_list[2], ch_list[1]),\n",
        "            AttentionBlock(ch_list[2], d_head),\n",
        "            # ResBlock(ch_list[1], ch_list[0]),\n",
        "            # nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            UpDownBlock(ch_list[0], in_ch, r=2),\n",
        "            # ResBlock(ch_list[1], ch_list[0]*2**2),\n",
        "            # nn.PixelShuffle(2),\n",
        "\n",
        "            ResBlock(ch_list[1], ch_list[0]),\n",
        "            AttentionBlock(ch_list[0], d_head),\n",
        "            ResBlock(ch_list[0], in_ch),\n",
        "\n",
        "            # # nn.GroupNorm(32, ch_list[0]), nn.SiLU(), nn.Conv2d(ch_list[0], in_ch, 3, 1, padding=3//2)\n",
        "            # nn.BatchNorm2d(ch_list[0]), nn.SiLU(), zero_module(nn.Conv2d(ch_list[0], in_ch, 3, 1, padding=3//2)) # zero\n",
        "        )\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x = self.encoder(x)\n",
        "    #     # print(x.shape)\n",
        "    #     commitment_loss, x, _ = self.vq(x)\n",
        "    #     # print(x.shape)\n",
        "    #     x = self.decoder(x)\n",
        "    #     return x, commitment_loss\n",
        "\n",
        "    # def encode(self, x):\n",
        "    #     x = self.encoder(x)\n",
        "    #     _, x, _ = self.vq(x)\n",
        "    #     return x\n",
        "\n",
        "    # def decode(self, x):\n",
        "    #     _, x, _ = self.vq(x)\n",
        "    #     return self.decoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # x = self.quantise(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # return self.quantise(x)\n",
        "        return x\n",
        "    def decode(self, x):\n",
        "        # _, x, _ = self.vq(x)\n",
        "        # x = self.quantise(x)\n",
        "        return self.decoder(x)\n",
        "    def quantise(self, x): # [b,c,h,w]->[b,h,w,c]->[b,c,h,w]\n",
        "        return self.vq(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
        "\n",
        "\n",
        "\n",
        "batch=2\n",
        "in_ch=3\n",
        "z_dim=3\n",
        "h,w = 64,64\n",
        "model = VQVAE(in_ch, d_model=16, out_ch=z_dim, depth=4, num_res_blocks=1, n_head=-1, d_head=4).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "x = torch.rand((batch, in_ch, h, w), device=device)\n",
        "# out, _ = model(x)\n",
        "# print(out.shape)\n",
        "# x = model.quantise(x)\n",
        "# print(x.shape)\n",
        "# print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7CtfbCdIpCG",
        "outputId": "9ae91b19-bd81-423b-aea0-667f3dc450f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "89100\n",
            "torch.Size([2, 3, 8, 8])\n",
            "torch.Size([2, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # encoder mult=[1,2,4,4,8,8] # depth_list=[0,4,8,2,2,2]\n",
        "        # decoder mult=[1,2,4,4,8,8] # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, width_list[0], 3, 2, padding=3//2),\n",
        "\n",
        "            UpDownBlock(width_list[0], width_list[0], r=1/2),\n",
        "            ResBlock(width_list[0]),\n",
        "            UpDownBlock(width_list[0], width_list[-1], r=1/2),\n",
        "            ResBlock(width_list[-1]),\n",
        "            AttentionBlock(width_list[-1], d_head=4),\n",
        "            UpDownBlock(width_list[-1], out_ch, r=1),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            UpDownBlock(out_ch, width_list[-1], r=1),\n",
        "            AttentionBlock(width_list[-1], d_head=4),\n",
        "            ResBlock(width_list[-1]),\n",
        "            UpDownBlock(width_list[-1], width_list[0], r=2),\n",
        "            ResBlock(width_list[0]),\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpDownBlock(width_list[0], width_list[0], r=2),\n",
        "            nn.ConvTranspose2d(width_list[0], in_ch, 3, 2, padding=3//2, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=16, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "sx = model.encoder(x)\n",
        "print(sx.shape)\n",
        "out = model.decoder(sx)\n",
        "# out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rJQDMix9EpYZ",
        "cellView": "form",
        "outputId": "705c80f1-fe4a-4ebe-af84-82a6ba8163f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0.013936544768512249\n",
            "0.0064633130095899105\n",
            "0.004799853079020977\n",
            "0.003942935727536678\n",
            "0.0035130612086504698\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0052362047..1.0508311].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.08883832..1.085736].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0.0033564239274710417\n",
            "0.0030573855619877577\n",
            "0.002910301787778735\n",
            "0.0029100344981998205\n",
            "0.0028732307255268097\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.034490407..1.0544688].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.08285265..1.0893917].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "0.0027025442104786634\n",
            "0.002752227708697319\n",
            "0.0026929909363389015\n",
            "0.0025778631679713726\n",
            "0.002675396855920553\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.019913796..1.055535].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.12980875..1.0115194].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0.0024853264912962914\n",
            "0.00241015269421041\n",
            "0.0024605384096503258\n",
            "0.002548074349761009\n",
            "0.0023233098909258842\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.014104944..1.0713092].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.120775074..1.0225575].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARYAAAEWCAYAAACjTbhPAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASjJJREFUeJztvXuQHNV5N/z0ZaZnr7PalbSrBa2QbUDCMrYRINbgxAE5KnwJBDmxXeQ1dlHhgywEUKXsUpXBgXK8xK4UhERAcBERV6woUb0FDv4+Q3hFLMopIUB+scHYMmASLUi7AqG978z05Xx/SMx5nl9rRhrRskbw/La2qntO9zmnT585c57b73GMMYYUCoUiQ7gnugMKheLdB11YFApF5tCFRaFQZA5dWBQKRebQhUWhUGQOXVgUCkXm0IVFoVBkDl1YFApF5tCFRaFQZA5dWBQKReY4bgvLhg0b6LTTTqNCoUCrVq2ip59++ng1pVAomgzO8YgV+td//Vf60pe+RPfddx+tWrWK7rrrLtqyZQvt2rWLFi5cWPfeJEloz5491NHRQY7jZN01hUJxjDDG0NTUFPX395PrHmFPYo4Dzj//fDM0NFQ9j+PY9Pf3m+Hh4SPeOzIyYohI//Vf/5v0f2Rk5IjfY58yRqVSoZ07d9L69eurn7muS6tXr6bt27enri+Xy1Qul6vn5tAG6o+3j1CuvZOIiHJ5e71bFreTl7PHjkmgdrnjceK4epyDsvbAHhdwo2TkB75rzydNJMpcqHcmZOdQ7zTZ/rrQRgjnxHZvUSTLRJtJLMrwBQc5Uz0uuDA+bPxyjvxFmiJ4TmPLZ2P49fJsvYagCC7NsWfJwSMHnnyfedansgM1s/EZL1WgrzgP7Hke+tNRsB8UjvCj7LP+jMM84HNmInRqFRER0Rx7FBcey+AIspsTkuPjsTJfTgPKw+6/zbf34hi0e3bW5Fj7pelJ+sbKJdTR0UFHQuYLy5tvvklxHFNvb6/4vLe3l371q1+lrh8eHqbbbrst9XmuvZPyHYdZWPLyukYWFrfOwhIc48JShgnlQb1RnYWlUmdhceosLG5GC0vQwMJSqbOwRBktLPkjLCwB71OdhSWfO/qFJYD+BMe4sAR1Fpb8ERaW+AQsLAFbWHAMCmxhyafeIB2ViuKEW4XWr19PExMT1f+RkZET3SWFQvEOkfmOZf78+eR5Ho2NjYnPx8bGqK+vL3V9EAQU8O3C2x1rOfhPROSwXUkBNyXsCcIQV1d5LhdaWca34S78WudAv50ktrwFfjnDBH4ZWDsh9N1n9yZQhip1/suVGPw9sGUoTrjwnD67FfVv7EeMHNzRQT0Ru9aHzsfsl7MCv24GHsxlv23wI0s56B+vKQ/1RKxND+cIjKXPPkDRlZd50Hcf6k3Yu26BrUaZi1uwk8ZpijsqDpwHLqsrTmB+8+ugSt+Dc9bfvA9zhJ3HbK4nuN2sg8x3LPl8nlauXElbt261HUoS2rp1Kw0ODmbdnEKhaEJkvmMhIlq3bh1dddVVdO6559L5559Pd911F83MzNBXvvKV49GcQqFoMhyXheXzn/88vfHGG3TrrbfS6OgofeQjH6FHH300pdCtB/dASG4YEhFRiYkmcYfc05kKExFw219HvPBgjzkX2qFwYFtrYLvscaUriE0OiEIO7wOKQmyvn0AbFVSssm2vgb1+HPExkP3xQL6YM3b8Wh1ZD7cStbryRhS/QqarnMMyNnx+QbaPYgl/bh/FrVi+ayGZwFY/iRN2HWzEY3mxy547D+JEyNpMiVvQZo4Jbz689zIbWpPI5whjWa/HDBIG3lcIYxKzdkrwrt2ID7zsTwGexTCxxgNRaKpkX67HZN7SbImOFsdlYSEiuv766+n6668/XtUrFIomxgm3CikUincfdGFRKBSZ47iJQu8UnhuT97acz0xsUQlNv9wBDcxvYMZzmVk0TJkZrXAbeVAGuggu7HuuLHOT2m0mkVzH+aUhyNYJmCi52S+C5zTMQc1F028i24yZ/igHcngLe04DY4DCf1jhDnLSOazCfq+g+ZQuiZtPI9Ar5WDcI+EgB/1hY+vi2IEDX8KeJYL+GFZvnMN5AKZy1t889HWWP3gkxycBnQ+/NIYyU+dZcI6IeQC6GdS9UWSfM/VdYDogj+nkfAM26zrQHYtCocgcurAoFIrM0bSi0HjFpdzb2222JUWpJGJbV/S7Rc/NMjtv8eTVbazM81Ckgm0l24LOYX+gE1Nsa1uCresc281HYPYsg0gVse27WwGTOxd/QBTyQGRoaWUxRyg6siY9MLknIIJy83cYyv5U2L0VGbaTekcz7IXOgvi1KC/rLeT5gMmXy8Npp8rYihyDEhNhUOztZO8+hPeFITLcbFwBcYKbiWfQ5A/vqMxiieKUF7O8lxIrVuF3wWVzZLqCIpXsBBfdTIxmfuvq7lXsfeUy+kbXhu5YFApF5tCFRaFQZA5dWBQKReZoWh1LFBlyDiksYqYLCOfkdQlzR/ZzUAbhu9x8iWbQiOk0QqQXSWq7VVdAT1EGWo4S828vQz0z7FqMbi5BKHRMtW3ThvNwgCztowmX9R2DpPml6Bbvgp6CdyGC/kRMGYE6KFRUBEwf4wEhS4R6JlaXm9J7sZAHNLWm9HKGHUMoBZsYPujawLOAe0FQBdqcYbqlEuinSnDtLHu3GM2cQOd5NLYD9fAo6RzwFmG0esyrTdHp2EIe0NxAcLPuWBQKRfbQhUWhUGQOXVgUCkXmaFodS6kcU/w2cScLZUdfAo/pMFLcq8hWxp42Bvk5ZsSfBvQmFQK/Ec7mBo4rGPbOVS4RumtzBjnQzTigH/KYr4EPgyDUKEjNAHqdhMX+V6Dvc62sTqgI9R28WnApoZgJ8D4UIqery3RkyKPmBDi2jNkM6imzZ5mZw2eGinmbyMHLL8Ox8zy4lvsPyXr4FMLwCBPKaz2mrMG+Ytc5PQR+GfiXGdyHUu7/js9oQkCPk9Ty3G9gtdAdi0KhyBy6sCgUiszRtKKQR0mVqY2bd8MYtnR8bw3iDUbSJuzaVNQtE5vQcdmDrSKXsFJmT7jZ5cxmqQhm1ibQk8Vg23Rq77oFw50Ppch+x13G0dxsmFiJ/M6piGFmk09CuWHn4mAMMywGkYqzl6Uc8UEUEVJVJPsTMhf2BMzfOLac0NsHMSVk5yEwq3mELgCsHJnoWH8M9BUzhch5IMtSaU7Y+CVQxl0SMA0NqhBE1DTUU2Dn3MRsUI6tA92xKBSKzKELi0KhyBy6sCgUiszRtDqWyDhVBjifZdkyYIZNGGO9A5mgXIgr56k1UyzvTLaNQJZOUgxktrwMCo8QZO0KaxPdvg13r8cySMvJRW90+xau3ZiEDMbAZ/UmqMdxeH9QIQSmVq4biWQsRcR1WQnK5RiiX7MJiivyYUL2XpIyjA8zN8fQdWRh4y8f9UzCTIyhAGjjZmMbQ0VcN+EA8xq6M/AxilOaJgCrF3Nvc8K7PHxP8qB/FEnUQAc0wdw7fBZaUi6pjkWhUJxA6MKiUCgyR9OKQo5JyDm0XYvZfjlGT1e2JcckZAY9VHnOXswpzOzESR0S7oPltbeEGEkr5JZ6ZeiRikTJIqo15aNauwNoZhTiD4pmtW/EyFbD3oMHLgB8nMPUgMC4sy06JkUrR1Km4cx+yJ7G34kPdn0kzOYP6sBvqyDjA9HMwLzgXfCQ4Jxdi8TWeC4HHr2NcWw50J2CzwNoAvrOo+59MCPHJPQC9jDCd1kbumNRKBSZo+GF5cknn6TPfvaz1N/fT47j0MMPPyzKjTF066230qJFi6ilpYVWr15NL730Ulb9VSgUJwEaXlhmZmbowx/+MG3YsOGw5d/+9rfp7rvvpvvuu4927NhBbW1ttGbNGiqVjj7vq0KhOLnRsI7l0ksvpUsvvfSwZcYYuuuuu+jrX/86XXbZZURE9L3vfY96e3vp4Ycfpi984QtH3U5pLqboUMIyl5ls0TWZu1LHYGJDF3rBFgZmUMe3ArVB9+xU9C6rE9nlQHzmbtZovuQu6wlEvMZgSuRu8ik1Cqs4RllfXirM1hh+kGOydgSmewJGu1nmsl7B56rUjh7GjAeczS0GxYnbKk6lWRYj0NnAz4CZGiPQXfZ7ivoqP88Y5NB9AdI+cHeGlPaBvT90vU/FjLBzJzUP5Dl/92ji5vnnPPiitMDE5BHXBt6Jk2NsAuzJHB+/fLWRqY7l1VdfpdHRUVq9enX1s2KxSKtWraLt27cf9p5yuUyTk5PiX6FQnNzIdGEZHR0lIqLe3l7xeW9vb7UMMTw8TMVisfq/ePHiLLukUChOAE64VWj9+vU0MTFR/R8ZGTnRXVIoFO8Qmfqx9PX1ERHR2NgYLVq0qPr52NgYfeQjHznsPUEQUBAEqc9znqHcIb8F7kKfgKzocht82tddXstc6FHsTfmB8/uQioDd6yMjPPgL5LheAAVx5heAzHOYnY53waQSh7Nwea82gz+RdGHHMPgKc9l2IOOBjwxyjIrAoD6mrj4I/GO4qzm0YcCPhVeVwPviIRkO0LChzwunLTDox8LDEUCngExrXI2B9AucTsMD5ZqH05Q/C86DCN8nL8NUiGx+g+ORA2Prs2fLwTzwmCaO15KkNUk1kemOZenSpdTX10dbt26tfjY5OUk7duygwcHBLJtSKBRNjIZ3LNPT0/Tyyy9Xz1999VV67rnnqLu7mwYGBuimm26ib37zm3T66afT0qVL6ZZbbqH+/n66/PLLs+y3QqFoYjS8sDz77LP0e7/3e9XzdevWERHRVVddRQ8++CB99atfpZmZGbrmmmtofHycLrroInr00UepUCg01E4lNJQc2mLnmKSUcsVn2zhMco77sZhtw3EXyU3MGH0aAd1czLa2IchbUUo0Ym04aPa0wORTKFJxl/WUe72pE46A23D24FEdBrIyRiE70r+diwwGOsTvTAUfpMIhuDkeTfcwXkwUqRfB7DgobsG1rBzdF7iohpHZFYg452ZkL+Xuz2XX2vcRSReFVGR9yr+CueKDqMZIAFLPFcFYivEDEW/W2GXBY2x8FRR566DhheUTn/hEyu7N4TgO3X777XT77bc3WrVCoXiX4IRbhRQKxbsPurAoFIrM0bS0CWRM1Qc+jqz5C02HXGZPJYZCqnmntvQvXOZRrAU7Mdd34LUol3OxMVXG6k2ZZdE2LdjmJKRoii79yFbGyuDtR9ydHK3dcF7h4fSYFL5O4nmU/Xkic3SvDyFWwM2xsQQ3fe6G4Bo0U4POhfcH54HwXqjNUneoocPeRyRDDBJwD0hRfzCTssE2UvwH7Bi/DOxZkhhY62CizpbtsQ/vqEx2InisP5VSbZcMhO5YFApF5tCFRaFQZA5dWBQKReZoWh1LGCZkDsnYPpN1U177XMz0a8vSREQuc3lGP4iYhcujnwHqCQRtAnSoAvXyc/SDCJnsjTQFEaz53McjRXzPPkj7w2DoP6sT5HmH0QJEKE6nGOv5swBDfW3GzXRiPx7yAP2Jwfe9xMpRX8XVPGXQL2CbHtPF+dgfNmswE6ODWQT4cYplgvkEQWEM7zrh+g/UvwB9Bv8ChBE6NLFqQN+Y0uExxYoDA5TP2wflfiwp/pA60B2LQqHIHLqwKBSKzNG0opDvMjNYHaZ7QQqHkaC4i+TRxOgXz+UbuC/F/s/d6wmvhf6xPSgy2kkzI1RUj3Us9WBcFMJI7DqiEbp987AGGB4cLhHdjBkFmJh0hHxlglYvgkYjGDDZB6StY9elWPDlqYj0xRCRqPYYpMaS9w1dHfh0QqswDgqbexjWYBJ8n7xMVsOj9zHDAJq8uVjnuXXaFJ4MytKvUChOIHRhUSgUmUMXFoVCkTmaVseSGCuK5jijeMoUzGkTQI5Ed3tujgPehIiZ39wUexvI/qwijCRH1n4hEyMLPQ/fd+u4bpN0zUfm+4TJxCiTo32eN5NKTi7o5WQ1qWSCbOYgO5nsGz4ImHAFeVrtd3LwYjYGdTIhogqKUIfALkBqBk5/gCZ3A0oXziyI9AYO6yCoX8jFd82QoLsAhhxwqgYwRfMkFTj38XXGbI5jtsoZpqBxOW1CBWupDd2xKBSKzKELi0KhyBzNKwolMSWHMmpxr1QXtobc0xSTwOOy6YlE3RhxytjJoC+YmJuLO+h5i6KQiIROiVg8AzmaICGiurY1XJpB0SRYh5QLybT5htmglynIF/w85cXMxa2UJCQ/8LlIBe8vqZeEPF1x7TK8lb0Hg/OAN4miNLgE8ETw6KXL50UM7z2Cc+4mgSTqdUnVQMDh44euDSiuVrhLAMyDKLTRzQ4zv0foVl4HumNRKBSZQxcWhUKROXRhUSgUmaNpdSxhxZA5FHpqfC4Tg1zJ5F4XngbNzzw40wdZNmQ6hTSLF0SnMnkaLXDIMsbN0Wi+5HIuRvaGqQRhnK0M3Lx52ECdMiKiXI7HQMj+CPUC2mwhKTzPBpC48vfJCJM26sQk+FOjmgkzHnA3+RSjP48iR9UMmpTzrO9waco2zOBi/AYDqve4rgb7k8A8EJkVUI1RJw876r1c5hYRH8H1whMJ7mUHfY+x9Ls8urnOAAB0x6JQKDKHLiwKhSJz6MKiUCgyR9PqWCiJq8Ko1D8AWxk7RXfolB8Ck59RBuW+BAm6SteR9dOZ9EzN81SiRqaMSLmW10nAjaxwvHvOEVzoE670qef2A2PgIJMZ1wFhkzWi7g9/zvx8kPUsgvfAIw7wubj+JaVnkm06Ye02BUvAEagjJIceZkLkFR3BN4VnawD/JQy7qJdmkqtc0EXJhe9Nnjm6YKaCOLF+LCJDKCqH6qChHcvw8DCdd9551NHRQQsXLqTLL7+cdu3aJa4plUo0NDREPT091N7eTmvXrqWxsbFGmlEoFCc5GlpYtm3bRkNDQ/TUU0/R448/TmEY0u///u/TzMxM9Zqbb76ZHnnkEdqyZQtt27aN9uzZQ1dccUXmHVcoFM2LhkShRx99VJw/+OCDtHDhQtq5cyf9zu/8Dk1MTNADDzxAmzZtoosvvpiIiDZu3EjLly+np556ii644IJj6ySPbsatIjtOJZ+CeiJGZl2B/XGFhQwnEH2K9fKIXSR+Q9duuZWs52peOyr64Lmpcy2XPTAUAOWA2m3yZ0HRR5onD0PyzMEvPYJLP7eGx9ifOu7tKHLWi47APsRMjENRNuSJxpCwG0Qz7tLvgkjFRVsDCd0SMN3zkJUQwhiMW/u3P4fvQKgFJEKYF3Ml1gaM5YzHo5vtfeFvK7p5YmKCiIi6u7uJiGjnzp0UhiGtXr26es2yZctoYGCAtm/fftg6yuUyTU5Oin+FQnFy45gXliRJ6KabbqILL7yQVqxYQUREo6OjlM/nqaurS1zb29tLo6Ojh61neHiYisVi9X/x4sXH2iWFQtEkOOaFZWhoiF544QXavHnzO+rA+vXraWJiovo/MjLyjupTKBQnHsdkbr7++uvphz/8IT355JN06qmnVj/v6+ujSqVC4+PjYtcyNjZGfX19h60rCAIKgiD1uYlNNYQ8Yn7OHroq1/EJxzB8h5vYkAaAmZtTrttwraBCgEvrmYJRlnXq6AyQNkG6s6NPeG0GuZROqg7TmugDyPYx2jbZwJtUZoDDHx+8Fs651RoTJ8C1PCWCi+x3vAtHaNOpQ5vAXRZQz5SmKWDHqZ/o2mb9lCMBe0cOvr9UpjFG/QEv0GP7BB/KfMxGEPIxgP4x2gSuD4pKqFGsjYZ2LMYYuv766+mhhx6iJ554gpYuXSrKV65cSblcjrZu3Vr9bNeuXbR7924aHBxspCmFQnESo6Edy9DQEG3atIl+8IMfUEdHR1VvUiwWqaWlhYrFIl199dW0bt066u7ups7OTrrhhhtocHDwmC1CCoXi5ENDC8u9995LRESf+MQnxOcbN26kL3/5y0REdOedd5LrurR27Voql8u0Zs0auueeexruWJLYHSLfxVWQHJoTCGOSJqhT5m4GUyI31R2BgUzkG4ayVNQtN4OmypjZGli5K5jHmG1JjZGtCnEHxKQE6N3qWC8pZtGxDmReSyXd4oOdqtSpcZxGwmfgEaKbuVRQj0Uv9fZQ5uOmYRRz+RhARZjYS8wDGCA+v8pgXi6X0YzN6kFpA719eZpnqNdlY1AG8zcSYXvMpIzz0mc0AS6bT4539NHNDS0s9Wjy3kahUKANGzbQhg0bGqlaoVC8i6BBiAqFInPowqJQKDJH00Y3myQmc0g/IJiywK53BD52ccZZ1mOMpBVqgdrJp4jAtIkRpikPem57rV2GpkyMpOWmWEzwzU2mqHtImdyT2roIYcJFXRZca6j2tRz1zO8HO8TN3/BOQriW6cjclL6jRt8OVgxN1jYpJ6I/UA20KTJEYI56Ns6YnQF1GvXmAbIgivFKRU1zdkDM1iDrybGwFUy2Fhv7ReHJ8JLjFd2sUCgURwNdWBQKRebQhUWhUGSOptWxkDFV2Z3LmejizGV4J+1XLeBEvB6QZZmQfISEilJvgDJ6nZD9ejQAWIas/TxTIwr09WgT0KU/MdwfBike2JNCf5wcyvr2EEMghM4HdWLo3m5q64dwLDkFhAPjI95JKjMA6K9yOXsC76+lwGnqZPtpygfuPxTBtYwdEGj6U++WKdBQi4GqEj5IHs4ZdjdmpyyDH0vC/JDwuUqszBHJ439LtAkKhUJxOOjColAoMkfTikJJnJBzaOvLd47I5uYwE2ACTF2prTX/AMzNPEEYEnOhaCR2jnBx2tTJ2kcLIDf94tYeo5LZtZiUzBFlR6BPS+2teT1MTMIy7E89Bjk28qnxAPB3gjUimxpv0nFqi5VoYsefz7jM+gd2/ZmIk0xLYJuJGHcIpeBmWni3qcTvXIpCcaNODIbj4XeBzWF4zxjNH9ZJ8M7ZFR0e3Tx3nKKbFQqF4migC4tCocgcurAoFIrM0cQ6FqK3Cd+43iBO+8xXUS9pNxEJvQrKudwsmiIBqKNOSMn6dRKtp4nf2HOh2zeeC3MzUiPUNnsmEeqduBs6yOE8LB4yFSBbmWCEx3EXA1afNoHqvDNsMxG6NgAvQ+Y3ZAtkpzmS4LqJlKkeqRHYqWtk/EHMCk3kQRk0yk3uqTgCuJYzPqTcBexxCJMtlayBUSNgWY69FOEtUI9zA6A7FoVCkTl0YVEoFJmjaUWhg563B/doibBJyrWQbxxTybnQcsdJsOsRHIPsk4o0rt3EYRKNsWO09NaxxKZTJTMxDqN16zC21Y9urpOjOtWGBL83HflcW/zBvlOdaGtM/saB+bQ50i4J+D65/IyRvfw+6A+IbZzBzcyl5JvqURIDMTlOPiYb4fxx/ZTR29YboihkywL8ZkNEfM6392Le6QoX4+oQtdeD7lgUCkXm0IVFoVBkDl1YFApF5mhaHUsSReREB02nHjMTo3zKRXYXlCEpHQJ3wYb2OMs6Sssp6Zm7j0ObqBbgKo4kQf0HawOTyYO+iJujHaBy56ZNJ65t9iSSrucpfRCLbjZgRncgmxjXP6AOgRc6Lr4TQB0iusZCKey572NFEAUcVqrHEbDUtbZYRUoMSeBDRybWcypT9iQqiTLjslmTtIqyxMivnc/mdwK/9cg2x0MkcqDz4Kx6MehUQggr4AnuU98p7rfBmoiOXsWiOxaFQpE9dGFRKBSZQxcWhUKROZpYx5KQ87ZQx2RAp57ffiopPMjaPLE6ulMwnwCU7RNCObd2m2kPbKfWpbKdOpQKeHMquXyd9g9Dg5a6olqPYGFDHUvteupSNWDYAGi3RAbDlM6nzthiknN2nnKLd7F/tk8u6IBmWCshsK4R6G6iaXtvC7j08zCHGHz4sTcRd5DBOZt6TlYE9fBMn3nQQeXgvMSeLYHxqpTts/DQl6h8nGgT7r33Xjr77LOps7OTOjs7aXBwkH70ox/ZzpZKNDQ0RD09PdTe3k5r166lsbGxRppQKBTvAjS0sJx66ql0xx130M6dO+nZZ5+liy++mC677DL6xS9+QUREN998Mz3yyCO0ZcsW2rZtG+3Zs4euuOKK49JxhULRvHDM0SRkroPu7m76zne+Q5/73OdowYIFtGnTJvrc5z5HRES/+tWvaPny5bR9+3a64IILjqq+yclJKhaLdMZf/Rd5hXYiAjLt2h7O5OXqZNCmtGs+h2gjlTRKXiusnqlKj96dnRMupyKfYfssyLTB3CyubSC6OR3zUDvRu+fljvpaYYtOxQKACwCrJjUVUQyQ9mboD4tOxzAG6F++xWoAEtAG7Evy1eMV8euibGF5Rpy/sN+arac6u0WZx1zxgxBMyI5s01SYGOXg+NQ283sd8FzsFQV5WRYAGXpctuUhugswtwOHTcxobor+68/OoYmJCers7KR6OGblbRzHtHnzZpqZmaHBwUHauXMnhWFIq1evrl6zbNkyGhgYoO3bt9esp1wu0+TkpPhXKBQnNxpeWJ5//nlqb2+nIAjo2muvpYceeojOOussGh0dpXw+T11dXeL63t5eGh0drVnf8PAwFYvF6v/ixYsbfgiFQtFcaHhhOfPMM+m5556jHTt20HXXXUdXXXUVvfjii8fcgfXr19PExET1f2Rk5JjrUigUzYGGzc35fJ4+8IEPEBHRypUr6ZlnnqG//du/pc9//vNUqVRofHxc7FrGxsaor6+vZn1BEFAQBKnPkzAixzuoL0jquIiLROHI5oZWx3qWas6slioFNjdegroRDCPg+a+QikCwgcl6UqH/QgeEOg1uKkeZHMzGnD0e2uDDcyTFG3eh91KJy3lf4b6ULf/ozc183FPvmtcDTXgwBtGMdb93YSwXRVYUv9B/WZR9JnxcnP+nN696vHF2rSg74BRs+wE+hzz3fZY8DPV5oBsRISygT+P0By646SMbn2FhBPi1CDlVAk+q14A29h07yCVJQuVymVauXEm5XI62bt1aLdu1axft3r2bBgcH32kzCoXiJEJDO5b169fTpZdeSgMDAzQ1NUWbNm2iH//4x/TYY49RsVikq6++mtatW0fd3d3U2dlJN9xwAw0ODh61RUihULw70NDCsm/fPvrSl75Ee/fupWKxSGeffTY99thj9MlPfpKIiO68805yXZfWrl1L5XKZ1qxZQ/fcc88xdczEEZlDkbou297HaBrjsgbKOiAWuHXEHeG9ijl6sW/sOE6ZopF5rTZjG/cMRvJstD8Lsm00KbOtq3uE6Ga3TlIyLnJiQrKUGOmxaPB6436E6GaecwsJAFE24pJRyszPxtLzgbwaWo1YbuIAopLPi3dVjz888f+KsqXJz8X5dvNhW6eZE2X50L6H6Zm8KHNc+Y7aikxsQrZC0BLkjTVxJ+B2UGJmbQ+euZLIivkXP4Yyl79PLtY2sFo0tLA88MADdcsLhQJt2LCBNmzY0Ei1CoXiXQYNQlQoFJlDFxaFQpE5mja62SRJlRXcCDUK6ljsYSrKtp5JMmXarK17wIv5mQNXozmVl2MUKTeZppJPgWaHM/w7GMVdh4UNIaKJU4V8DEA3Asm6uB4FTb/C/J0aHzCHs2tToRNgyhd6sBCiptnNMfTd9+S1HnuWEigO/r9kRfX4FeAOvGfyInG+O7EKkNm8vJYntC+2gzu9K9t0YqufKQNTX4AJ8JhSCjMw+L69FxPRux7qXOxxau4xPZ1IqpfKRFAbumNRKBSZQxcWhUKROXRhUSgUmaNpdSxJTFXGN85iZYAGgCcv99CpxEE9ATuu03ZKVZNiIGOu03XK0nWBXwZz3EglmQPaBMHWjrQJ7GYn5eMCtAlctsZRcIS/uCzDcATD3fZROcL1ALX1XEQiQl9k8jtMk8BXUTt8wyM5BpiB0nUsv4ALNP2FfFv1eLd3liwrlMW5w8ao8KZsk9MWVEo4Z+W1M3lbT3/8hijrARa7A2TpCiZzkqrBYd0r4DhjNgnmZxPBuHvMj8Vx+NzSTIgKheIEQhcWhUKROZpWFCJjqj7cwoSLmbr5ljyqvT0mAhf61Padm4VlkQemOnGWIpADM5+wodamv0sRf3vSLV2WI8Mdj/5GAnE0fzPxq44khGZzjAIW4k8dqSnCxOXQ93oR3ggRUe3VGcs63N5ERHOzVgxwQU4ql2arxwm+97IUaSImCjm+/Crx7iWJFLfg1VIwt6963E27RdlnWuT5L+nU6vGT4bmibMrvqB678D3xMO2ez8RBeH88fIS/k1TIRR3ojkWhUGQOXVgUCkXm0IVFoVBkjubVsSRRlR6As8YlIOgZVmaOEKIv3euhUOhY0N8fE5CzY6+OooIk+0EqST3X+aCbfh3aBCdBaoTaZSZO0f+z/uAg2OmArvdJHs3NrE00/YokZFKhgGK6W0fnk1KDsWOW1z1V6jv1Xc/jMusFNlpiNAowD2YdMOWHth0f4hFiZt71WkDfUZFs/6dEB6rHH4r/R5Tl4ilx3uVb9//e7mlRZkJLv2BA54NJ4nlIRAwDnWPv0xHshGpuVigUJxC6sCgUisyhC4tCocgcTatjiSshGeegvoC7IzsQci68vEHHkspJyBOHQxF3NU+XgV6HXWHS/uLYqr0WM/tR7TbrBh3U8z85Qi2OoE1IcUfYQ3wOdOeW6QdkGyllCSurk6Q+Ne51shqkEgOwu32vdigHEVGB6R9Sj8Vc8VGjEMaSxjJiflNeJKkpc3lbNomhHBBGEIf7q8dToFOZKMn53tZq/fY/4EyIsunEZg2YJclpWUlzqFaBPlSczpR7+8dIn1oHumNRKBSZQxcWhUKROZpWFIoiywzvMjYux4NoXZebNqESYI/n2zpkc+PiTsr0ixG6XBRC8Qv6wM2pqWhrU8fUilHKIsL76JPCE7D28zFCMU6MH5oWUWRgTUbI6M/PfdmfCBjSOPNbKotBJYRr2THJJPXcdB+7KELJzicRM/dCGMiBGdtIDqUHEIV4qq8YRI9S2X612ooHRFm3s0+cF/P2Xjd/iiibBbNxwMS8Zcm4KKu4Vhz7Bcz9AqQlK5Xse0HWOi4mGWa6T0oyursedMeiUCgyhy4sCoUic+jColAoMkfT6liSxNImcJd+JxW+z2RH1JukKMhqy/Nc3YCs5SkTqbgR28QMfXVoE4TJFovw2tqUDzxznYOu26hvYMqcdOgCM7WmskjiWLJQCggbQEYyjhja5Nf6PrDO5yANIGfiB9MnZ6JzK1IXgJkQZxmVQwQMbTySIvUOCN0ZbHkFxstnep08PFdXIMdrScGWzw8KoqwyK04pYVkcOxJp4p5nxm0ZpFDcV5E6Fj4vXFDwheK7YI9/a7QJd9xxBzmOQzfddFP1s1KpRENDQ9TT00Pt7e20du1aGhsbeyfNKBSKkwzHvLA888wz9A//8A909tlni89vvvlmeuSRR2jLli20bds22rNnD11xxRXvuKMKheLkwTGJQtPT03TllVfSd7/7XfrmN79Z/XxiYoIeeOAB2rRpE1188cVERLRx40Zavnw5PfXUU3TBBRccfcc8h9xDNFx8R5pK5MWOQyDaNhDl6tZhieMVYRlKN0KkQSaz1MXMTJxiZePmZhTNkBSb1ROW4Vo2JkjCDSZln/Udd7aclBvN6MgglzCRy8lBAi4WBVxJpFk4KksTco6NlwsiVCWHSdLsYUCY+N0WFnx53zSYlNtYYvVYSh5Ec29VD8tA9RZMARNcq322ed6kKOvJ2WsXzsqy0wNJmJ2btuNXmYNIaK9DXstEN9e8Jco6Y5t8vtglH2z/tBzbyZDPYRTjuMrAvktTPs7m5qGhIfr0pz9Nq1evFp/v3LmTwjAUny9btowGBgZo+/bth62rXC7T5OSk+FcoFCc3Gt6xbN68mX7605/SM888kyobHR2lfD5PXV1d4vPe3l4aHR09bH3Dw8N02223NdoNhULRxGhoxzIyMkI33ngjff/736dCAfeQx4b169fTxMRE9X9kZCSTehUKxYlDQzuWnTt30r59++icc86pfhbHMT355JP093//9/TYY49RpVKh8fFxsWsZGxujvr6+w9YZBAEFQZD63Bz6I5K6CSdlhq1D8147kDbt/l/HlIZJtoTne8qyihXxMFI0RZvDHb79iTzjpz4w+BuuN6nPfpdEnEGutus7lmG9PDLag9+niJmCk0jWE1bktTELT8jDtcmcpInjyeFmwHzqMpOu52PSdakbmWdVETRTkm0sbLP1jIdSzzUFPv7cxb+9Xeq2AqYfWt62R5QNFKS4P5Ystn0PQK8D+qIW9j7zULYwsYxyfa5kqdtrWsX5tGM3BnPookB8vOwYxBCVXQ8NLSyXXHIJPf/88+Kzr3zlK7Rs2TL62te+RosXL6ZcLkdbt26ltWvXEhHRrl27aPfu3TQ4ONhIUwqF4iRGQwtLR0cHrVixQnzW1tZGPT091c+vvvpqWrduHXV3d1NnZyfdcMMNNDg42JBFSKFQnNzI3PP2zjvvJNd1ae3atVQul2nNmjV0zz33ZN2MQqFoYjgG/ddPMCYnJ6lYLFLv//MQuYcSdAt38hQTP0/QDjI6ZoNjioy0fsFjZYBUVDnX60g513Hr6MNBIZNnsj+S6VMsZf+I6UYioD1zEnst+sM4yHDH6RdSWSXtoZuTsj7K/l7eKioceK73t7G+gp5k1xvyt6zMXM+DAryvHOpY7HEA0zZhzIJL4efyzIWy7yvbbX/bW+X4BIwZIYTnKk1LVvxx5gsy7kv3ej+013YbSZuQAEtcnJ9fPTaQnaEQSL2GX7Zj4njS35/7uEy3zxdlzxwYEOe/YjqXciTHJyyzDAOM9iIuz9DP7rycJiYmqLOzk+pBgxAVCkXm0IVFoVBkjpMiupmbaR1TWxRKIOIVg5vFJhPtxKa2mAR3UsLuTXn0QwIzLp05kAycu9sjTzEKqHzDjgm/HRbd7KbGAH47Yru1TaAoYa1gZDhGKviOlRk+2CnFiVXzrKnzg61yu76lfZE4/4//YSZkcOnvyGFErm3HLcipyziwac2peVH2YfB06PdteaEg3+0B5rafgOvA7JTsz1Jjx+CN+E1R1hZZouupWdlGaKR4s5+LxMju7ct7cywhnZOTjHacKD1PUjRb5En3/zccOwa7QzmWFe7iz8m0awetp6A7FoVCkTl0YVEoFJlDFxaFQpE5mlbH4kYJuYdkas6Sz+VsIunujzqDNOMXp8aSVzrsZkzk5aR0Lhaxh5kA5LUe07m0eEh1b+VldwbcpX1JN8CZ8hIDtAnG6k2cGOgWoL8OoweLII7AC2yZB8nlkzk5VVqY7Xd+SdbTM271DW9NShPt7KjsUZvTVT32HRna4c7Jc66/aglkm62MZiL0cHyk2ToM7djmE6mnmGEJ3NtCaRaehODaFsZaX5huF2UJcwHIGalneiuWVAjRrG2zlMBXMgK3A/ZaYgMuABVGdwD6vAJktzgttDqXuUqXKHt13I57VxujmCgfvUu/7lgUCkXm0IVFoVBkjqYVhRJDVZsr96hN5T/mRNvoSJqKJub7QxB3mOemAwJEkmJlq3WSZrgrC3JmWdaaZ2IJeOzGQPJsWD3opcvNjC7Ig8iG5xE3lcuxjNl45V0pUs0Ds2x/m63YBxPprNNSPd4vd/Lkgt26wCKWQ6gnJth6s0GYmJAVG+Yh+up+KUZ2+GiCt6ZY15f9CVttPTOR7E8llmJdpWT71wm5m2fYcM2GcpwrMPfinL04D64NfiDFOj9vy2ddcG1g9uAI56WR/essWHGnGxLDvZmzkc8VJpqh6FUPumNRKBSZQxcWhUKROXRhUSgUmaNpdSwmMmQOyZCGsawbH5n4uQkZTNFgCvZ4omxM0M5ZxtBNP5QyqMv0IXNl2Z/YlfK902XbPKUHImDbbHLweFS2URntl31nuoky6CIMs0E6CbDLwcO0MVd9B2TmnoLVIXy0OC7KwmKXvLbVyui9sWREc/P2fZUPgLs46KByLFq3HaLTA5Lm3g7GiuYn0oQbsCz1nW9I/dD4nNRTvMICfTsKkNyMsfMFidTjeKFkYYvmbDu75+aJsgmmmzCRnBMlUFUUebEj+25c2b8ycy2Yg0how0IiEsjkgBkQCpF16T8jL78LUYt9RxVmYg6TWfoFHR10x6JQKDKHLiwKhSJz6MKiUCgyR9PqWOIoJnNIbha+K0gnwBO6ocwJFAYRz8GOXvosUTjqWBzQxyRMj4Gm/Ri4EcrMndwA49f7e/ZWj18Iu0TZ9D5ZTym2grgTSTncYX0vy6LUL0elwqgRYHy6Wy2b/EJgeS/Ok+leWlvt1OkGjoduxlA/H/wwDDDjzYzZ8xAGs9OVOpaEsbLNb5N6i9acHdsOX4YCOHlZ7yTzRymDTqziWRf/QgF0PuDvEQX23qgs2ywxagaCrJZ+Ts5TPr0CyCpZcuS5zyYu+jrl2aMk4MJfSCCLYWDHqwX0Xu9j72iKsfpVjncmRIVCoagHXVgUCkXmaF5RaKZMyaGkVA5LmuT46KbPtmpQljdyC+q12PMI6LB8llA7ArdzRxKSSRM3bCNdT0bLdnbZus6a96IoO/O0ndXj30wvEWWvEpib56yp2pmTZk/+JLErt8ARuogz0327K0Wz6ZLdzr94QN73yuuSELrFt+LEWaeIIuokOwb9IG5VJmS9ix37LAFE8rrT8vw019Y74QCRdGzHuXiKFKFcECfK0UvV48k35RyZbmfPeUCKSS1gCnZKC6vH7bPniLJcxZJN52OIVIewCzewbUbgeu8EE+J8ir2/0pwU8aZY1LbfBuEIID530f7qcQD9+UC3fSd7p22dJVfO7XrQHYtCocgcurAoFIrMoQuLQqHIHE2rY4nCMjmHWNNcRmnggjt7xJjVDCTxrsDTeZ41mZbALOvPMqZ7oDdARn+uNXAhxCCGa+fT9upxWyDzXs+Z/64edxak/BqXPy7PvbbqsTcLnWdsb6EDJkEwdQaMtS7lXh8xHQawws0m0tw85Rarx+6kbLPTs3L5M9OgYxkHd/vQ6lG6YzkGX1ko9Qs9jDlvcbtsM2H9mzGynpLTJs4Dd3f1eNaVk8Tkbb25AMIzQAfUx8baMWeIMmIm7xyYux2ksvBrZwaoQOJ3l7kzRBUIS2Gm6FlMVAfhEmPM/JwzUoloWGK2ORaOUI6Ok47lL//yL8lxHPG/bNmyanmpVKKhoSHq6emh9vZ2Wrt2LY2NjTXShEKheBegYVHogx/8IO3du7f6/5Of/KRadvPNN9MjjzxCW7ZsoW3bttGePXvoiiuuyLTDCoWi+dGwKOT7PvX19aU+n5iYoAceeIA2bdpEF198MRERbdy4kZYvX05PPfUUXXDBBY01FDpV99gkZkxrnjS18py9SY8USzogkZbHvAjNtPSUbGPbwZT3Y4oVzm4rwwpsD8HcOxnbsZpxd4my3nYrFnQA0XYeWM/m9tjzoEsSN7uuPTez0iyMJHrTzGS4qFtuj8/os1v9uUSaPd8al7l621rZWDrynczkLIOce0B68DqQaKyX5TVemoekX62yDwscayKNK7Lv5bIdn8mCfLe58rg4Hxu32/upBVLEo4rdYUeQMG1iT1Gcx3P2OU9zZUKwKGJjkJOimIN5p1t7bJ2R9O6dyUni7WnGaueUQS3AWPRyFcl2V3Zgnsbs2Txok7EAxCU7nyvxcfS8femll6i/v5/e97730ZVXXkm7dx+UV3fu3ElhGNLq1aur1y5btowGBgZo+/bttapTKBTvQjS0Y1m1ahU9+OCDdOaZZ9LevXvptttuo49//OP0wgsv0OjoKOXzeeqCFAm9vb00Ojpas85yuUxlFoMwOTlZ81qFQnFyoKGF5dJLL60en3322bRq1SpasmQJ/du//Ru1tLTUubM2hoeH6bbbbjumexUKRXPiHZmbu7q66IwzzqCXX36ZPvnJT1KlUqHx8XGxaxkbGzusTuZtrF+/ntatW1c9n5ycpMWLF5PvO+TkDsrRDpN1IxDeDDOjuQ4wtzsQ7cySX0dGyuHTkb12mqRsnUOTMmNADx3Qv2D4dZu915kPJtwOq5swwGTWQXLnNudaXcAc7Op4UvjEBVp8CNWe32X1CwtBJ3XZgO3D3jlZz55x2b/9TL/gGVlPO0tWPjUNei4wKc/vsPJ9vwc6lRyYWlnYxfSMNAUnFaZfgLGb7pJ6iq48ezZg0O9hOqgpkrqRXL8cg2DG9mfPa3IMKqGtJw/uAa4r51c4YesxwAA4C2bikM2vOAQXBaYLdOG+BCOsPfa9gf74jEUvx6LYUV9XD+/IQW56eppeeeUVWrRoEa1cuZJyuRxt3bq1Wr5r1y7avXs3DQ4O1qwjCALq7OwU/wqF4uRGQzuWv/iLv6DPfvaztGTJEtqzZw994xvfIM/z6Itf/CIVi0W6+uqrad26ddTd3U2dnZ10ww030ODgYOMWIYVCcVKjoYXltddeoy9+8Yu0f/9+WrBgAV100UX01FNP0YIFC4iI6M477yTXdWnt2rVULpdpzZo1dM899xyXjisUiuZFQwvL5s2b65YXCgXasGEDbdiw4R11iohoMiF6W33hMBk+8cGdPbYy54r8blH0qa4Rce45Vp5/PB4QZc+WTq8eBznIjjclXZ4DfhrUZwc7vc/6E+Rd6WPiz1jfh6jUJcpmCtL938l9oHrcMr1UlM1WmM7Ak7oHD2gL3KKlG9j/luz7T3vstZEv/U9OmQeZI0tW/9ABdAKntFnZPylIXc0iX7rpe2WrO+oDvcBcG/iRzNmB99qkHiX27XO//4D0yyjNSu/vXcwVPl+RepM3z7I+QUmnbH8AfIu8Gfv1ObBfjkGBZUk84Mv+lCtSv1eI7FjPgRKxXJLvwWMM+m1AY1BoseczZXmfb4CSgk3TjhKEBrRbnVTE9DZlUgY5hUJxAqELi0KhyBxNG91MsamGgTo8YhgIjYkl4w6n94uioLxPnF/V/3L1eEm4R5Tt22dp0PZG0jLlAhMd744PI5gP5HbebbVb0qQbsrl32b7PtmCiMbld7mFRrjNzsu/tbIvak+sWZf9jForzeQXbn3ynNMPOMXPvKUUpUr0+Kft3KgtBWNAmzcRuYH+v5kWS6a1CwMrGomddkr5Q00Ze285MplEZTMgslMH15Dso5uX7K+aYmFCU77rUace9Bd6Ja+SzdDAWvYUDUsTLMwa3Dl+KIZU5+VyvjFmxch+wwr05LcWPPGOP7yjI70I7s453F+Rca4OI/XzbDCuTk7jAxN6JxIqGc/I114XuWBQKRebQhUWhUGQOXVgUCkXmaFody8IWordzSUXMhT5aIOXKAmOh/3ibNAufk5cmyUVFe++nWmWI/v8Zs+f/8RYksZqW8vwBZmakMiaM3yvOF5Z/XT3Oj74hyv6bMa/ND2VIfv9/S3fyltetnuDbpX8QZQtyVtb/8fgyUfZfwZni/H+/saJ6PNAqQ+tdxkQXVeQzf+GD4PYd2nHPAS0AsRD9IJDJ0scgodtrB+xvW6kM+peyNBOXmenztA6pQ+gmq/8440xpam2ble/oQ91WrzLRIXVS49OnVY9DYG+b7JTm3dZWW+6NyOds7WHJzNxFoswtynpb2lii91iatEcSqQPKMX3NwnlyX9DKyoJY0kG0g0v/lGPfb3tJ6nEOsMR/HstYMdvAPkR3LAqFInPowqJQKDKHLiwKhSJzNK2OJewokJs/KCf6jBLv7H4p585LbNa7axZIP5alQC04HVldgFuScvd3z3/IlrVJ6sfRSOob3mAu/y2e1C/sLkrZ9sXXrCv+65Uu2R9GvTj/ZakXuMTZKM5X5qy+6P1vydCAdsYMuZx+Lcq+0LZAnA9fYjP2hctPE2Vvts2vHnd3y6mx7zXpizEXMirPBPRMLOyiG7ItFvLyt2xZn9UtTZSk3mSmLPUNbVO23q5xqR9qK9mxfPOAfF/+AenPNPF/LUWo3w4+HK/0Vo/d06QPUGEFJJv37HmhXTp5RGU79wKS+rxyInVJXZHtX1G64NAZrnzOQosdkykg4mdRDVQCisscZC6YmrFzvALjHLJ7Y5bhIIbvTD3ojkWhUGQOXVgUCkXmaFpR6Dtffo1a2w6KFR2JdZc+vVWakHOTdpvZ9UvJgu/nIcH2azwxFLC59VsxIA7kli9eBC7hrr23Y0pua/fDlrPPsybmMnj0F//bmlNzI9Lkt+It6SK+pGKf2wMWUJYbi9yc3Eu3BbI/Qbs9f7kkxZTFORtRPf6G3PYvSLpkmyw5VgmSyHkV+6AQsEztsfwg741XjyvAntbbJdn/QyZ2FtplPZOJncrTr8pnfj2S4imLsiBnWiaQp07bRiknmQ/z53bJa8nOkzcmJUu/F9v31RLIuWYgXGNBm20zystrfU/2fc6zc7EA9YYsGZ0LLv1TRo7lgdCO9XQFkssz6cthYu3cHDAL1IHuWBQKRebQhUWhUGQOXVgUCkXmaFodyyJ/D7UdUh70M9axVlgL857VTXiBNDMmRupKOhhD/XQk5cq2Dpb8uiiVGJMRyMgsyfeeknS57uqQsvbPx62c3pLIspyxz5WPpFlxHpi8OTFdXBkXZSX2KIEvdQ8TiRyvILHyfX8O3LxZ9r7Al7qjciT1KAkzmZbKciyNw0zRBOxpkIA8YCERjivH+a0JeZ6btX0otEvdQ2dg9WmFvDTdh77UJU0ynYsXS12EO8nGr0sUkQcscdM5q7cYDWXZXNk+57wp2R8XMg92Mb6DMvBw+HmpGyHfzvc5MCk7eZYxNAKqD6CS6GSuGMYHxkTXjuUk0weVIeNDPeiORaFQZA5dWBQKReZoWlGoOD1D7YdEmc7IihAFIHmeesuaC0Pg+vVBhIlmWfQnkE4no9abNSoBafICGXnsMdNmsUP2p+LLa9d82HbK9aWX7tTs+6rH43lpXu5sfVmcV16x215PcoYTtyzmFkqRpbNNvuJRJi7ORFLk62RRtyEQirfMycF1HLtdbnGBjJlY8jAwISctIBrlWEIsiCZ2wIzdElkT7v5pKQpVZu3YvlKR729P+/tlPb/LmOjAmXRlzt7r98i+l6elmXgqb8XgkRk5zjEjgB8Fk3FgpBvE4sSKau3grlCpyPmUi5lYl5eiSc7YdxS2yDEIHDnfZzvtd6G1IFUIbsH2r58zF86Cq28d6I5FoVBkDl1YFApF5tCFRaFQZI6m1bHsGc9Ra+WgXDjp9lQ/f78jdREVxpSVdEECbUj8PsLcpRNPyot9zAUbGfMrECUdt9g2nZJsw3XleZ7LxMCelu9gpl9JZEZds1Iun93PImnnS70OVzdMzJcy+WxRvuKI6U6igvxdmQtZGeigMHm6x+pxwGTruPZeB93AfWlu5nEOUQB6gFkZUT1RsSb4/zsD+o9Ze+8rRpblwIydZ4qVNpLz4GevM51YRZp6p8H8fSBv30kI48VH9oOLIMzCk/3rYK74uUDOHxPj+LF3FEsFUcIy6XmQMD4E94EWpktqdWT/glZWr2vbn55Rl36FQnEC0fDC8vrrr9Of/MmfUE9PD7W0tNCHPvQhevbZZ6vlxhi69dZbadGiRdTS0kKrV6+ml156KdNOKxSK5kZDC8uBAwfowgsvpFwuRz/60Y/oxRdfpL/5m7+hefMskfC3v/1tuvvuu+m+++6jHTt2UFtbG61Zs4ZKpVKdmhUKxbsJDelY/vqv/5oWL15MGzdadrOlS22CcmMM3XXXXfT1r3+dLrvsMiIi+t73vke9vb308MMP0xe+8IWjbmt81KdK4aBcOMXkzOlOyUJvmKzYv1jKihUP3f/tsQsODCXG5J4Dt+pyAfQoLMy8AL4OQQyJukPrE2BykI2OZeELFwPb3TypdPGWsPD+/3WKKAvbre7BDeaLsiSW9S4qWLayyJd6nImy7asbSn1CuSD1DZx2omWelL3d2OpGTJv03ckZOQaG+V6AOww58I4SxmbW4cs2E9a9gU6pZ0qM1AH1iEyEsj8+Y85zXakPOtAt/T3yjm1n/lKp9woc+257gAXOc+R8eouNbasj+9MOCeU9RldRiKUfksNcjWYTOYcT8OnKsTEKE1lPN9Nl8dsc/zglhf/3f/93Ovfcc+mP/uiPaOHChfTRj36Uvvvd71bLX331VRodHaXVq1dXPysWi7Rq1Sravn37Yessl8s0OTkp/hUKxcmNhhaW3/zmN3TvvffS6aefTo899hhdd9119Od//uf0T//0T0RENDo6SkREvb294r7e3t5qGWJ4eJiKxWL1f/HixcfyHAqFoonQkCiUJAmde+659K1vfYuIiD760Y/SCy+8QPfddx9dddVVx9SB9evX07p166rnk5OTtHjxYurNz1FbcHALGxm7HzMFiPot2K2jm5dbtbac3AJ7LBETQeRzgW85wRQdQPSnYUni/URuayMYUm7yDh2IIs3ZrXYIrHB5MMtOMbfw8aIUd2KWqC0P7aNoFrPyIJHjVWDnFWC7y4OckrDfpFbYrkdM3MnhM8N4+a4V1UJH9n1hhzQ3m9g+50Cv7Htp1taT9EnRI4ZI35hd64DJdt9btn8emH7ngdib9+y8dLtEEc33GYseJLdvMzDubD4ZA+TeruxfGNuxbXPl+IQsYXwLbBlK8P48LhnB+HiMpW6aMc+VXJgUddDQjmXRokV01llnic+WL19Ou3cfDF7p6ztIETA2JjPYjY2NVcsQQRBQZ2en+FcoFCc3GlpYLrzwQtq1S/LK/vrXv6YlS5YQ0UFFbl9fH23durVaPjk5STt27KDBwcEMuqtQKE4GNCQK3XzzzfSxj32MvvWtb9Ef//Ef09NPP033338/3X///URE5DgO3XTTTfTNb36TTj/9dFq6dCndcsst1N/fT5dffvnx6L9CoWhCNLSwnHfeefTQQw/R+vXr6fbbb6elS5fSXXfdRVdeeWX1mq9+9as0MzND11xzDY2Pj9NFF11Ejz76KBUKhTo1H6Zj8xaR33rwnoCxkcczUs7zycqZbRE8TgWSnrewhGUkzYN+gcn6oGPxQfYvB8ytuiLlZUzpFPlchwBMaweYeRxC/accKRLGBdtOgPwQE/Y5c7F85oojr42YTmoGMhUwUZ9CMLU6wJCWMNm/PCefK2Iu/VEs60HXd6rY8wT0AG2BNHHHbHRzoWyTZyqInB5R5jjAptZl+zR+QM6nxf3W5O5FUofRD8nWYrL6qw4IR0hYErfYBwXaHLg6sPN8Iq2i05AQj8j214HxqrC5RmXQzeSkbrJctnPPS6Qebj+b37PTNiRlunT05uaGY4U+85nP0Gc+85ma5Y7j0O2330633357o1UrFIp3CTRWSKFQZI6mjW6mxBz8J6KYeSoWOqUI47DtqQsMZBHJbXjOsVt/3OqHzMVwzoBpFeSbiGXh8hPwUAXzasB24RUQhVpybGsJEkIQyC16uczY3Ry5RY+YGVAKQkTGkWxlSWT7PgPkyEHJnofg2RpAhHDEInQNjEHMvFldMFEa8IYuME/TBMpcGEtmTaUYrmUOqeSA9yrBeZ6JE/PaZf/iNiYiRLKNqEXOmTxjewPJTJBXBzk5gSowv1oZAfoceNPmSyDuMCY448hxn2Uiex4mbZzI52zhEeAQDT4yZ8Vwd8Y+ozLIKRSKEwpdWBQKReZoOlHIHPKynRXkzcwq5IJmOrRbPO7BSEQUkxQZKmzLGQKBDr+1BDlYgNOZIqaN98FygKJQyLwaKzkQGXheHxAnQhC/yhX7gXHQCmPHwA8hyA/EppxTWxQKhSgE/QELUshEERSFKmz8MOgwBHE1ZIOLYlIMv3sRE2lACiAm4VEC+aEJrEIVJqqFYGVMHJb/GLxy4xgC+ZgHbZRDr1TbpgfE5GFJvhNufSqByJIvyfHi4nQeRSE2tvkZmPtIVsajC+H9zfDA0Fl7PHvo2BgY/MPAMUdz1W8Rr732msYLKRRNjJGRETr11FPrXtN0C0uSJLRnzx4yxtDAwACNjIyom/9h8HZMlY5PbegY1Uej42OMoampKerv7ycXOS4ATScKua5Lp556apU+QeOH6kPH58jQMaqPRsanWCwe+SJS5a1CoTgO0IVFoVBkjqZdWIIgoG984xsUACeG4iB0fI4MHaP6OJ7j03TKW4VCcfKjaXcsCoXi5IUuLAqFInPowqJQKDKHLiwKhSJzNO3CsmHDBjrttNOoUCjQqlWr6Omnnz7RXTohGB4epvPOO486Ojpo4cKFdPnll6d4h0ulEg0NDVFPTw+1t7fT2rVrU4Tm7wXccccdVXrUt6Fjc4LSIpsmxObNm00+nzf/+I//aH7xi1+YP/3TPzVdXV1mbGzsRHftt441a9aYjRs3mhdeeME899xz5lOf+pQZGBgw09PT1WuuvfZas3jxYrN161bz7LPPmgsuuMB87GMfO4G9/u3j6aefNqeddpo5++yzzY033lj9/L0+Nm+99ZZZsmSJ+fKXv2x27NhhfvOb35jHHnvMvPzyy9Vr7rjjDlMsFs3DDz9sfvazn5k/+IM/MEuXLjVzc3PH3G5TLiznn3++GRoaqp7HcWz6+/vN8PDwCexVc2Dfvn2GiMy2bduMMcaMj4+bXC5ntmzZUr3ml7/8pSEis3379hPVzd8qpqamzOmnn24ef/xx87u/+7vVhUXHxpivfe1r5qKLLqpZniSJ6evrM9/5zneqn42Pj5sgCMy//Mu/HHO7TScKVSoV2rlzp0jT6rourV69umaa1vcSJiYmiIiou/tgbuedO3dSGIZivJYtW0YDAwPvmfEaGhqiT3/602IMiHRsiI5PWuSjQdMtLG+++SbFcdxQmtb3CpIkoZtuuokuvPBCWrFiBREdTGubz+epq6tLXPteGa/NmzfTT3/6UxoeHk6VvdfHhuj4pEU+GjRddLOiNoaGhuiFF16gn/zkJye6K02BkZERuvHGG+nxxx9vOL3MewXHIy3y0aDpdizz588nz/MaStP6XsD1119PP/zhD+k///M/BclOX18fVSoVGh8fF9e/F8Zr586dtG/fPjrnnHPI933yfZ+2bdtGd999N/m+T729ve/ZsXkbxyMt8tGg6RaWfD5PK1euFGlakyShrVu3vifTtBpj6Prrr6eHHnqInnjiCVq6dKkoX7lyJeVyOTFeu3btot27d7/rx+uSSy6h559/np577rnq/7nnnktXXnll9fi9OjZv44SlRT5mte9xxObNm00QBObBBx80L774ornmmmtMV1eXGR0dPdFd+63juuuuM8Vi0fz4xz82e/furf7Pzs5Wr7n22mvNwMCAeeKJJ8yzzz5rBgcHzeDg4Ans9YkDtwoZo2Pz9NNPG9/3zV/91V+Zl156yXz/+983ra2t5p//+Z+r19xxxx2mq6vL/OAHPzA///nPzWWXXfbuNDcbY8zf/d3fmYGBAZPP5835559vnnrqqRPdpRMCOsgknvrfuHFj9Zq5uTnzZ3/2Z2bevHmmtbXV/OEf/qHZu3fviev0CQQuLDo2xjzyyCNmxYoVJggCs2zZMnP//feL8iRJzC233GJ6e3tNEATmkksuMbt27XpHbSptgkKhyBxNp2NRKBQnP3RhUSgUmUMXFoVCkTl0YVEoFJlDFxaFQpE5dGFRKBSZQxcWhUKROXRhUSgUmUMXFoVCkTl0YVEoFJlDFxaFQpE5dGFRKBSZ4/8HUV8PvzsbfT8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "0.00236297445371747\n",
            "0.0023213138338178396\n",
            "0.0023487680591642857\n"
          ]
        }
      ],
      "source": [
        "# @title train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler(device)\n",
        "\n",
        "def train(model, optim, dataloader, scheduler=None):\n",
        "    model.train()\n",
        "    # for i, (x, _) in enumerate(dataloader):\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        # x1 = F.interpolate(x1, size=(16,16)).repeat(1,3,1,1)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x_, commitment_loss = model(x)\n",
        "            # loss = commitment_loss + F.mse_loss(x, x_)\n",
        "\n",
        "            x_ = model(x)\n",
        "            loss = F.mse_loss(x, x_)\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if i%10 == 0:\n",
        "            # with torch.no_grad():\n",
        "            #     state = buffer[12][40][0]\n",
        "                # transform = transforms.Compose([transforms.ToTensor()])\n",
        "            #     x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "            #     out = model(x).squeeze(0)\n",
        "            #     sx = model.encoder(x).squeeze(0)\n",
        "            #     out = model.decoder(sx).squeeze(0)\n",
        "            #     imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "            #     imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i % 100 == 0: print(loss.item())\n",
        "\n",
        "\n",
        "# for i in range(1):\n",
        "for i in range(10): # 10\n",
        "    print(i)\n",
        "    train(model, optim, train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # z = torch.randn(1,z_dim,8,8).to(device)\n",
        "        # _, z, _ = model.vq(z)\n",
        "        # z = model.quantise(z)\n",
        "\n",
        "        # out = model.decode(z)\n",
        "        # imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "        state = buffer[12][40][0]\n",
        "        transform = transforms.Compose([transforms.ToTensor()])\n",
        "        x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "        sx = model.encoder(x)\n",
        "        out = model.decoder(sx)\n",
        "        imshow(torchvision.utils.make_grid(x.cpu()))\n",
        "        imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "        imshow(torchvision.utils.make_grid(out.cpu()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "80ab0169-a6a1-42a3-b1d6-fe20a031a1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.00213</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">smart-frost-48</strong> at: <a href='https://wandb.ai/bobdole/vqvae/runs/0m90pl5i' target=\"_blank\">https://wandb.ai/bobdole/vqvae/runs/0m90pl5i</a><br> View project at: <a href='https://wandb.ai/bobdole/vqvae' target=\"_blank\">https://wandb.ai/bobdole/vqvae</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250324_005640-0m90pl5i/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250324_012606-ep62bj2n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/vqvae/runs/ep62bj2n' target=\"_blank\">easy-armadillo-49</a></strong> to <a href='https://wandb.ai/bobdole/vqvae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/vqvae' target=\"_blank\">https://wandb.ai/bobdole/vqvae</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/vqvae/runs/ep62bj2n' target=\"_blank\">https://wandb.ai/bobdole/vqvae/runs/ep62bj2n</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"vqvae\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXLAI-bVQ6G9"
      },
      "outputs": [],
      "source": [
        "# z = torch.randn(1,z_dim,16,16).to(device)\n",
        "# with torch.no_grad(): out = model.decode(z)\n",
        "# imshow(torchvision.utils.make_grid(out))\n",
        "\n",
        "with torch.no_grad():\n",
        "    state = buffer[12][40][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "    # out = model(x).squeeze(0)\n",
        "    sx = model.encoder(x)#.squeeze(0)\n",
        "    out = model.decoder(sx)#.squeeze(0)\n",
        "    imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "    imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "# out, _ = model(x)\n",
        "# imshow(torchvision.utils.make_grid(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI3uaeX-r73O"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yfLogBYztcuM"
      },
      "outputs": [],
      "source": [
        "# @title chatgpt quantizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_emb, emb_dim, beta=0.5, decay=0.99):\n",
        "        super().__init__()\n",
        "        self.num_emb, self.emb_dim = num_emb, emb_dim\n",
        "        self.beta, self.decay = beta, decay\n",
        "        self.epsilon = 1e-5\n",
        "\n",
        "        self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim))\n",
        "        # self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim).uniform_(-3**.5, 3**.5))\n",
        "        # self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim).uniform_(-1./num_emb, 1./num_emb))\n",
        "\n",
        "        # Register buffers for EMA updates.\n",
        "        self.register_buffer('ema_cluster_size', torch.zeros(num_emb))\n",
        "        self.register_buffer('ema_w', self.embeddings.data.clone())\n",
        "        # self.ema_cluster_size = nn.Parameter(torch.zeros(num_emb), requires_grad=False)\n",
        "        # self.ema_w = nn.Parameter(self.embeddings.data.clone(), requires_grad=False)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Save the original shape and flatten the input to (batch_size * ..., emb_dim)\n",
        "        input_shape = z.shape\n",
        "        # flat_z = z.view(-1, self.emb_dim)\n",
        "\n",
        "        # flat_z = z.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        flat_z = z.permute(0,2,3,1).flatten(end_dim=-2) # [b*h*w,c]\n",
        "        # print(flat_z.shape)\n",
        "\n",
        "        # distances = (torch.sum(flat_z ** 2, dim=1, keepdim=True) + torch.sum(self.embeddings ** 2, dim=1) - 2 * torch.matmul(flat_z, self.embeddings.t()))\n",
        "        distances = (torch.sum(flat_z**2, dim=1, keepdim=True) + torch.sum(self.embeddings**2, dim=1) - 2*torch.matmul(flat_z, self.embeddings.T))\n",
        "        enc_ind = torch.argmin(distances, dim=1)\n",
        "        encs = F.one_hot(enc_ind, self.num_emb).type(flat_z.dtype)\n",
        "        # Quantise the input by replacing with the nearest embedding.\n",
        "        z_q = torch.matmul(encs, self.embeddings).view(input_shape)\n",
        "\n",
        "        commitment_loss = self.beta * F.mse_loss(z_q.detach(), z) # commitment loss.\n",
        "        # loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q-z.detach())**2) # comitment, codebook\n",
        "\n",
        "        if self.training:\n",
        "            # EMA update for cluster size.\n",
        "            ema_cluster_size = torch.sum(encs, dim=0)\n",
        "            self.ema_cluster_size = self.ema_cluster_size * self.decay + (1 - self.decay) * ema_cluster_size\n",
        "            # Laplace smoothing to avoid zero counts.\n",
        "            n = torch.sum(self.ema_cluster_size)\n",
        "            self.ema_cluster_size = ((self.ema_cluster_size + self.epsilon) / (n + self.num_emb * self.epsilon)) * n\n",
        "\n",
        "            # EMA update for the embedding weights.\n",
        "            # dw = torch.matmul(encs.t(), flat_z)\n",
        "            dw = encs.T @ flat_z.detach()\n",
        "            # print('vq fwd', encs.T, flat_z)\n",
        "            self.ema_w = self.ema_w * self.decay + (1 - self.decay) * dw\n",
        "\n",
        "            # Update embeddings with the EMA values.\n",
        "            self.embeddings.data = self.ema_w / self.ema_cluster_size.unsqueeze(1)\n",
        "\n",
        "        # # Compute perplexity of the encs.\n",
        "        # avg_probs = torch.mean(encs, dim=0)\n",
        "        # perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + self.epsilon)))\n",
        "\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # print(z_q.shape)\n",
        "        # flat_z = z.permute(0,2,3,1).flatten(end_dim=-2) # [b*h*w,c]\n",
        "\n",
        "        # z_q = z_q.transpose(1,2).reshape(*bchw)\n",
        "        # return loss, z_q, perplexity, enc_ind\n",
        "        return commitment_loss, z_q, enc_ind\n",
        "\n",
        "emb_dim, num_emb = 4,20\n",
        "# x = torch.randn(2, 3, 4)\n",
        "x = torch.randn(2, emb_dim, 5, 7)\n",
        "vq = VectorQuantizerEMA(num_emb, emb_dim, beta=0.5) # chat gpt\n",
        "loss, z_q, enc_ind = vq(x)\n",
        "print(z_q.shape)\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4iRZMIxPsMTV"
      },
      "outputs": [],
      "source": [
        "# @title vqvae conv\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=32, z_dim=3):\n",
        "        super().__init__()\n",
        "        d_list=[16, 3] # 849126\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        kernel = 3\n",
        "        self.encoder = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, 16, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 14\n",
        "            nn.Conv2d(in_ch, d_model, kernel, 2, kernel//2), nn.BatchNorm2d(d_model), act,# nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(d_model, z_dim, kernel, 2, kernel//2), act\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, d_model, kernel, stride=2, padding=kernel//2, output_padding=1), nn.BatchNorm2d(d_model), act,\n",
        "            # nn.Upsample(scale_factor=2),\n",
        "            nn.ConvTranspose2d(d_model, in_ch, kernel, 2, padding=kernel//2, output_padding=1)\n",
        "        )\n",
        "        # self.vq = VectorQuantizerEMA(num_emb=20, emb_dim=z_dim, beta=0.5) # chat gpt\n",
        "        self.vq = FSQ(levels = z_dim*[8])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # print(x.shape)\n",
        "        commitment_loss, quantised, encoding_indices = self.vq(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(quantised)\n",
        "        return x, commitment_loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.quantise(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.quantise(x)\n",
        "    def decode(self, x):\n",
        "        # _, x, _ = self.vq(x)\n",
        "        x = self.quantise(x)\n",
        "        return self.decoder(x)\n",
        "    def quantise(self, x): # [b,c,h,w]->[b,h,w,c]->[b,c,h,w]\n",
        "        return self.vq(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
        "\n",
        "\n",
        "in_ch=3\n",
        "d_model=32\n",
        "z_dim=3\n",
        "model = VQVAE(in_ch, d_model, z_dim).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 16x16 conv 17651 ; pixel(3)(3)  ; (1)(1)  ; (3,7,15)(3,7)  ; (3,5,7)(3,5) 42706 ; 7,5 70226\n",
        "\n",
        "x = torch.randn((2, in_ch, 64, 64), device=device)\n",
        "# out, _ = model(x)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "psJQyxGNkOlE"
      },
      "outputs": [],
      "source": [
        "# @title vqvae from CompVis\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/models/autoencoder.py\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from contextlib import contextmanager\n",
        "\n",
        "from ldm.modules.diffusionmodules.model import Encoder, Decoder\n",
        "from ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "class VQModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 n_embed,\n",
        "                 embed_dim,\n",
        "                 remap=None,\n",
        "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "                 use_ema=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25, remap=remap, sane_index_shape=sane_index_shape)\n",
        "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "\n",
        "        self.use_ema = use_ema\n",
        "        if self.use_ema:\n",
        "            self.model_ema = LitEma(self)\n",
        "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
        "\n",
        "    @contextmanager\n",
        "    def ema_scope(self, context=None):\n",
        "        if self.use_ema:\n",
        "            self.model_ema.store(self.parameters())\n",
        "            self.model_ema.copy_to(self)\n",
        "            if context is not None:\n",
        "                print(f\"{context}: Switched to EMA weights\")\n",
        "        try:\n",
        "            yield None\n",
        "        finally:\n",
        "            if self.use_ema:\n",
        "                self.model_ema.restore(self.parameters())\n",
        "                if context is not None:\n",
        "                    print(f\"{context}: Restored training weights\")\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        if self.use_ema:\n",
        "            self.model_ema(self)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        # https://github.com/pytorch/pytorch/issues/37142\n",
        "        # try not to fool the heuristics\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # autoencode\n",
        "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
        "                                            predicted_indices=ind)\n",
        "\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # discriminator\n",
        "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        log_dict = self._validation_step(batch, batch_idx)\n",
        "        with self.ema_scope():\n",
        "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
        "        return log_dict\n",
        "\n",
        "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
        "                                        self.global_step,\n",
        "                                        last_layer=self.get_last_layer(),\n",
        "                                        split=\"val\"+suffix,\n",
        "                                        predicted_indices=ind\n",
        "                                        )\n",
        "\n",
        "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
        "                                            self.global_step,\n",
        "                                            last_layer=self.get_last_layer(),\n",
        "                                            split=\"val\"+suffix,\n",
        "                                            predicted_indices=ind\n",
        "                                            )\n",
        "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
        "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "\n",
        "class VQModelInterface(VQModel):\n",
        "    def __init__(self, embed_dim, *args, **kwargs):\n",
        "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, h, force_not_quantize=False):\n",
        "        # also go through quantization layer\n",
        "        if not force_not_quantize:\n",
        "            quant, emb_loss, info = self.quantize(h)\n",
        "        else:\n",
        "            quant = h\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "\n",
        "class AutoencoderKL(pl.LightningModule):\n",
        "    def __init__(self, ddconfig, lossconfig, embed_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        assert ddconfig[\"double_z\"]\n",
        "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, sample_posterior=True):\n",
        "        posterior = self.encode(input)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        return dec, posterior\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # train encoder+decoder+logvar\n",
        "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # train the discriminator\n",
        "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "\n",
        "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "\n",
        "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XgDIoz8Jm8B6"
      },
      "outputs": [],
      "source": [
        "# @title encoder from CompVis\n",
        "\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/cin-ldm-vq-f8.yaml\n",
        "# ddconfig:\n",
        "#     double_z: false\n",
        "#     z_channels: 4\n",
        "#     resolution: 256\n",
        "#     in_channels: 3\n",
        "#     out_ch: 3\n",
        "#     ch: 128\n",
        "#     ch_mult: 1,2,3,4\n",
        "#     num_res_blocks: 2\n",
        "#     attn_resolutions: 32\n",
        "\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/ffhq-ldm-vq-4.yaml\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/celebahq-ldm-vq-4.yaml\n",
        "# embed_dim: 3\n",
        "# n_embed: 8192 = 2^13 ~ 16^3\n",
        "# ddconfig:\n",
        "#   double_z: false\n",
        "#   z_channels: 3\n",
        "#   resolution: 256\n",
        "#   in_channels: 3\n",
        "#   out_ch: 3\n",
        "#   ch: 128\n",
        "#   ch_mult: 1,2,4\n",
        "#   num_res_blocks: 2\n",
        "#   attn_resolutions: []\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch=128, out_ch=3, ch_mult=(1,2,3,4), num_res_blocks=2,\n",
        "                 attn_resolutions=32, dropout=0.0, resamp_with_conv=True, in_channels=3,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult) # 1,1,2,3,4\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level] # 128 * 1,1,2,3,4\n",
        "            block_out = ch*ch_mult[i_level] # 128 * 1,2,3,4\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1: # downsample at all except last\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "encoder:\n",
        "convin k3s1\n",
        "res attn (),\n",
        "norm act convout k3s1\n",
        "\n",
        "self.conv_in = torch.nn.Conv2d(in_channels, self.ch, 3, 1, padding=3//2)\n",
        "\n",
        "res res down res res down\n",
        "res att res\n",
        "\n",
        "self.conv_out = nn.Sequential(\n",
        "    nn.GroupNorm(32, ch), nn.SiLU(), torch.nn.Conv2d(block_in, z_channels, 3, 1, padding=3//2)\n",
        ")\n",
        "\n",
        "me: down res res down res res\n",
        "down res att down res att = lvl lvl\n",
        "\n",
        "decoder:\n",
        "conv_in\n",
        "res att res\n",
        "res res up res res up\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nWb4L-uhmL4u"
      },
      "outputs": [],
      "source": [
        "# @title CompVis model.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L368\n",
        "\n",
        "# pytorch_diffusion + derived encoder decoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.modules.attention import LinearAttention\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
        "    From Fairseq.\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in tensor2tensor, but differs slightly\n",
        "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    # swish\n",
        "    return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def Normalize(in_channels, num_groups=32):\n",
        "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0,1,0,1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if temb_channels > 0:\n",
        "            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        if temb is not None:\n",
        "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class LinAttnBlock(LinearAttention):\n",
        "    \"\"\"to match AttnBlock usage\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = q.reshape(b,c,h*w)\n",
        "        q = q.permute(0,2,1)   # b,hw,c\n",
        "        k = k.reshape(b,c,h*w) # b,c,hw\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b,c,h*w)\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
        "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
        "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
        "    if attn_type == \"vanilla\":\n",
        "        return AttnBlock(in_channels)\n",
        "    elif attn_type == \"none\":\n",
        "        return nn.Identity(in_channels)\n",
        "    else:\n",
        "        return LinAttnBlock(in_channels)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.use_timestep = use_timestep\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            self.temb = nn.Module()\n",
        "            self.temb.dense = nn.ModuleList([\n",
        "                torch.nn.Linear(self.ch,\n",
        "                                self.temb_ch),\n",
        "                torch.nn.Linear(self.temb_ch,\n",
        "                                self.temb_ch),\n",
        "            ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, t=None, context=None):\n",
        "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
        "        if context is not None:\n",
        "            # assume aligned context, cat along channel axis\n",
        "            x = torch.cat((x, context), dim=1)\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            assert t is not None\n",
        "            temb = get_timestep_embedding(t, self.ch)\n",
        "            temb = self.temb.dense[0](temb)\n",
        "            temb = nonlinearity(temb)\n",
        "            temb = self.temb.dense[1](temb)\n",
        "        else:\n",
        "            temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.conv_out.weight\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
        "                 attn_type=\"vanilla\", **ignorekwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "        self.give_pre_end = give_pre_end\n",
        "        self.tanh_out = tanh_out\n",
        "\n",
        "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
        "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
        "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
        "        print(\"Working with z of shape {} = {} dimensions.\".format(self.z_shape, np.prod(self.z_shape)))\n",
        "\n",
        "        # z to block_in\n",
        "        self.conv_in = torch.nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #assert z.shape[1:] == self.z_shape[1:]\n",
        "        self.last_z_shape = z.shape\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # z to block_in\n",
        "        h = self.conv_in(z)\n",
        "\n",
        "        # middle\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](h, temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        if self.give_pre_end:\n",
        "            return h\n",
        "\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        if self.tanh_out:\n",
        "            h = torch.tanh(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
        "                                     ResnetBlock(in_channels=in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=2 * in_channels, out_channels=4 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=4 * in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
        "                                     Upsample(in_channels, with_conv=True)])\n",
        "        # end\n",
        "        self.norm_out = Normalize(in_channels)\n",
        "        self.conv_out = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.model):\n",
        "            if i in [1,2,3]:\n",
        "                x = layer(x, None)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        h = self.norm_out(x)\n",
        "        h = nonlinearity(h)\n",
        "        x = self.conv_out(h)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsampleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
        "                 ch_mult=(2,2), dropout=0.0):\n",
        "        super().__init__()\n",
        "        # upsampling\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        block_in = in_channels\n",
        "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            res_block = []\n",
        "            block_out = ch * ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                res_block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "            self.res_blocks.append(nn.ModuleList(res_block))\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                self.upsample_blocks.append(Upsample(block_in, True))\n",
        "                curr_res = curr_res * 2\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # upsampling\n",
        "        h = x\n",
        "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                h = self.res_blocks[i_level][i_block](h, None)\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                h = self.upsample_blocks[k](h)\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class LatentRescaler(nn.Module):\n",
        "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
        "        super().__init__()\n",
        "        # residual block, interpolate, residual block\n",
        "        self.factor = factor\n",
        "        self.conv_in = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "        self.attn = AttnBlock(mid_channels)\n",
        "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "\n",
        "        self.conv_out = nn.Conv2d(mid_channels, out_channels, kernel_size=1,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "        for block in self.res_block1:\n",
        "            x = block(x, None)\n",
        "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
        "        x = self.attn(x)\n",
        "        for block in self.res_block2:\n",
        "            x = block(x, None)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
        "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        intermediate_chn = ch * ch_mult[-1]\n",
        "        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
        "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
        "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
        "                               out_ch=None)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
        "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.rescaler(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleDecoder(nn.Module):\n",
        "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
        "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        tmp_chn = z_channels*ch_mult[-1]\n",
        "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
        "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
        "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
        "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampler(nn.Module):\n",
        "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
        "        super().__init__()\n",
        "        assert out_size >= in_size\n",
        "        num_blocks = int(np.log2(out_size//in_size))+1\n",
        "        factor_up = 1.+ (out_size % in_size)\n",
        "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
        "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
        "                                       out_channels=in_channels)\n",
        "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
        "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
        "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Resize(nn.Module):\n",
        "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
        "        super().__init__()\n",
        "        self.with_conv = learned\n",
        "        self.mode = mode\n",
        "        if self.with_conv:\n",
        "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
        "            raise NotImplementedError()\n",
        "            assert in_channels is not None\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, scale_factor=1.0):\n",
        "        if scale_factor==1.0:\n",
        "            return x\n",
        "        else:\n",
        "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
        "        return x\n",
        "\n",
        "class FirstStagePostProcessor(nn.Module):\n",
        "    def __init__(self, ch_mult:list, in_channels,\n",
        "                 pretrained_model:nn.Module=None,\n",
        "                 reshape=False,\n",
        "                 n_channels=None,\n",
        "                 dropout=0.,\n",
        "                 pretrained_config=None):\n",
        "        super().__init__()\n",
        "        if pretrained_config is None:\n",
        "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.pretrained_model = pretrained_model\n",
        "        else:\n",
        "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.instantiate_pretrained(pretrained_config)\n",
        "        self.do_reshape = reshape\n",
        "        if n_channels is None:\n",
        "            n_channels = self.pretrained_model.encoder.ch\n",
        "\n",
        "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
        "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3, stride=1,padding=1)\n",
        "        blocks = []\n",
        "        downs = []\n",
        "        ch_in = n_channels\n",
        "        for m in ch_mult:\n",
        "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
        "            ch_in = m * n_channels\n",
        "            downs.append(Downsample(ch_in, with_conv=False))\n",
        "        self.model = nn.ModuleList(blocks)\n",
        "        self.downsampler = nn.ModuleList(downs)\n",
        "\n",
        "\n",
        "    def instantiate_pretrained(self, config):\n",
        "        model = instantiate_from_config(config)\n",
        "        self.pretrained_model = model.eval()\n",
        "        # self.pretrained_model.train = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_with_pretrained(self,x):\n",
        "        c = self.pretrained_model.encode(x)\n",
        "        if isinstance(c, DiagonalGaussianDistribution):\n",
        "            c = c.mode()\n",
        "        return  c\n",
        "\n",
        "    def forward(self,x):\n",
        "        z_fs = self.encode_with_pretrained(x)\n",
        "        z = self.proj_norm(z_fs)\n",
        "        z = self.proj(z)\n",
        "        z = nonlinearity(z)\n",
        "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
        "            z = submodel(z,temb=None)\n",
        "            z = downmodel(z)\n",
        "        if self.do_reshape:\n",
        "            z = rearrange(z,'b c h w -> b (h w) c')\n",
        "        return z\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5bYiW-RnBdAi"
      },
      "outputs": [],
      "source": [
        "# @title quantizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# airalcorn2\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "        self.decay = decay\n",
        "        # Small constant to avoid numerical instability in embedding updates.\n",
        "        self.epsilon = 1e-5\n",
        "\n",
        "        # Dictionary embeddings.\n",
        "        limit = 3 ** 0.5\n",
        "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_( -limit, limit)\n",
        "        if use_ema: self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else: self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        # Exponential moving average of the cluster counts.\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
        "        distances = ((flat_x ** 2).sum(1, keepdim=True) - 2 * flat_x @ self.e_i_ts + (self.e_i_ts ** 2).sum(0, keepdim=True))\n",
        "        encoding_indices = distances.argmin(1)\n",
        "        quantized_x = F.embedding(encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        # See second term of Equation (3).\n",
        "        if not self.use_ema: dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else: dictionary_loss = None\n",
        "\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
        "                # Cluster counts.\n",
        "                encoding_one_hots = F.one_hot(encoding_indices, self.num_embeddings).type(flat_x.dtype)\n",
        "                n_i_ts = encoding_one_hots.sum(0)\n",
        "                # Updated exponential moving average of the cluster counts.\n",
        "                # See Equation (6).\n",
        "                self.N_i_ts(n_i_ts)\n",
        "\n",
        "                # Exponential moving average of the embeddings. See Equation (7).\n",
        "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
        "                self.m_i_ts(embed_sums)\n",
        "\n",
        "                # This is kind of weird.\n",
        "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
        "                # and Equation (8).\n",
        "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
        "                N_i_ts_stable = ((self.N_i_ts.average + self.epsilon) / (N_i_ts_sum + self.num_embeddings * self.epsilon) * N_i_ts_sum)\n",
        "\n",
        "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
        "\n",
        "        return (quantized_x, dictionary_loss, commitment_loss, encoding_indices.view(x.shape[0], -1),)\n",
        "\n",
        "\n",
        "# rosinality\n",
        "class Quantize(nn.Module):\n",
        "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.n_embed = n_embed\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        embed = torch.randn(dim, n_embed)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, input):\n",
        "        flatten = input.reshape(-1, self.dim)\n",
        "        dist = (flatten.pow(2).sum(1, keepdim=True) - 2 * flatten @ self.embed + self.embed.pow(2).sum(0, keepdim=True))\n",
        "        _, embed_ind = (-dist).max(1)\n",
        "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
        "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
        "        quantize = self.embed_code(embed_ind)\n",
        "\n",
        "        if self.training:\n",
        "            embed_onehot_sum = embed_onehot.sum(0)\n",
        "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
        "            dist_fn.all_reduce(embed_onehot_sum)\n",
        "            dist_fn.all_reduce(embed_sum)\n",
        "            self.cluster_size.data.mul_(self.decay).add_(embed_onehot_sum, alpha=1 - self.decay)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = ((self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n)\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "        diff = (quantize.detach() - input).pow(2).mean()\n",
        "        quantize = input + (quantize - input).detach()\n",
        "        return quantize, diff, embed_ind\n",
        "\n",
        "    def embed_code(self, embed_id):\n",
        "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
        "\n",
        "# CompVis\n",
        "from einops import rearrange\n",
        "class VectorQuantizer2(nn.Module): # https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py#L213\n",
        "    def __init__(self, n_e, e_dim, beta, sane_index_shape=False): # sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + torch.sum(self.embedding.weight**2, dim=1) - 2 * torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        return z_q, loss, min_encoding_indices\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "\n",
        "emb_dim, num_emb = 4,20\n",
        "# x = torch.randn(2, 3, 4)\n",
        "# x = torch.randn(2, 3, 4, 4)\n",
        "# vq = VectorQuantizer(emb_dim, num_emb, use_ema, decay)\n",
        "# vq = Quantize(emb_dim, num_emb, decay=0.99, eps=1e-5)\n",
        "vq = VectorQuantizer2(num_emb, emb_dim, beta=0.5, sane_index_shape=False) # CompVis\n",
        "out = vq(x)\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6gGjNCKog8za"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5)\n",
        "        shift = torch.tan(offset / half_l)\n",
        "        return torch.tanh(z + shift) * half_l - offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "print(fsq.codebook)\n",
        "\n",
        "batch_size, seq_len = 1, 1\n",
        "x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "lact = fsq.codes_to_indexes(la)\n",
        "print(lact)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bhCqoI0_fX1B"
      },
      "outputs": [],
      "source": [
        "# @title tried no round, LogitNormalCDF\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def logit(x): return torch.log(x/(1-x)) # x in (0,1)\n",
        "def LogitNormalCDF(x, mu=0, std=.5): # _/- for std<1.8; /-/ for std>1.8\n",
        "    cdf = 1/2 * (1 + torch.erf((logit(x)-mu)/(2**.5*std)))\n",
        "    return cdf\n",
        "\n",
        "# class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "#     def __init__(self, levels):\n",
        "#         super().__init__()\n",
        "#         self.eps = eps\n",
        "#         self.levels = torch.tensor(levels, device=device)\n",
        "#         # level in levels\n",
        "\n",
        "    # linear, normal,\n",
        "    # center = LogitNormalCDF(torch.linspace(0,1,level), mu=0, std=3)\n",
        "    # def forward(self, z):\n",
        "    #     z = F.sigmoid(z)\n",
        "    #     ind = torch.argmin((z-center).abs())\n",
        "\n",
        "\n",
        "    # threshold = LogitNormalCDF(torch.linspace(0,1,level+1), mu=0, std=3)[1:-1]\n",
        "\n",
        "    # def forward(self, z):\n",
        "    #     z = F.sigmoid(z)\n",
        "    #     center[ind]\n",
        "\n",
        "\n",
        "# linear, normal,\n",
        "\n",
        "center = [LogitNormalCDF(torch.linspace(0,1,level), mu=0, std=3) for level in levels]\n",
        "# print(center)\n",
        "# def forward(self, z):\n",
        "z = torch.linspace(-2,2,7).repeat(3,1).T\n",
        "\n",
        "z = F.sigmoid(z)\n",
        "# ind = [torch.argmin((z-c).abs()) for c in center]\n",
        "# print(ind)\n",
        "\n",
        "threshold = [LogitNormalCDF(torch.linspace(0,1,level+1), mu=0, std=1)[1:-1] for level in levels]\n",
        "print(threshold)\n",
        "\n",
        "def get_vjp(v):\n",
        "    return torch.autograd.grad(y, x, v)\n",
        "out = torch.vmap(get_vjp)(I_N)\n",
        "\n",
        "\n",
        "# def forward(self, z):\n",
        "#     z = F.sigmoid(z)\n",
        "#     center[ind]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "axZFsNiThSmT"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains vector_quantize_pytorch.py GroupedResidualVQ\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/vector_quantize_pytorch.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "from math import ceil\n",
        "from functools import partial, cache\n",
        "from itertools import zip_longest\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import Module, ModuleList\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from vector_quantize_pytorch.vector_quantize_pytorch import VectorQuantize\n",
        "\n",
        "from einops import rearrange, repeat, reduce, pack, unpack\n",
        "\n",
        "from einx import get_at\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def first(it):\n",
        "    return it[0]\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cast_tuple(t, length = 1):\n",
        "    return t if isinstance(t, tuple) else ((t,) * length)\n",
        "\n",
        "def unique(arr):\n",
        "    return list({*arr})\n",
        "\n",
        "def round_up_multiple(num, mult):\n",
        "    return ceil(num / mult) * mult\n",
        "\n",
        "# distributed helpers\n",
        "\n",
        "def is_distributed():\n",
        "    return dist.is_initialized() and dist.get_world_size() > 1\n",
        "\n",
        "def get_maybe_sync_seed(device, max_size = 10_000):\n",
        "    rand_int = torch.randint(0, max_size, (), device = device)\n",
        "\n",
        "    if is_distributed():\n",
        "        dist.all_reduce(rand_int)\n",
        "\n",
        "    return rand_int.item()\n",
        "\n",
        "# the mlp for generating the neural implicit codebook\n",
        "# from Huijben et al. https://arxiv.org/abs/2401.14732\n",
        "\n",
        "class MLP(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_hidden = None,\n",
        "        depth = 4,             # they used 4 layers in the paper\n",
        "        l2norm_output = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim_hidden = default(dim_hidden, dim)\n",
        "\n",
        "        self.proj_in = nn.Linear(2 * dim, dim)\n",
        "\n",
        "        layers = ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            layers.append(nn.Sequential(\n",
        "                nn.Linear(dim, dim_hidden),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(dim_hidden, dim)\n",
        "            ))\n",
        "\n",
        "        self.layers = layers\n",
        "        self.l2norm_output = l2norm_output\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        codes,\n",
        "        *,\n",
        "        condition\n",
        "    ):\n",
        "        one_headed = codes.ndim == 2\n",
        "\n",
        "        if one_headed:\n",
        "            codes = rearrange(codes, 'c d -> 1 c d')\n",
        "\n",
        "        heads, num_codes, batch, seq_len = codes.shape[0], codes.shape[-2], condition.shape[0], condition.shape[-2]\n",
        "\n",
        "        codes = repeat(codes, 'h c d -> h b n c d', n = seq_len, b = batch)\n",
        "        condition = repeat(condition, 'b n d -> h b n c d', c = num_codes, h = heads)\n",
        "\n",
        "        x = torch.cat((condition, codes), dim = -1)\n",
        "        x = self.proj_in(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x) + x\n",
        "\n",
        "        if self.l2norm_output:\n",
        "            x = F.normalize(x, dim = -1)\n",
        "\n",
        "        if not one_headed:\n",
        "            return x\n",
        "\n",
        "        return rearrange(x, '1 ... -> ...')\n",
        "\n",
        "# main class\n",
        "\n",
        "class ResidualVQ(Module):\n",
        "    \"\"\" Follows Algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        num_quantizers: int | None = None,\n",
        "        codebook_size: int | tuple[int, ...],\n",
        "        codebook_dim = None,\n",
        "        shared_codebook = False,\n",
        "        heads = 1,\n",
        "        quantize_dropout = False,\n",
        "        quantize_dropout_cutoff_index = 0,\n",
        "        quantize_dropout_multiple_of = 1,\n",
        "        accept_image_fmap = False,\n",
        "        implicit_neural_codebook = False, # QINCo from https://arxiv.org/abs/2401.14732\n",
        "        mlp_kwargs: dict = dict(),\n",
        "        **vq_kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert heads == 1, 'residual vq is not compatible with multi-headed codes'\n",
        "        assert exists(num_quantizers) or isinstance(codebook_size, tuple)\n",
        "\n",
        "        codebook_dim = default(codebook_dim, dim)\n",
        "        codebook_input_dim = codebook_dim * heads\n",
        "\n",
        "        requires_projection = codebook_input_dim != dim\n",
        "        self.project_in = nn.Linear(dim, codebook_input_dim) if requires_projection else nn.Identity()\n",
        "        self.project_out = nn.Linear(codebook_input_dim, dim) if requires_projection else nn.Identity()\n",
        "        self.has_projections = requires_projection\n",
        "\n",
        "        self.accept_image_fmap = accept_image_fmap\n",
        "\n",
        "        self.implicit_neural_codebook = implicit_neural_codebook\n",
        "\n",
        "        if implicit_neural_codebook:\n",
        "            vq_kwargs.update(\n",
        "                learnable_codebook = True,\n",
        "                ema_update = False\n",
        "            )\n",
        "\n",
        "        if shared_codebook:\n",
        "            vq_kwargs.update(\n",
        "                manual_ema_update = True,\n",
        "                manual_in_place_optimizer_update = True\n",
        "            )\n",
        "\n",
        "        # take care of maybe different codebook sizes across depth\n",
        "\n",
        "        codebook_sizes = cast_tuple(codebook_size, num_quantizers)\n",
        "\n",
        "        num_quantizers = default(num_quantizers, len(codebook_sizes))\n",
        "        assert len(codebook_sizes) == num_quantizers\n",
        "\n",
        "        self.num_quantizers = num_quantizers\n",
        "\n",
        "        self.codebook_sizes = codebook_sizes\n",
        "        self.uniform_codebook_size = len(unique(codebook_sizes)) == 1\n",
        "\n",
        "        # define vq across layers\n",
        "\n",
        "        self.layers = ModuleList([VectorQuantize(dim = codebook_dim, codebook_size = layer_codebook_size, codebook_dim = codebook_dim, accept_image_fmap = accept_image_fmap, **vq_kwargs) for layer_codebook_size in codebook_sizes])\n",
        "\n",
        "        assert all([not vq.has_projections for vq in self.layers])\n",
        "\n",
        "        self.quantize_dropout = quantize_dropout and num_quantizers > 1\n",
        "\n",
        "        assert quantize_dropout_cutoff_index >= 0\n",
        "\n",
        "        self.quantize_dropout_cutoff_index = quantize_dropout_cutoff_index\n",
        "        self.quantize_dropout_multiple_of = quantize_dropout_multiple_of  # encodec paper proposes structured dropout, believe this was set to 4\n",
        "\n",
        "        # setting up the MLPs for implicit neural codebooks\n",
        "\n",
        "        self.mlps = None\n",
        "\n",
        "        if implicit_neural_codebook:\n",
        "            self.mlps = ModuleList([MLP(dim = codebook_dim, l2norm_output = first(self.layers).use_cosine_sim, **mlp_kwargs) for _ in range(num_quantizers - 1)])\n",
        "        else:\n",
        "            self.mlps = (None,) * (num_quantizers - 1)\n",
        "\n",
        "        # sharing codebook logic\n",
        "\n",
        "        self.shared_codebook = shared_codebook\n",
        "\n",
        "        if not shared_codebook:\n",
        "            return\n",
        "\n",
        "        assert self.uniform_codebook_size\n",
        "\n",
        "        first_vq, *rest_vq = self.layers\n",
        "        codebook = first_vq._codebook\n",
        "\n",
        "        for vq in rest_vq:\n",
        "            vq._codebook = codebook\n",
        "\n",
        "    @property\n",
        "    def codebook_size(self):\n",
        "        return self.layers[0].codebook_size\n",
        "\n",
        "    @property\n",
        "    def codebook_dim(self):\n",
        "        return self.layers[0].codebook_dim\n",
        "\n",
        "    @property\n",
        "    def codebooks(self):\n",
        "        codebooks = [layer._codebook.embed for layer in self.layers]\n",
        "\n",
        "        codebooks = tuple(rearrange(codebook, '1 ... -> ...') for codebook in codebooks)\n",
        "\n",
        "        if not self.uniform_codebook_size:\n",
        "            return codebooks\n",
        "\n",
        "        codebooks = torch.stack(codebooks)\n",
        "        return codebooks\n",
        "\n",
        "    def get_codes_from_indices(self, indices):\n",
        "\n",
        "        batch, quantize_dim = indices.shape[0], indices.shape[-1]\n",
        "\n",
        "        # may also receive indices in the shape of 'b h w q' (accept_image_fmap)\n",
        "\n",
        "        indices, ps = pack([indices], 'b * q')\n",
        "\n",
        "        # because of quantize dropout, one can pass in indices that are coarse\n",
        "        # and the network should be able to reconstruct\n",
        "\n",
        "        if quantize_dim < self.num_quantizers:\n",
        "            assert self.quantize_dropout > 0., 'quantize dropout must be greater than 0 if you wish to reconstruct from a signal with less fine quantizations'\n",
        "            indices = F.pad(indices, (0, self.num_quantizers - quantize_dim), value = -1)\n",
        "\n",
        "        # take care of quantizer dropout\n",
        "\n",
        "        mask = indices == -1.\n",
        "        indices = indices.masked_fill(mask, 0) # have it fetch a dummy code to be masked out later\n",
        "\n",
        "        if not self.implicit_neural_codebook and self.uniform_codebook_size:\n",
        "\n",
        "            all_codes = get_at('q [c] d, b n q -> q b n d', self.codebooks, indices)\n",
        "\n",
        "        else:\n",
        "            # else if using implicit neural codebook, or non uniform codebook sizes, codes will need to be derived layer by layer\n",
        "\n",
        "            code_transform_mlps = (None, *self.mlps)\n",
        "\n",
        "            all_codes = []\n",
        "            quantized_out = 0.\n",
        "\n",
        "            for codes, indices, maybe_transform_mlp in zip(self.codebooks, indices.unbind(dim = -1), code_transform_mlps):\n",
        "\n",
        "                if exists(maybe_transform_mlp):\n",
        "                    codes = maybe_transform_mlp(codes, condition = quantized_out)\n",
        "                    layer_codes = get_at('b n [c] d, b n -> b n d', codes, indices)\n",
        "                else:\n",
        "                    layer_codes = get_at('[c] d, b n -> b n d', codes, indices)\n",
        "\n",
        "                all_codes.append(layer_codes)\n",
        "                quantized_out += layer_codes\n",
        "\n",
        "            all_codes = torch.stack(all_codes)\n",
        "\n",
        "        # mask out any codes that were dropout-ed\n",
        "\n",
        "        all_codes = all_codes.masked_fill(rearrange(mask, 'b n q -> q b n 1'), 0.)\n",
        "\n",
        "        # if (accept_image_fmap = True) then return shape (quantize, batch, height, width, dimension)\n",
        "\n",
        "        all_codes, = unpack(all_codes, ps, 'q b * d')\n",
        "\n",
        "        return all_codes\n",
        "\n",
        "    def get_output_from_indices(self, indices):\n",
        "        codes = self.get_codes_from_indices(indices)\n",
        "        codes_summed = reduce(codes, 'q ... -> ...', 'sum')\n",
        "        return self.project_out(codes_summed)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        mask = None,\n",
        "        indices: Tensor | list[Tensor] | None = None,\n",
        "        return_all_codes = False,\n",
        "        sample_codebook_temp = None,\n",
        "        freeze_codebook = False,\n",
        "        rand_quantize_dropout_fixed_seed = None\n",
        "    ):\n",
        "        num_quant, quant_dropout_multiple_of, return_loss, device = self.num_quantizers, self.quantize_dropout_multiple_of, exists(indices), x.device\n",
        "\n",
        "        x = self.project_in(x)\n",
        "\n",
        "        assert not (self.accept_image_fmap and exists(indices))\n",
        "\n",
        "        quantized_out = 0.\n",
        "        residual = x\n",
        "\n",
        "        all_losses = []\n",
        "        all_indices = []\n",
        "\n",
        "        if isinstance(indices, list):\n",
        "            indices = torch.stack(indices)\n",
        "\n",
        "        if return_loss:\n",
        "            assert not torch.any(indices == -1), 'some of the residual vq indices were dropped out. please use indices derived when the module is in eval mode to derive cross entropy loss'\n",
        "            ce_losses = []\n",
        "\n",
        "        should_quantize_dropout = self.training and self.quantize_dropout and not return_loss\n",
        "\n",
        "        # sample a layer index at which to dropout further residual quantization\n",
        "        # also prepare null indices and loss\n",
        "\n",
        "        if should_quantize_dropout:\n",
        "\n",
        "            # check if seed is manually passed in\n",
        "\n",
        "            if not exists(rand_quantize_dropout_fixed_seed):\n",
        "                rand_quantize_dropout_fixed_seed = get_maybe_sync_seed(device)\n",
        "\n",
        "            rand = random.Random(rand_quantize_dropout_fixed_seed)\n",
        "\n",
        "            rand_quantize_dropout_index = rand.randrange(self.quantize_dropout_cutoff_index, num_quant)\n",
        "\n",
        "            if quant_dropout_multiple_of != 1:\n",
        "                rand_quantize_dropout_index = round_up_multiple(rand_quantize_dropout_index + 1, quant_dropout_multiple_of) - 1\n",
        "\n",
        "            null_indices_shape = (x.shape[0], *x.shape[-2:]) if self.accept_image_fmap else tuple(x.shape[:2])\n",
        "            null_indices = torch.full(null_indices_shape, -1., device = device, dtype = torch.long)\n",
        "            null_loss = torch.full((1,), 0., device = device, dtype = x.dtype)\n",
        "\n",
        "        # setup the mlps for implicit neural codebook\n",
        "\n",
        "        maybe_code_transforms = (None,) * len(self.layers)\n",
        "\n",
        "        if self.implicit_neural_codebook:\n",
        "            maybe_code_transforms = (None, *self.mlps)\n",
        "\n",
        "        # save all inputs across layers, for use during expiration at end under shared codebook setting\n",
        "\n",
        "        all_residuals = []\n",
        "\n",
        "        # go through the layers\n",
        "\n",
        "        for quantizer_index, (vq, maybe_mlp) in enumerate(zip(self.layers, maybe_code_transforms)):\n",
        "\n",
        "            if should_quantize_dropout and quantizer_index > rand_quantize_dropout_index:\n",
        "                all_indices.append(null_indices)\n",
        "                all_losses.append(null_loss)\n",
        "                continue\n",
        "\n",
        "            layer_indices = None\n",
        "            if return_loss:\n",
        "                layer_indices = indices[..., quantizer_index]\n",
        "\n",
        "            # setup the transform code function to be passed into VectorQuantize forward\n",
        "\n",
        "            if exists(maybe_mlp):\n",
        "                maybe_mlp = partial(maybe_mlp, condition = quantized_out)\n",
        "\n",
        "            # save for expiration\n",
        "\n",
        "            all_residuals.append(residual)\n",
        "\n",
        "            # vector quantize forward\n",
        "\n",
        "            quantized, *rest = vq(\n",
        "                residual,\n",
        "                mask = mask,\n",
        "                indices = layer_indices,\n",
        "                sample_codebook_temp = sample_codebook_temp,\n",
        "                freeze_codebook = freeze_codebook,\n",
        "                codebook_transform_fn = maybe_mlp\n",
        "            )\n",
        "\n",
        "            residual = residual - quantized.detach()\n",
        "            quantized_out = quantized_out + quantized\n",
        "\n",
        "            if return_loss:\n",
        "                ce_loss = rest[0]\n",
        "                ce_losses.append(ce_loss)\n",
        "                continue\n",
        "\n",
        "            embed_indices, loss = rest\n",
        "\n",
        "            all_indices.append(embed_indices)\n",
        "            all_losses.append(loss)\n",
        "\n",
        "        # if shared codebook, update ema only at end\n",
        "\n",
        "        if self.training and self.shared_codebook:\n",
        "            shared_layer = first(self.layers)\n",
        "            shared_layer._codebook.update_ema()\n",
        "            shared_layer.update_in_place_optimizer()\n",
        "            shared_layer.expire_codes_(torch.cat(all_residuals, dim = -2))\n",
        "\n",
        "        # project out, if needed\n",
        "\n",
        "        quantized_out = self.project_out(quantized_out)\n",
        "\n",
        "        # whether to early return the cross entropy loss\n",
        "\n",
        "        if return_loss:\n",
        "            return quantized_out, sum(ce_losses)\n",
        "\n",
        "        # stack all losses and indices\n",
        "\n",
        "        all_losses, all_indices = map(partial(torch.stack, dim = -1), (all_losses, all_indices))\n",
        "\n",
        "        ret = (quantized_out, all_indices, all_losses)\n",
        "\n",
        "        if return_all_codes:\n",
        "            # whether to return all codes from all codebooks across layers\n",
        "            all_codes = self.get_codes_from_indices(all_indices)\n",
        "\n",
        "            # will return all codes in shape (quantizer, batch, sequence length, codebook dimension)\n",
        "            ret = (*ret, all_codes)\n",
        "\n",
        "        return ret\n",
        "\n",
        "# grouped residual vq\n",
        "\n",
        "class GroupedResidualVQ(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        groups = 1,\n",
        "        accept_image_fmap = False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.groups = groups\n",
        "        assert (dim % groups) == 0\n",
        "        dim_per_group = dim // groups\n",
        "\n",
        "        self.accept_image_fmap = accept_image_fmap\n",
        "\n",
        "        self.rvqs = ModuleList([])\n",
        "\n",
        "        for _ in range(groups):\n",
        "            self.rvqs.append(ResidualVQ(\n",
        "                dim = dim_per_group,\n",
        "                accept_image_fmap = accept_image_fmap,\n",
        "                **kwargs\n",
        "            ))\n",
        "\n",
        "    @property\n",
        "    def codebooks(self):\n",
        "        return torch.stack(tuple(rvq.codebooks for rvq in self.rvqs))\n",
        "\n",
        "    @property\n",
        "    def split_dim(self):\n",
        "        return 1 if self.accept_image_fmap else -1\n",
        "\n",
        "    def get_codes_from_indices(self, indices):\n",
        "        codes = tuple(rvq.get_codes_from_indices(chunk_indices) for rvq, chunk_indices in zip(self.rvqs, indices))\n",
        "        return torch.stack(codes)\n",
        "\n",
        "    def get_output_from_indices(self, indices):\n",
        "        outputs = tuple(rvq.get_output_from_indices(chunk_indices) for rvq, chunk_indices in zip(self.rvqs, indices))\n",
        "        return torch.cat(outputs, dim = self.split_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        indices = None,\n",
        "        return_all_codes = False,\n",
        "        sample_codebook_temp = None,\n",
        "        freeze_codebook = False,\n",
        "        mask = None,\n",
        "    ):\n",
        "        shape, split_dim, device = x.shape, self.split_dim, x.device\n",
        "        assert shape[split_dim] == self.dim\n",
        "\n",
        "        # split the feature dimension into groups\n",
        "\n",
        "        x = x.chunk(self.groups, dim = split_dim)\n",
        "\n",
        "        indices = default(indices, tuple())\n",
        "        return_ce_loss = len(indices) > 0\n",
        "        assert len(indices) == 0 or len(indices) == self.groups\n",
        "\n",
        "        forward_kwargs = dict(\n",
        "            return_all_codes = return_all_codes,\n",
        "            sample_codebook_temp = sample_codebook_temp,\n",
        "            mask = mask,\n",
        "            freeze_codebook = freeze_codebook,\n",
        "            rand_quantize_dropout_fixed_seed = get_maybe_sync_seed(device) if self.training else None\n",
        "        )\n",
        "\n",
        "        # invoke residual vq on each group\n",
        "\n",
        "        out = tuple(rvq(chunk, indices = chunk_indices, **forward_kwargs) for rvq, chunk, chunk_indices in zip_longest(self.rvqs, x, indices))\n",
        "        out = tuple(zip(*out))\n",
        "\n",
        "        # if returning cross entropy loss to rvq codebooks\n",
        "\n",
        "        if return_ce_loss:\n",
        "            quantized, ce_losses = out\n",
        "            return torch.cat(quantized, dim = split_dim), sum(ce_losses)\n",
        "\n",
        "        # otherwise, get all the zipped outputs and combine them\n",
        "\n",
        "        quantized, all_indices, commit_losses, *maybe_all_codes = out\n",
        "\n",
        "        quantized = torch.cat(quantized, dim = split_dim)\n",
        "        all_indices = torch.stack(all_indices)\n",
        "        commit_losses = torch.stack(commit_losses)\n",
        "\n",
        "        ret = (quantized, all_indices, commit_losses, *maybe_all_codes)\n",
        "        return ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dr2bajYVyjbN"
      },
      "outputs": [],
      "source": [
        "# @title CompVis vqvae\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VectorQuantizer2(nn.Module): # https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py#L213\n",
        "    def __init__(self, n_e, e_dim, beta, sane_index_shape=False): # sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        return z_q, loss, min_encoding_indices\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "\n",
        "\n",
        "# class VQModel(pl.LightningModule):\n",
        "class VQModel(nn.Module):\n",
        "    def __init__(self, n_embed, embed_dim,):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        z_channels = 4 # 4?\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25)\n",
        "        self.quant_conv = torch.nn.Conv2d(z_channels, embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, z_channels, 1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cS-HV67C8S_m"
      },
      "outputs": [],
      "source": [
        "# @title airalcorn2 vqvae\n",
        "# https://github.com/airalcorn2/vqvae-pytorch/blob/master/vqvae.py\n",
        "# Ported from: https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb.\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        layers = []\n",
        "        for i in range(num_residual_layers):\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_hiddens,\n",
        "                        out_channels=num_residual_hiddens,\n",
        "                        kernel_size=3,\n",
        "                        padding=1,\n",
        "                    ),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_residual_hiddens,\n",
        "                        out_channels=num_hiddens,\n",
        "                        kernel_size=1,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for layer in self.layers:\n",
        "            h = h + layer(h)\n",
        "\n",
        "        # ResNet V1-style.\n",
        "        return torch.relu(h)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        # The last ReLU from the Sonnet example is omitted because ResidualStack starts\n",
        "        # off with a ReLU.\n",
        "        conv = nn.Sequential()\n",
        "        for downsampling_layer in range(num_downsampling_layers):\n",
        "            if downsampling_layer == 0:\n",
        "                out_channels = num_hiddens // 2\n",
        "            elif downsampling_layer == 1:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, num_hiddens)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            conv.add_module(\n",
        "                f\"down{downsampling_layer}\",\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            conv.add_module(f\"relu{downsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        conv.add_module(\n",
        "            \"final_conv\",\n",
        "            nn.Conv2d(\n",
        "                in_channels=num_hiddens,\n",
        "                out_channels=num_hiddens,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "        )\n",
        "        self.conv = conv\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        return self.residual_stack(h)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        num_hiddens,\n",
        "        num_upsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=embedding_dim,\n",
        "            out_channels=num_hiddens,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "        upconv = nn.Sequential()\n",
        "        for upsampling_layer in range(num_upsampling_layers):\n",
        "            if upsampling_layer < num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            elif upsampling_layer == num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens // 2)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, 3)\n",
        "\n",
        "            upconv.add_module(\n",
        "                f\"up{upsampling_layer}\",\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            if upsampling_layer < num_upsampling_layers - 1:\n",
        "                upconv.add_module(f\"relu{upsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        self.upconv = upconv\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h = self.residual_stack(h)\n",
        "        x_recon = self.upconv(h)\n",
        "        return x_recon\n",
        "\n",
        "\n",
        "class SonnetExponentialMovingAverage(nn.Module):\n",
        "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
        "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
        "    # of \"Neural Discrete Representation Learning\".\n",
        "    def __init__(self, decay, shape):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.counter = 0\n",
        "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
        "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
        "\n",
        "    def update(self, value):\n",
        "        self.counter += 1\n",
        "        with torch.no_grad():\n",
        "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
        "            self.average = self.hidden / (1 - self.decay ** self.counter)\n",
        "\n",
        "    def __call__(self, value):\n",
        "        self.update(value)\n",
        "        return self.average\n",
        "\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay, epsilon):\n",
        "        super().__init__()\n",
        "        # See Section 3 of \"Neural Discrete Representation Learning\" and:\n",
        "        # https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L142.\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "        # Weight for the exponential moving average.\n",
        "        self.decay = decay\n",
        "        # Small constant to avoid numerical instability in embedding updates.\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Dictionary embeddings.\n",
        "        limit = 3 ** 0.5\n",
        "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_( -limit, limit)\n",
        "        if use_ema: self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else: self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        # Exponential moving average of the cluster counts.\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
        "        distances = ((flat_x ** 2).sum(1, keepdim=True) - 2 * flat_x @ self.e_i_ts + (self.e_i_ts ** 2).sum(0, keepdim=True))\n",
        "        encoding_indices = distances.argmin(1)\n",
        "        quantized_x = F.embedding(encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        # See second term of Equation (3).\n",
        "        if not self.use_ema:\n",
        "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else:\n",
        "            dictionary_loss = None\n",
        "\n",
        "        # See third term of Equation (3).\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        # Straight-through gradient. See Section 3.2.\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
        "                # Cluster counts.\n",
        "                encoding_one_hots = F.one_hot(encoding_indices, self.num_embeddings).type(flat_x.dtype)\n",
        "                n_i_ts = encoding_one_hots.sum(0)\n",
        "                # Updated exponential moving average of the cluster counts.\n",
        "                # See Equation (6).\n",
        "                self.N_i_ts(n_i_ts)\n",
        "\n",
        "                # Exponential moving average of the embeddings. See Equation (7).\n",
        "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
        "                self.m_i_ts(embed_sums)\n",
        "\n",
        "                # This is kind of weird.\n",
        "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
        "                # and Equation (8).\n",
        "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
        "                N_i_ts_stable = ((self.N_i_ts.average + self.epsilon) / (N_i_ts_sum + self.num_embeddings * self.epsilon) * N_i_ts_sum)\n",
        "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
        "        return (quantized_x, dictionary_loss, commitment_loss, encoding_indices.view(x.shape[0], -1),)\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "        embedding_dim,\n",
        "        num_embeddings,\n",
        "        use_ema,\n",
        "        decay,\n",
        "        epsilon,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens,\n",
        "        )\n",
        "        self.pre_vq_conv = nn.Conv2d(\n",
        "            in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1\n",
        "        )\n",
        "        self.vq = VectorQuantizer(\n",
        "            embedding_dim, num_embeddings, use_ema, decay, epsilon\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            embedding_dim,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens,\n",
        "        )\n",
        "\n",
        "    def quantize(self, x):\n",
        "        z = self.pre_vq_conv(self.encoder(x))\n",
        "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = self.vq(z)\n",
        "        return (z_quantized, dictionary_loss, commitment_loss, encoding_indices)\n",
        "\n",
        "    def forward(self, x):\n",
        "        (z_quantized, dictionary_loss, commitment_loss, _) = self.quantize(x)\n",
        "        x_recon = self.decoder(z_quantized)\n",
        "        return {\n",
        "            \"dictionary_loss\": dictionary_loss,\n",
        "            \"commitment_loss\": commitment_loss,\n",
        "            \"x_recon\": x_recon,\n",
        "        }\n",
        "\n",
        "# https://github.com/airalcorn2/vqvae-pytorch/blob/master/train_vqvae.py\n",
        "        # out = model(imgs)\n",
        "        # recon_error = criterion(out[\"x_recon\"], imgs) / train_data_variance\n",
        "        # total_recon_error += recon_error.item()\n",
        "        # loss = recon_error + beta * out[\"commitment_loss\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "15v0h5A74X2Z"
      },
      "outputs": [],
      "source": [
        "# @title rosinality from sonet\n",
        "# https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import distributed as dist_fn\n",
        "\n",
        "\n",
        "# Copyright 2018 The Sonnet Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Borrowed from https://github.com/deepmind/sonnet and ported it to PyTorch\n",
        "\n",
        "\n",
        "class Quantize(nn.Module):\n",
        "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.n_embed = n_embed\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        embed = torch.randn(dim, n_embed)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, input):\n",
        "        flatten = input.reshape(-1, self.dim)\n",
        "        dist = (flatten.pow(2).sum(1, keepdim=True) - 2 * flatten @ self.embed + self.embed.pow(2).sum(0, keepdim=True))\n",
        "        _, embed_ind = (-dist).max(1)\n",
        "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
        "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
        "        quantize = self.embed_code(embed_ind)\n",
        "\n",
        "        if self.training:\n",
        "            embed_onehot_sum = embed_onehot.sum(0)\n",
        "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
        "            dist_fn.all_reduce(embed_onehot_sum)\n",
        "            dist_fn.all_reduce(embed_sum)\n",
        "            self.cluster_size.data.mul_(self.decay).add_(embed_onehot_sum, alpha=1 - self.decay)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = ((self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n)\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "        diff = (quantize.detach() - input).pow(2).mean()\n",
        "        quantize = input + (quantize - input).detach()\n",
        "        return quantize, diff, embed_ind\n",
        "\n",
        "    def embed_code(self, embed_id):\n",
        "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channel, channel, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel, in_channel, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv(input)\n",
        "        out += input\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channel, channel, n_res_block, n_res_channel, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        if stride == 4:\n",
        "            blocks = [\n",
        "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 2, channel, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel, channel, 3, padding=1),\n",
        "            ]\n",
        "\n",
        "        elif stride == 2:\n",
        "            blocks = [\n",
        "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 2, channel, 3, padding=1),\n",
        "            ]\n",
        "\n",
        "        for i in range(n_res_block):\n",
        "            blocks.append(ResBlock(channel, n_res_channel))\n",
        "\n",
        "        blocks.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.blocks(input)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channel, out_channel, channel, n_res_block, n_res_channel, stride\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        blocks = [nn.Conv2d(in_channel, channel, 3, padding=1)]\n",
        "\n",
        "        for i in range(n_res_block):\n",
        "            blocks.append(ResBlock(channel, n_res_channel))\n",
        "\n",
        "        blocks.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        if stride == 4:\n",
        "            blocks.extend(\n",
        "                [\n",
        "                    nn.ConvTranspose2d(channel, channel // 2, 4, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.ConvTranspose2d(\n",
        "                        channel // 2, out_channel, 4, stride=2, padding=1\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        elif stride == 2:\n",
        "            blocks.append(\n",
        "                nn.ConvTranspose2d(channel, out_channel, 4, stride=2, padding=1)\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.blocks(input)\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel=3,\n",
        "        channel=128,\n",
        "        n_res_block=2,\n",
        "        n_res_channel=32,\n",
        "        embed_dim=64,\n",
        "        n_embed=512,\n",
        "        decay=0.99,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_b = Encoder(in_channel, channel, n_res_block, n_res_channel, stride=4)\n",
        "        self.enc_t = Encoder(channel, channel, n_res_block, n_res_channel, stride=2)\n",
        "        self.quantize_conv_t = nn.Conv2d(channel, embed_dim, 1)\n",
        "        self.quantize_t = Quantize(embed_dim, n_embed)\n",
        "        self.dec_t = Decoder(\n",
        "            embed_dim, embed_dim, channel, n_res_block, n_res_channel, stride=2\n",
        "        )\n",
        "        self.quantize_conv_b = nn.Conv2d(embed_dim + channel, embed_dim, 1)\n",
        "        self.quantize_b = Quantize(embed_dim, n_embed)\n",
        "        self.upsample_t = nn.ConvTranspose2d(\n",
        "            embed_dim, embed_dim, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.dec = Decoder(\n",
        "            embed_dim + embed_dim,\n",
        "            in_channel,\n",
        "            channel,\n",
        "            n_res_block,\n",
        "            n_res_channel,\n",
        "            stride=4,\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        quant_t, quant_b, diff, _, _ = self.encode(input)\n",
        "        dec = self.decode(quant_t, quant_b)\n",
        "\n",
        "        return dec, diff\n",
        "\n",
        "    def encode(self, input):\n",
        "        enc_b = self.enc_b(input)\n",
        "        enc_t = self.enc_t(enc_b)\n",
        "\n",
        "        quant_t = self.quantize_conv_t(enc_t).permute(0, 2, 3, 1)\n",
        "        quant_t, diff_t, id_t = self.quantize_t(quant_t)\n",
        "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
        "        diff_t = diff_t.unsqueeze(0)\n",
        "\n",
        "        dec_t = self.dec_t(quant_t)\n",
        "        enc_b = torch.cat([dec_t, enc_b], 1)\n",
        "\n",
        "        quant_b = self.quantize_conv_b(enc_b).permute(0, 2, 3, 1)\n",
        "        quant_b, diff_b, id_b = self.quantize_b(quant_b)\n",
        "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
        "        diff_b = diff_b.unsqueeze(0)\n",
        "\n",
        "        return quant_t, quant_b, diff_t + diff_b, id_t, id_b\n",
        "\n",
        "    def decode(self, quant_t, quant_b):\n",
        "        upsample_t = self.upsample_t(quant_t)\n",
        "        quant = torch.cat([upsample_t, quant_b], 1)\n",
        "        dec = self.dec(quant)\n",
        "\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_t, code_b):\n",
        "        quant_t = self.quantize_t.embed_code(code_t)\n",
        "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
        "        quant_b = self.quantize_b.embed_code(code_b)\n",
        "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
        "\n",
        "        dec = self.decode(quant_t, quant_b)\n",
        "\n",
        "        return dec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GRVHtJAs0bXp"
      },
      "outputs": [],
      "source": [
        "# @title CompVis stable-diffusion model.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L368\n",
        "\n",
        "# pytorch_diffusion + derived encoder decoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.modules.attention import LinearAttention\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
        "    From Fairseq.\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in tensor2tensor, but differs slightly\n",
        "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    # swish\n",
        "    return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def Normalize(in_channels, num_groups=32):\n",
        "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0,1,0,1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if temb_channels > 0:\n",
        "            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        if temb is not None:\n",
        "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class LinAttnBlock(LinearAttention):\n",
        "    \"\"\"to match AttnBlock usage\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = q.reshape(b,c,h*w)\n",
        "        q = q.permute(0,2,1)   # b,hw,c\n",
        "        k = k.reshape(b,c,h*w) # b,c,hw\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b,c,h*w)\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
        "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
        "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
        "    if attn_type == \"vanilla\":\n",
        "        return AttnBlock(in_channels)\n",
        "    elif attn_type == \"none\":\n",
        "        return nn.Identity(in_channels)\n",
        "    else:\n",
        "        return LinAttnBlock(in_channels)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.use_timestep = use_timestep\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            self.temb = nn.Module()\n",
        "            self.temb.dense = nn.ModuleList([\n",
        "                torch.nn.Linear(self.ch,\n",
        "                                self.temb_ch),\n",
        "                torch.nn.Linear(self.temb_ch,\n",
        "                                self.temb_ch),\n",
        "            ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, t=None, context=None):\n",
        "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
        "        if context is not None:\n",
        "            # assume aligned context, cat along channel axis\n",
        "            x = torch.cat((x, context), dim=1)\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            assert t is not None\n",
        "            temb = get_timestep_embedding(t, self.ch)\n",
        "            temb = self.temb.dense[0](temb)\n",
        "            temb = nonlinearity(temb)\n",
        "            temb = self.temb.dense[1](temb)\n",
        "        else:\n",
        "            temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.conv_out.weight\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
        "                 attn_type=\"vanilla\", **ignorekwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "        self.give_pre_end = give_pre_end\n",
        "        self.tanh_out = tanh_out\n",
        "\n",
        "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
        "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
        "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
        "        print(\"Working with z of shape {} = {} dimensions.\".format(self.z_shape, np.prod(self.z_shape)))\n",
        "\n",
        "        # z to block_in\n",
        "        self.conv_in = torch.nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #assert z.shape[1:] == self.z_shape[1:]\n",
        "        self.last_z_shape = z.shape\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # z to block_in\n",
        "        h = self.conv_in(z)\n",
        "\n",
        "        # middle\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](h, temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        if self.give_pre_end:\n",
        "            return h\n",
        "\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        if self.tanh_out:\n",
        "            h = torch.tanh(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
        "                                     ResnetBlock(in_channels=in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=2 * in_channels, out_channels=4 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=4 * in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
        "                                     Upsample(in_channels, with_conv=True)])\n",
        "        # end\n",
        "        self.norm_out = Normalize(in_channels)\n",
        "        self.conv_out = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.model):\n",
        "            if i in [1,2,3]:\n",
        "                x = layer(x, None)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        h = self.norm_out(x)\n",
        "        h = nonlinearity(h)\n",
        "        x = self.conv_out(h)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsampleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
        "                 ch_mult=(2,2), dropout=0.0):\n",
        "        super().__init__()\n",
        "        # upsampling\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        block_in = in_channels\n",
        "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            res_block = []\n",
        "            block_out = ch * ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                res_block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "            self.res_blocks.append(nn.ModuleList(res_block))\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                self.upsample_blocks.append(Upsample(block_in, True))\n",
        "                curr_res = curr_res * 2\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # upsampling\n",
        "        h = x\n",
        "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                h = self.res_blocks[i_level][i_block](h, None)\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                h = self.upsample_blocks[k](h)\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class LatentRescaler(nn.Module):\n",
        "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
        "        super().__init__()\n",
        "        # residual block, interpolate, residual block\n",
        "        self.factor = factor\n",
        "        self.conv_in = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "        self.attn = AttnBlock(mid_channels)\n",
        "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "\n",
        "        self.conv_out = nn.Conv2d(mid_channels, out_channels, kernel_size=1,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "        for block in self.res_block1:\n",
        "            x = block(x, None)\n",
        "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
        "        x = self.attn(x)\n",
        "        for block in self.res_block2:\n",
        "            x = block(x, None)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
        "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        intermediate_chn = ch * ch_mult[-1]\n",
        "        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
        "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
        "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
        "                               out_ch=None)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
        "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.rescaler(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleDecoder(nn.Module):\n",
        "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
        "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        tmp_chn = z_channels*ch_mult[-1]\n",
        "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
        "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
        "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
        "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampler(nn.Module):\n",
        "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
        "        super().__init__()\n",
        "        assert out_size >= in_size\n",
        "        num_blocks = int(np.log2(out_size//in_size))+1\n",
        "        factor_up = 1.+ (out_size % in_size)\n",
        "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
        "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
        "                                       out_channels=in_channels)\n",
        "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
        "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
        "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Resize(nn.Module):\n",
        "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
        "        super().__init__()\n",
        "        self.with_conv = learned\n",
        "        self.mode = mode\n",
        "        if self.with_conv:\n",
        "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
        "            raise NotImplementedError()\n",
        "            assert in_channels is not None\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, scale_factor=1.0):\n",
        "        if scale_factor==1.0:\n",
        "            return x\n",
        "        else:\n",
        "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
        "        return x\n",
        "\n",
        "class FirstStagePostProcessor(nn.Module):\n",
        "    def __init__(self, ch_mult:list, in_channels,\n",
        "                 pretrained_model:nn.Module=None,\n",
        "                 reshape=False,\n",
        "                 n_channels=None,\n",
        "                 dropout=0.,\n",
        "                 pretrained_config=None):\n",
        "        super().__init__()\n",
        "        if pretrained_config is None:\n",
        "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.pretrained_model = pretrained_model\n",
        "        else:\n",
        "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.instantiate_pretrained(pretrained_config)\n",
        "        self.do_reshape = reshape\n",
        "        if n_channels is None:\n",
        "            n_channels = self.pretrained_model.encoder.ch\n",
        "\n",
        "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
        "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3, stride=1,padding=1)\n",
        "        blocks = []\n",
        "        downs = []\n",
        "        ch_in = n_channels\n",
        "        for m in ch_mult:\n",
        "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
        "            ch_in = m * n_channels\n",
        "            downs.append(Downsample(ch_in, with_conv=False))\n",
        "        self.model = nn.ModuleList(blocks)\n",
        "        self.downsampler = nn.ModuleList(downs)\n",
        "\n",
        "\n",
        "    def instantiate_pretrained(self, config):\n",
        "        model = instantiate_from_config(config)\n",
        "        self.pretrained_model = model.eval()\n",
        "        # self.pretrained_model.train = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_with_pretrained(self,x):\n",
        "        c = self.pretrained_model.encode(x)\n",
        "        if isinstance(c, DiagonalGaussianDistribution):\n",
        "            c = c.mode()\n",
        "        return  c\n",
        "\n",
        "    def forward(self,x):\n",
        "        z_fs = self.encode_with_pretrained(x)\n",
        "        z = self.proj_norm(z_fs)\n",
        "        z = self.proj(z)\n",
        "        z = nonlinearity(z)\n",
        "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
        "            z = submodel(z,temb=None)\n",
        "            z = downmodel(z)\n",
        "        if self.do_reshape:\n",
        "            z = rearrange(z,'b c h w -> b (h w) c')\n",
        "        return z\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BwCLJCY_dFD6"
      },
      "outputs": [],
      "source": [
        "# @title CompVis taming-transformers vqvae quantize.py\n",
        "# https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import einsum\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    \"\"\"\n",
        "    see https://github.com/MishaLaskin/vqvae/blob/d761a999e2267766400dc646d82d3ac3657771d4/models/quantizer.py\n",
        "    ____________________________________________\n",
        "    Discretization bottleneck part of the VQ-VAE.\n",
        "    Inputs:\n",
        "    - n_e : number of embeddings\n",
        "    - e_dim : dimension of embedding\n",
        "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
        "    _____________________________________________\n",
        "    \"\"\"\n",
        "\n",
        "    # NOTE: this class contains a bug regarding beta; see VectorQuantizer2 for\n",
        "    # a fix and use legacy=False to apply that fix. VectorQuantizer2 can be\n",
        "    # used wherever VectorQuantizer has been used before and is additionally\n",
        "    # more efficient.\n",
        "    def __init__(self, n_e, e_dim, beta):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Inputs the output of the encoder network z and maps it to a discrete\n",
        "        one-hot vector that is the index of the closest embedding vector e_j\n",
        "        z (continuous) -> z_q (discrete)\n",
        "        z.shape = (batch, channel, height, width)\n",
        "        quantization pipeline:\n",
        "            1. get encoder input (B,C,H,W)\n",
        "            2. flatten input to (B*H*W,C)\n",
        "        \"\"\"\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = z.permute(0, 2, 3, 1).contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.matmul(z_flattened, self.embedding.weight.t())\n",
        "\n",
        "        ## could possible replace this here\n",
        "        # #\\start...\n",
        "        # find closest encodings\n",
        "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
        "\n",
        "        min_encodings = torch.zeros(\n",
        "            min_encoding_indices.shape[0], self.n_e).to(z)\n",
        "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
        "\n",
        "        # dtype min encodings: torch.float32\n",
        "        # min_encodings shape: torch.Size([2048, 512])\n",
        "        # min_encoding_indices.shape: torch.Size([2048, 1])\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
        "        #.........\\end\n",
        "\n",
        "        # with:\n",
        "        # .........\\start\n",
        "        #min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        #z_q = self.embedding(min_encoding_indices)\n",
        "        # ......\\end......... (TODO)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
        "            torch.mean((z_q - z.detach()) ** 2)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # perplexity\n",
        "        e_mean = torch.mean(min_encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # shape specifying (batch, height, width, channel)\n",
        "        # TODO: check for more easy handling with nn.Embedding\n",
        "        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n",
        "        min_encodings.scatter_(1, indices[:,None], 1)\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n",
        "\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q\n",
        "\n",
        "\n",
        "class GumbelQuantize(nn.Module):\n",
        "    \"\"\"\n",
        "    credit to @karpathy: https://github.com/karpathy/deep-vector-quantization/blob/main/model.py (thanks!)\n",
        "    Gumbel Softmax trick quantizer\n",
        "    Categorical Reparameterization with Gumbel-Softmax, Jang et al. 2016\n",
        "    https://arxiv.org/abs/1611.01144\n",
        "    \"\"\"\n",
        "    def __init__(self, num_hiddens, embedding_dim, n_embed, straight_through=True, kl_weight=5e-4, temp_init=1.0, use_vqinterface=True, remap=None, unknown_index=\"random\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_embed = n_embed\n",
        "\n",
        "        self.straight_through = straight_through\n",
        "        self.temperature = temp_init\n",
        "        self.kl_weight = kl_weight\n",
        "\n",
        "        self.proj = nn.Conv2d(num_hiddens, n_embed, 1)\n",
        "        self.embed = nn.Embedding(n_embed, embedding_dim)\n",
        "\n",
        "        self.use_vqinterface = use_vqinterface\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_embed} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_embed\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z, temp=None, return_logits=False):\n",
        "        # force hard = True when we are in eval mode, as we must quantize. actually, always true seems to work\n",
        "        hard = self.straight_through if self.training else True\n",
        "        temp = self.temperature if temp is None else temp\n",
        "\n",
        "        logits = self.proj(z)\n",
        "        if self.remap is not None:\n",
        "            # continue only with used logits\n",
        "            full_zeros = torch.zeros_like(logits)\n",
        "            logits = logits[:,self.used,...]\n",
        "\n",
        "        soft_one_hot = F.gumbel_softmax(logits, tau=temp, dim=1, hard=hard)\n",
        "        if self.remap is not None:\n",
        "            # go back to all entries but unused set to zero\n",
        "            full_zeros[:,self.used,...] = soft_one_hot\n",
        "            soft_one_hot = full_zeros\n",
        "        z_q = einsum('b n h w, n d -> b d h w', soft_one_hot, self.embed.weight)\n",
        "\n",
        "        # + kl divergence to the prior loss\n",
        "        qy = F.softmax(logits, dim=1)\n",
        "        diff = self.kl_weight * torch.sum(qy * torch.log(qy * self.n_embed + 1e-10), dim=1).mean()\n",
        "\n",
        "        ind = soft_one_hot.argmax(dim=1)\n",
        "        if self.remap is not None:\n",
        "            ind = self.remap_to_used(ind)\n",
        "        if self.use_vqinterface:\n",
        "            if return_logits:\n",
        "                return z_q, diff, (None, None, ind), logits\n",
        "            return z_q, diff, (None, None, ind)\n",
        "        return z_q, diff, ind\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        b, h, w, c = shape\n",
        "        assert b*h*w == indices.shape[0]\n",
        "        indices = rearrange(indices, '(b h w) -> b h w', b=b, h=h, w=w)\n",
        "        if self.remap is not None:\n",
        "            indices = self.unmap_to_all(indices)\n",
        "        one_hot = F.one_hot(indices, num_classes=self.n_embed).permute(0, 3, 1, 2).float()\n",
        "        z_q = einsum('b n h w, n d -> b d h w', one_hot, self.embed.weight)\n",
        "        return z_q\n",
        "\n",
        "\n",
        "class VectorQuantizer2(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly\n",
        "    avoids costly matrix multiplications and allows for post-hoc remapping of indices.\n",
        "    \"\"\"\n",
        "    # NOTE: due to a bug the beta term was applied to the wrong term. for\n",
        "    # backwards compatibility we use the buggy version by default, but you can\n",
        "    # specify legacy=False to fix it.\n",
        "    def __init__(self, n_e, e_dim, beta, remap=None, unknown_index=\"random\", sane_index_shape=False, legacy=True):\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "        self.legacy = legacy\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_e\n",
        "\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z, temp=None, rescale_logits=False, return_logits=False):\n",
        "        assert temp is None or temp==1.0, \"Only for interface compatible with Gumbel\"\n",
        "        assert rescale_logits==False, \"Only for interface compatible with Gumbel\"\n",
        "        assert return_logits==False, \"Only for interface compatible with Gumbel\"\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "        perplexity = None\n",
        "        min_encodings = None\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        # loss = torch.mean((z_q.detach()-z)**2) + self.beta * torch.mean((z_q - z.detach()) ** 2) # legacy\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        if self.remap is not None:\n",
        "            min_encoding_indices = min_encoding_indices.reshape(z.shape[0],-1) # add batch axis\n",
        "            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n",
        "            min_encoding_indices = min_encoding_indices.reshape(-1,1) # flatten\n",
        "        if self.sane_index_shape:\n",
        "            min_encoding_indices = min_encoding_indices.reshape(\n",
        "                z_q.shape[0], z_q.shape[2], z_q.shape[3])\n",
        "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # shape specifying (batch, height, width, channel)\n",
        "        if self.remap is not None:\n",
        "            indices = indices.reshape(shape[0],-1) # add batch axis\n",
        "            indices = self.unmap_to_all(indices)\n",
        "            indices = indices.reshape(-1) # flatten again\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "class EmbeddingEMA(nn.Module):\n",
        "    def __init__(self, num_tokens, codebook_dim, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        weight = torch.randn(num_tokens, codebook_dim)\n",
        "        self.weight = nn.Parameter(weight, requires_grad = False)\n",
        "        self.cluster_size = nn.Parameter(torch.zeros(num_tokens), requires_grad = False)\n",
        "        self.embed_avg = nn.Parameter(weight.clone(), requires_grad = False)\n",
        "        self.update = True\n",
        "\n",
        "    def forward(self, embed_id):\n",
        "        return F.embedding(embed_id, self.weight)\n",
        "\n",
        "    def cluster_size_ema_update(self, new_cluster_size):\n",
        "        self.cluster_size.data.mul_(self.decay).add_(new_cluster_size, alpha=1 - self.decay)\n",
        "\n",
        "    def embed_avg_ema_update(self, new_embed_avg):\n",
        "        self.embed_avg.data.mul_(self.decay).add_(new_embed_avg, alpha=1 - self.decay)\n",
        "\n",
        "    def weight_update(self, num_tokens):\n",
        "        n = self.cluster_size.sum()\n",
        "        smoothed_cluster_size = (\n",
        "                (self.cluster_size + self.eps) / (n + num_tokens * self.eps) * n\n",
        "            )\n",
        "        #normalize embedding average with smoothed cluster size\n",
        "        embed_normalized = self.embed_avg / smoothed_cluster_size.unsqueeze(1)\n",
        "        self.weight.data.copy_(embed_normalized)\n",
        "\n",
        "\n",
        "class EMAVectorQuantizer(nn.Module):\n",
        "    def __init__(self, n_embed, embedding_dim, beta, decay=0.99, eps=1e-5,\n",
        "                remap=None, unknown_index=\"random\"):\n",
        "        super().__init__()\n",
        "        self.codebook_dim = codebook_dim\n",
        "        self.num_tokens = num_tokens\n",
        "        self.beta = beta\n",
        "        self.embedding = EmbeddingEMA(self.num_tokens, self.codebook_dim, decay, eps)\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_embed} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_embed\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        #z, 'b c h w -> b h w c'\n",
        "        z = rearrange(z, 'b c h w -> b h w c')\n",
        "        z_flattened = z.reshape(-1, self.codebook_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = z_flattened.pow(2).sum(dim=1, keepdim=True) + \\\n",
        "            self.embedding.weight.pow(2).sum(dim=1) - 2 * \\\n",
        "            torch.einsum('bd,nd->bn', z_flattened, self.embedding.weight) # 'n d -> d n'\n",
        "\n",
        "\n",
        "        encoding_indices = torch.argmin(d, dim=1)\n",
        "\n",
        "        z_q = self.embedding(encoding_indices).view(z.shape)\n",
        "        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        if self.training and self.embedding.update:\n",
        "            #EMA cluster size\n",
        "            encodings_sum = encodings.sum(0)\n",
        "            self.embedding.cluster_size_ema_update(encodings_sum)\n",
        "            #EMA embedding average\n",
        "            embed_sum = encodings.transpose(0,1) @ z_flattened\n",
        "            self.embedding.embed_avg_ema_update(embed_sum)\n",
        "            #normalize embed_avg and update weight\n",
        "            self.embedding.weight_update(self.num_tokens)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * F.mse_loss(z_q.detach(), z)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        #z_q, 'b h w c -> b c h w'\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w')\n",
        "        return z_q, loss, (perplexity, encodings, encoding_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7CSqxzfG_2rb"
      },
      "outputs": [],
      "source": [
        "# @title CompVis stable-diffusion autoencoder.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/models/autoencoder.py\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# from taming.modules.vqvae.quantize import VectorQuantizer2 as VectorQuantizer\n",
        "\n",
        "from ldm.modules.diffusionmodules.model import Encoder, Decoder\n",
        "from ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "class VQModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 n_embed,\n",
        "                 embed_dim,\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 image_key=\"image\",\n",
        "                 colorize_nlabels=None,\n",
        "                 monitor=None,\n",
        "                 batch_resize_range=None,\n",
        "                 scheduler_config=None,\n",
        "                 lr_g_factor=1.0,\n",
        "                 remap=None,\n",
        "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "                 use_ema=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.image_key = image_key\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n",
        "                                        remap=remap,\n",
        "                                        sane_index_shape=sane_index_shape)\n",
        "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        if colorize_nlabels is not None:\n",
        "            assert type(colorize_nlabels)==int\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        self.batch_resize_range = batch_resize_range\n",
        "        if self.batch_resize_range is not None:\n",
        "            print(f\"{self.__class__.__name__}: Using per-batch resizing in range {batch_resize_range}.\")\n",
        "\n",
        "        self.use_ema = use_ema\n",
        "        if self.use_ema:\n",
        "            self.model_ema = LitEma(self)\n",
        "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
        "\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
        "        self.scheduler_config = scheduler_config\n",
        "        self.lr_g_factor = lr_g_factor\n",
        "\n",
        "    @contextmanager\n",
        "    def ema_scope(self, context=None):\n",
        "        if self.use_ema:\n",
        "            self.model_ema.store(self.parameters())\n",
        "            self.model_ema.copy_to(self)\n",
        "            if context is not None:\n",
        "                print(f\"{context}: Switched to EMA weights\")\n",
        "        try:\n",
        "            yield None\n",
        "        finally:\n",
        "            if self.use_ema:\n",
        "                self.model_ema.restore(self.parameters())\n",
        "                if context is not None:\n",
        "                    print(f\"{context}: Restored training weights\")\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
        "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        missing, unexpected = self.load_state_dict(sd, strict=False)\n",
        "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
        "        if len(missing) > 0:\n",
        "            print(f\"Missing Keys: {missing}\")\n",
        "            print(f\"Unexpected Keys: {unexpected}\")\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        if self.use_ema:\n",
        "            self.model_ema(self)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        if self.batch_resize_range is not None:\n",
        "            lower_size = self.batch_resize_range[0]\n",
        "            upper_size = self.batch_resize_range[1]\n",
        "            if self.global_step <= 4:\n",
        "                # do the first few batches with max size to avoid later oom\n",
        "                new_resize = upper_size\n",
        "            else:\n",
        "                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n",
        "            if new_resize != x.shape[2]:\n",
        "                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n",
        "            x = x.detach()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        # https://github.com/pytorch/pytorch/issues/37142\n",
        "        # try not to fool the heuristics\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # autoencode\n",
        "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
        "                                            predicted_indices=ind)\n",
        "\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # discriminator\n",
        "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        log_dict = self._validation_step(batch, batch_idx)\n",
        "        with self.ema_scope():\n",
        "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
        "        return log_dict\n",
        "\n",
        "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
        "                                        self.global_step,\n",
        "                                        last_layer=self.get_last_layer(),\n",
        "                                        split=\"val\"+suffix,\n",
        "                                        predicted_indices=ind\n",
        "                                        )\n",
        "\n",
        "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
        "                                            self.global_step,\n",
        "                                            last_layer=self.get_last_layer(),\n",
        "                                            split=\"val\"+suffix,\n",
        "                                            predicted_indices=ind\n",
        "                                            )\n",
        "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
        "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr_d = self.learning_rate\n",
        "        lr_g = self.lr_g_factor*self.learning_rate\n",
        "        print(\"lr_d\", lr_d)\n",
        "        print(\"lr_g\", lr_g)\n",
        "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
        "                                  list(self.decoder.parameters())+\n",
        "                                  list(self.quantize.parameters())+\n",
        "                                  list(self.quant_conv.parameters())+\n",
        "                                  list(self.post_quant_conv.parameters()),\n",
        "                                  lr=lr_g, betas=(0.5, 0.9))\n",
        "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
        "                                    lr=lr_d, betas=(0.5, 0.9))\n",
        "\n",
        "        if self.scheduler_config is not None:\n",
        "            scheduler = instantiate_from_config(self.scheduler_config)\n",
        "\n",
        "            print(\"Setting up LambdaLR scheduler...\")\n",
        "            scheduler = [\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                },\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                },\n",
        "            ]\n",
        "            return [opt_ae, opt_disc], scheduler\n",
        "        return [opt_ae, opt_disc], []\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "    def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n",
        "        log = dict()\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        x = x.to(self.device)\n",
        "        if only_inputs:\n",
        "            log[\"inputs\"] = x\n",
        "            return log\n",
        "        xrec, _ = self(x)\n",
        "        if x.shape[1] > 3:\n",
        "            # colorize with random projection\n",
        "            assert xrec.shape[1] > 3\n",
        "            x = self.to_rgb(x)\n",
        "            xrec = self.to_rgb(xrec)\n",
        "        log[\"inputs\"] = x\n",
        "        log[\"reconstructions\"] = xrec\n",
        "        if plot_ema:\n",
        "            with self.ema_scope():\n",
        "                xrec_ema, _ = self(x)\n",
        "                if x.shape[1] > 3: xrec_ema = self.to_rgb(xrec_ema)\n",
        "                log[\"reconstructions_ema\"] = xrec_ema\n",
        "        return log\n",
        "\n",
        "    def to_rgb(self, x):\n",
        "        assert self.image_key == \"segmentation\"\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
        "        x = F.conv2d(x, weight=self.colorize)\n",
        "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
        "        return x\n",
        "\n",
        "\n",
        "class VQModelInterface(VQModel):\n",
        "    def __init__(self, embed_dim, *args, **kwargs):\n",
        "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, h, force_not_quantize=False):\n",
        "        # also go through quantization layer\n",
        "        if not force_not_quantize:\n",
        "            quant, emb_loss, info = self.quantize(h)\n",
        "        else:\n",
        "            quant = h\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "\n",
        "class AutoencoderKL(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 embed_dim,\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 image_key=\"image\",\n",
        "                 colorize_nlabels=None,\n",
        "                 monitor=None,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.image_key = image_key\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        assert ddconfig[\"double_z\"]\n",
        "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        self.embed_dim = embed_dim\n",
        "        if colorize_nlabels is not None:\n",
        "            assert type(colorize_nlabels)==int\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
        "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        self.load_state_dict(sd, strict=False)\n",
        "        print(f\"Restored from {path}\")\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, sample_posterior=True):\n",
        "        posterior = self.encode(input)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        return dec, posterior\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # train encoder+decoder+logvar\n",
        "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # train the discriminator\n",
        "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "\n",
        "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "\n",
        "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.learning_rate\n",
        "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
        "                                  list(self.decoder.parameters())+\n",
        "                                  list(self.quant_conv.parameters())+\n",
        "                                  list(self.post_quant_conv.parameters()),\n",
        "                                  lr=lr, betas=(0.5, 0.9))\n",
        "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "        return [opt_ae, opt_disc], []\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_images(self, batch, only_inputs=False, **kwargs):\n",
        "        log = dict()\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        x = x.to(self.device)\n",
        "        if not only_inputs:\n",
        "            xrec, posterior = self(x)\n",
        "            if x.shape[1] > 3:\n",
        "                # colorize with random projection\n",
        "                assert xrec.shape[1] > 3\n",
        "                x = self.to_rgb(x)\n",
        "                xrec = self.to_rgb(xrec)\n",
        "            log[\"samples\"] = self.decode(torch.randn_like(posterior.sample()))\n",
        "            log[\"reconstructions\"] = xrec\n",
        "        log[\"inputs\"] = x\n",
        "        return log\n",
        "\n",
        "    def to_rgb(self, x):\n",
        "        assert self.image_key == \"segmentation\"\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
        "        x = F.conv2d(x, weight=self.colorize)\n",
        "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XOuGUdMZaxB6"
      },
      "outputs": [],
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ConvLayer\n",
        "# nn.Sequential(\n",
        "#     nn.Dropout2d(dropout), nn.Conv2d(in_ch, out_ch, 3, 1, 3//2, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU()\n",
        "# )\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.repeats = out_ch//in_ch\n",
        "        if out_ch//in_ch > 1:\n",
        "            self.func = lambda x: x.repeat_interleave(out_ch//in_ch, dim=1) # [b,i,h,w] -> [b,o,h,w]\n",
        "        elif in_ch//out_ch > 1:\n",
        "            self.func = lambda x: torch.unflatten(x, 1, (out_ch, in_ch//out_ch)).mean(dim=2) # [b,i,h,w] -> [b,o,i/o,h,w] -> [b,o,h,w]\n",
        "        else: print('err SameCh', in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        return self.func(x)\n",
        "\n",
        "class PixelShortcut(nn.Module): # up shortcut [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(SameCh(in_ch, out_ch*r**2), nn.PixelShuffle(r)) #\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SameCh(in_ch*r**2, out_ch)) #\n",
        "        else: self.net = SameCh(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): #\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module): # down main [b,i,2h,2w] -> [b,o,h,w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch//r**2, kernel_size, 1, kernel_size//2)\n",
        "\n",
        "    def forward(self, x): # [b,i,2h,2w] -> [b,o/4,2h,2w] -> [b,o,h,w]\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module): # up main [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch*r**2, kernel_size, 1, kernel_size//2)\n",
        "        # self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, 1, kernel_size//2) # InterpolateConvUpSampleLayer\n",
        "\n",
        "    def forward(self, x): # [b,i,h,w] -> [b,4o,h,w] -> [b,o,2h,2w]\n",
        "        # x = torch.nn.functional.interpolate(x, scale_r=self.r, mode=\"nearest\")\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "        # if self.r>1: self.net = nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), out_r=r), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), in_r=r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        else: self.net = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2)\n",
        "        # self.net.apply(self.init_conv_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# # https://arxiv.org/pdf/1707.02937\n",
        "# nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), 'out'), nn.PixelShuffle(r))\n",
        "# nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), 'in'))\n",
        "\n",
        "# https://github.com/fastai/fastai/blob/main/fastai/layers.py#L368\n",
        "def icnr_init(x, scale=2, init=nn.init.kaiming_normal_):\n",
        "    \"ICNR init of `x`, with `scale` and `init` function\"\n",
        "    ni,nf,h,w = x.shape\n",
        "    ni2 = int(ni/(scale**2))\n",
        "    k = init(x.new_zeros([ni2,nf,h,w])).transpose(0, 1)\n",
        "    k = k.contiguous().view(ni2, nf, -1)\n",
        "    k = k.repeat(1, 1, scale**2)\n",
        "    return k.contiguous().view([nf,ni,h,w]).transpose(0, 1)\n",
        "\n",
        "class PixelShuffle_ICNR(nn.Sequential):\n",
        "    \"Upsample by `scale` from `ni` filters to `nf` (default `ni`), using `nn.PixelShuffle`.\"\n",
        "    def __init__(self, ni, nf=None, scale=2, blur=False, norm_type=NormType.Weight, act_cls=defaults.activation):\n",
        "        super().__init__()\n",
        "        nf = ifnone(nf, ni)\n",
        "        layers = [ConvLayer(ni, nf*(scale**2), ks=1, norm_type=norm_type, act_cls=act_cls, bias_std=0),\n",
        "                  nn.PixelShuffle(scale)]\n",
        "        if norm_type == NormType.Weight:\n",
        "            layers[0][0].weight_v.data.copy_(icnr_init(layers[0][0].weight_v.data))\n",
        "            layers[0][0].weight_g.data.copy_(((layers[0][0].weight_v.data**2).sum(dim=[1,2,3])**0.5)[:,None,None,None])\n",
        "        else:\n",
        "            layers[0][0].weight.data.copy_(icnr_init(layers[0][0].weight.data))\n",
        "        if blur: layers += [nn.ReplicationPad2d((1,0,1,0)), nn.AvgPool2d(2, stride=1)]\n",
        "        super().__init__(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# block = EfficientViTBlock(in_ch, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU\n",
        "# self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio,\n",
        "#     use_bias=(True, True, False), norm=(None, None, norm), act_func=(act_func, act_func, None))\n",
        "class GLUMBConv(nn.Module):\n",
        "    # def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4, use_bias=False, norm=(None, None, \"ln2d\"), act_func=(\"silu\", \"silu\", None)):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4):\n",
        "        super().__init__()\n",
        "        mid_channels = round(in_ch * expand_ratio) if mid_channels is None else mid_channels\n",
        "        # self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        # self.inverted_conv = ConvLayer(in_ch, mid_channels * 2, 1, use_bias=use_bias[0], norm=norm[0], act_func=act_func[0],)\n",
        "        # self.depth_conv = ConvLayer(mid_channels * 2, mid_channels * 2, kernel_size, stride=stride, groups=mid_channels * 2, use_bias=use_bias[1], norm=norm[1], act_func=None,)\n",
        "        self.inverted_depth_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_channels*2, 1, 1, 0), nn.SiLU(),\n",
        "            nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2),\n",
        "        )\n",
        "        # self.point_conv = ConvLayer(mid_channels, out_ch, 1, use_bias=use_bias[2], norm=norm[2], act_func=act_func[2],)\n",
        "        self.point_conv = nn.Sequential(\n",
        "            nn.Conv2d(mid_channels, out_ch, 1, 1, 0, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.inverted_conv(x)\n",
        "        # x = self.depth_conv(x)\n",
        "        x = self.inverted_depth_conv(x)\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        x = x * nn.SiLU()(gate)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# main_block = ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel_size=3, stride=1, d_model=None,\n",
        "        use_bias=False, norm=(\"bn2d\", \"bn2d\"), act_func=(\"relu6\", None)):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_ch\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, d_model, kernel_size, stride, kernel_size//2), nn.SiLU(),\n",
        "            nn.Conv2d(d_model, out_ch, kernel_size, 1, kernel_size//2, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(self, in_ch, heads_ratio = 1.0, dim=32, expand_ratio=1, # expand_ratio=4\n",
        "        # scales: tuple[int, ...] = (5,), # (5,): sana\n",
        "        # act_func = \"hswish\", # nn.Hardswish()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # self.context_module = LiteMLA(in_ch, in_ch, heads_ratio=heads_ratio, dim=dim, norm=(None, norm), scales=scales,)\n",
        "        self.context_module = AttentionBlock(in_ch, d_head=8)\n",
        "        # self.local_module = MBConv(\n",
        "        self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.context_module(x)\n",
        "        x = x + self.local_module(x)\n",
        "        return x\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "    # def forward(self, x):\n",
        "    #     res = self.forward_main(self.pre_norm(x)) + self.shortcut(x)\n",
        "    #     res = self.post_act(res)\n",
        "    #     return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "w_FyFDkua0lA"
      },
      "outputs": [],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_block(block_type, d_model, norm=None, act=None):\n",
        "    if block_type == \"ResBlock\": return ResBlock(d_model) # ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "    # ResBlock: bn2d, relu ; EViT_GLU: trms2d, silu\n",
        "    elif block_type == \"EViT_GLU\": return EfficientViTBlock(d_model) # EfficientViTBlock(d_model, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU:scales=() ; EViTS5_GLU sana:scales=(5,)\n",
        "\n",
        "class LevelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, depth, block_type, norm=None, act=None, updown=None):\n",
        "        super().__init__()\n",
        "        stage = []\n",
        "        if updown=='up': stage.append(UpsampleBlock(in_ch, out_ch))\n",
        "        for d in range(depth):\n",
        "            # block = build_block(block_type=block_type, in_ch=d_model if d > 0 else in_ch, out_ch=d_model, norm=norm, act=act,)\n",
        "            block = build_block(block_type, out_ch if updown=='up' else in_ch, norm=norm, act=act,)\n",
        "            stage.append(block)\n",
        "        if updown=='down': stage.append(DownsampleBlock(in_ch, out_ch))\n",
        "        self.block = nn.Sequential(*stage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# stage = build_stage_main(width, depth, block_type)\n",
        "# downsample_block = DownsampleBlock(width, width_list[stage_id + 1])\n",
        "\n",
        "# upsample_block = UpsampleBlock(width_list[stage_id + 1], width)\n",
        "# stage.extend(build_stage_main(width, depth, block_type, \"bn2d\", \"silu\", input_width=width))\n",
        "\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = nn.Conv2d(in_ch, out_ch, 3, 2, 3//2)\n",
        "        # self.block = ConvPixelUnshuffleDownSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=3, r=1/2)\n",
        "        # self.shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(in_ch, out_ch, r=2)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=1/2)\n",
        "    def forward(self, x):\n",
        "        # print(\"DownsampleBlock fwd\", x.shape, self.block(x).shape + self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,4,8,2,2,2]\n",
        "\n",
        "        # # self.project_in = nn.Conv2d(in_ch, width_list[0], 3, 1, 3//2) # if depth_list[0] > 0:\n",
        "        self.project_in = DownsampleBlock(in_ch, width_list[0]) # shortcut=None # self.project_in = ConvPixelUnshuffleDownSampleLayer(in_ch, width_list[0], kernel_size=3, r=2)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[0], width_list[-1], depth=depth_list[0], block_type='ResBlock', updown='down'),\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "        )\n",
        "\n",
        "        self.out_block = nn.Conv2d(width_list[-1], out_ch, 3, 1, 3//2)\n",
        "        # self.out_shortcut = PixelUnshuffleChannelAveragingDownSampleLayer(width_list[-1], out_ch, r=1)\n",
        "        self.out_shortcut = PixelShortcut(width_list[-1], out_ch, r=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x = self.stages(x)\n",
        "        # print(\"Encoder fwd\", x.shape, self.out_block, self.out_shortcut(x).shape)\n",
        "        x = self.out_block(x) + self.out_shortcut(x)\n",
        "        return x\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = ConvPixelShuffleUpSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=3, r=2)\n",
        "        # self.block = InterpolateConvUpSampleLayer(in_ch=in_ch, out_ch=out_ch, kernel_size=3, r=2)\n",
        "        # self.shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, out_ch, r=2)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=2)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,o,2h,2w]\n",
        "        # print(\"UpsampleBlock fwd\", x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.in_block = nn.Conv2d(in_ch, width_list[-1], 3, 1, 3//2)\n",
        "        # self.in_shortcut = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, width_list[-1], r=1)\n",
        "        self.in_shortcut = PixelShortcut(in_ch, width_list[-1], r=1)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "            LevelBlock(width_list[-1], width_list[0], depth=depth_list[0], block_type='ResBlock', updown='up'),\n",
        "        )\n",
        "\n",
        "        # if depth_list[0] > 0:\n",
        "        # self.project_out = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(width_list[0]), nn.ReLU(), nn.Conv2d(width_list[0], out_ch, 3, 1, 3//2) # norm=\"trms2d\"\n",
        "        #     )\n",
        "        # else:\n",
        "        self.project_out = nn.Sequential(\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpsampleBlock(width_list[0], out_ch) # shortcut=None ; norm=\"trms2d\"\n",
        "            # nn.BatchNorm2d(width_list[0]), nn.ReLU(), ConvPixelShuffleUpSampleLayer(width_list[0], out_ch, kernel_size=3, r=2) # shortcut=None ; norm=\"trms2d\"\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.in_block(x) + self.in_shortcut(x)\n",
        "        x = self.stages(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(in_ch, out_ch, d_model, mult, depth_list)\n",
        "        self.decoder = Decoder(out_ch, in_ch, d_model, mult, depth_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# https://discuss.pytorch.org/t/is-there-a-layer-normalization-for-conv2d/7595/5\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "class LayerNorm2d(nn.LayerNorm):\n",
        "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
        "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=24, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# model = Encoder(in_ch, out_ch, d_model=32, mult=[1,1], depth_list=[2,2])\n",
        "# print(sum(p.numel() for p in model.project_in.parameters() if p.requires_grad)) # 896\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "# print(sum(p.numel() for p in model.out_shortcut.parameters() if p.requires_grad)) # 0\n",
        "# print(sum(p.numel() for p in model.out_block.parameters() if p.requires_grad)) # 18436\n",
        "# model = Decoder(out_ch, in_ch)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rkrEVsXxSs_C"
      },
      "outputs": [],
      "source": [
        "# @title efficientvit nn/ops.py\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from efficientvit.models.nn.act import build_act\n",
        "from efficientvit.models.nn.norm import build_norm\n",
        "from efficientvit.models.utils import get_same_padding, list_sum, resize, val2list, val2tuple\n",
        "\n",
        "__all__ = [\n",
        "    \"ConvLayer\",\n",
        "    \"UpSampleLayer\",\n",
        "    \"ConvPixelUnshuffleDownSampleLayer\",\n",
        "    \"PixelUnshuffleChannelAveragingDownSampleLayer\",\n",
        "    \"ConvPixelShuffleUpSampleLayer\",\n",
        "    \"ChannelDuplicatingPixelUnshuffleUpSampleLayer\",\n",
        "    \"LinearLayer\",\n",
        "    \"IdentityLayer\",\n",
        "    \"DSConv\",\n",
        "    \"MBConv\",\n",
        "    \"FusedMBConv\",\n",
        "    \"ResBlock\",\n",
        "    \"LiteMLA\",\n",
        "    \"EfficientViTBlock\",\n",
        "    \"ResidualBlock\",\n",
        "    \"DAGBlock\",\n",
        "    \"OpSequential\",\n",
        "]\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Basic Layers                                      #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        dilation=1,\n",
        "        groups=1,\n",
        "        use_bias=False,\n",
        "        dropout=0,\n",
        "        norm=\"bn2d\",\n",
        "        act_func=\"relu\",\n",
        "    ):\n",
        "        super(ConvLayer, self).__init__()\n",
        "\n",
        "        padding = get_same_padding(kernel_size)\n",
        "        padding *= dilation\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropout, inplace=False) if dropout > 0 else None\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=(kernel_size, kernel_size),\n",
        "            stride=(stride, stride),\n",
        "            padding=padding,\n",
        "            dilation=(dilation, dilation),\n",
        "            groups=groups,\n",
        "            bias=use_bias,\n",
        "        )\n",
        "        self.norm = build_norm(norm, num_features=out_channels)\n",
        "        self.act = build_act(act_func)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = self.conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.act:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mode=\"bicubic\",\n",
        "        size: Optional[int | tuple[int, int] | list[int]] = None,\n",
        "        factor=2,\n",
        "        align_corners=False,\n",
        "    ):\n",
        "        super(UpSampleLayer, self).__init__()\n",
        "        self.mode = mode\n",
        "        self.size = val2list(size, 2) if size is not None else None\n",
        "        self.factor = None if self.size is not None else factor\n",
        "        self.align_corners = align_corners\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if (self.size is not None and tuple(x.shape[-2:]) == self.size) or self.factor == 1:\n",
        "            return x\n",
        "        if x.dtype in [torch.float16, torch.bfloat16]:\n",
        "            x = x.float()\n",
        "        return resize(x, self.size, self.factor, self.mode, self.align_corners)\n",
        "\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        out_ratio = factor**2\n",
        "        assert out_channels % out_ratio == 0\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels // out_ratio,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.factor = factor\n",
        "        assert in_channels * factor**2 % out_channels == 0\n",
        "        self.group_size = in_channels * factor**2 // out_channels\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.pixel_unshuffle(x, self.factor)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, self.out_channels, self.group_size, H, W)\n",
        "        x = x.mean(dim=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        out_ratio = factor**2\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels * out_ratio,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InterpolateConvUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "        mode: str = \"nearest\",\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        self.mode = mode\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=self.factor, mode=self.mode)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ChannelDuplicatingPixelUnshuffleUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.factor = factor\n",
        "        assert out_channels * factor**2 % in_channels == 0\n",
        "        self.repeats = out_channels * factor**2 // in_channels\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.repeat_interleave(self.repeats, dim=1)\n",
        "        x = F.pixel_shuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LinearLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        use_bias=True,\n",
        "        dropout=0,\n",
        "        norm=None,\n",
        "        act_func=None,\n",
        "    ):\n",
        "        super(LinearLayer, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout, inplace=False) if dropout > 0 else None\n",
        "        self.linear = nn.Linear(in_features, out_features, use_bias)\n",
        "        self.norm = build_norm(norm, num_features=out_features)\n",
        "        self.act = build_act(act_func)\n",
        "\n",
        "    def _try_squeeze(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() > 2:\n",
        "            x = torch.flatten(x, start_dim=1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self._try_squeeze(x)\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.act:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Basic Blocks                                      #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class DSConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super(DSConv, self).__init__()\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        self.depth_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            in_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            groups=in_channels,\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "            use_bias=use_bias[0],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "            use_bias=use_bias[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.depth_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", \"relu6\", None),\n",
        "    ):\n",
        "        super(MBConv, self).__init__()\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 3)\n",
        "        norm = val2tuple(norm, 3)\n",
        "        act_func = val2tuple(act_func, 3)\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.inverted_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            1,\n",
        "            stride=1,\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "            use_bias=use_bias[0],\n",
        "        )\n",
        "        self.depth_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            groups=mid_channels,\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "            use_bias=use_bias[1],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            norm=norm[2],\n",
        "            act_func=act_func[2],\n",
        "            use_bias=use_bias[2],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.inverted_conv(x)\n",
        "        x = self.depth_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FusedMBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        groups=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.spatial_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            groups=groups,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.spatial_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GLUMBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        use_bias=False,\n",
        "        norm=(None, None, \"ln2d\"),\n",
        "        act_func=(\"silu\", \"silu\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 3)\n",
        "        norm = val2tuple(norm, 3)\n",
        "        act_func = val2tuple(act_func, 3)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        self.inverted_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels * 2,\n",
        "            1,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.depth_conv = ConvLayer(\n",
        "            mid_channels * 2,\n",
        "            mid_channels * 2,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            groups=mid_channels * 2,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=None,\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[2],\n",
        "            norm=norm[2],\n",
        "            act_func=act_func[2],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.inverted_conv(x)\n",
        "        x = self.depth_conv(x)\n",
        "\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        gate = self.glu_act(gate)\n",
        "        x = x * gate\n",
        "\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.conv1 = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.conv2 = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LiteMLA(nn.Module):\n",
        "    r\"\"\"Lightweight multi-scale linear attention\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        heads: Optional[int] = None,\n",
        "        heads_ratio: float = 1.0,\n",
        "        dim=8,\n",
        "        use_bias=False,\n",
        "        norm=(None, \"bn2d\"),\n",
        "        act_func=(None, None),\n",
        "        kernel_func=\"relu\",\n",
        "        scales: tuple[int, ...] = (5,),\n",
        "        eps=1.0e-15,\n",
        "    ):\n",
        "        super(LiteMLA, self).__init__()\n",
        "        self.eps = eps\n",
        "        heads = int(in_channels // dim * heads_ratio) if heads is None else heads\n",
        "\n",
        "        total_dim = heads * dim\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.qkv = ConvLayer(\n",
        "            in_channels,\n",
        "            3 * total_dim,\n",
        "            1,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.aggreg = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        3 * total_dim,\n",
        "                        3 * total_dim,\n",
        "                        scale,\n",
        "                        padding=get_same_padding(scale),\n",
        "                        groups=3 * total_dim,\n",
        "                        bias=use_bias[0],\n",
        "                    ),\n",
        "                    nn.Conv2d(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0]),\n",
        "                )\n",
        "                for scale in scales\n",
        "            ]\n",
        "        )\n",
        "        self.kernel_func = build_act(kernel_func, inplace=False)\n",
        "\n",
        "        self.proj = ConvLayer(\n",
        "            total_dim * (1 + len(scales)),\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def relu_linear_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
        "        B, _, H, W = list(qkv.size())\n",
        "\n",
        "        if qkv.dtype == torch.float16:\n",
        "            qkv = qkv.float()\n",
        "\n",
        "        qkv = torch.reshape(\n",
        "            qkv,\n",
        "            (\n",
        "                B,\n",
        "                -1,\n",
        "                3 * self.dim,\n",
        "                H * W,\n",
        "            ),\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[:, :, 0 : self.dim],\n",
        "            qkv[:, :, self.dim : 2 * self.dim],\n",
        "            qkv[:, :, 2 * self.dim :],\n",
        "        )\n",
        "\n",
        "        # lightweight linear attention\n",
        "        q = self.kernel_func(q)\n",
        "        k = self.kernel_func(k)\n",
        "\n",
        "        # linear matmul\n",
        "        trans_k = k.transpose(-1, -2)\n",
        "\n",
        "        v = F.pad(v, (0, 0, 0, 1), mode=\"constant\", value=1)\n",
        "        vk = torch.matmul(v, trans_k)\n",
        "        out = torch.matmul(vk, q)\n",
        "        if out.dtype == torch.bfloat16:\n",
        "            out = out.float()\n",
        "        out = out[:, :, :-1] / (out[:, :, -1:] + self.eps)\n",
        "\n",
        "        out = torch.reshape(out, (B, -1, H, W))\n",
        "        return out\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def relu_quadratic_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
        "        B, _, H, W = list(qkv.size())\n",
        "\n",
        "        qkv = torch.reshape(\n",
        "            qkv,\n",
        "            (\n",
        "                B,\n",
        "                -1,\n",
        "                3 * self.dim,\n",
        "                H * W,\n",
        "            ),\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[:, :, 0 : self.dim],\n",
        "            qkv[:, :, self.dim : 2 * self.dim],\n",
        "            qkv[:, :, 2 * self.dim :],\n",
        "        )\n",
        "\n",
        "        q = self.kernel_func(q)\n",
        "        k = self.kernel_func(k)\n",
        "\n",
        "        att_map = torch.matmul(k.transpose(-1, -2), q)  # b h n n\n",
        "        original_dtype = att_map.dtype\n",
        "        if original_dtype in [torch.float16, torch.bfloat16]:\n",
        "            att_map = att_map.float()\n",
        "        att_map = att_map / (torch.sum(att_map, dim=2, keepdim=True) + self.eps)  # b h n n\n",
        "        att_map = att_map.to(original_dtype)\n",
        "        out = torch.matmul(v, att_map)  # b h d n\n",
        "\n",
        "        out = torch.reshape(out, (B, -1, H, W))\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # generate multi-scale q, k, v\n",
        "        qkv = self.qkv(x)\n",
        "        multi_scale_qkv = [qkv]\n",
        "        for op in self.aggreg:\n",
        "            multi_scale_qkv.append(op(qkv))\n",
        "        qkv = torch.cat(multi_scale_qkv, dim=1)\n",
        "\n",
        "        H, W = list(qkv.size())[-2:]\n",
        "        if H * W > self.dim:\n",
        "            out = self.relu_linear_att(qkv).to(qkv.dtype)\n",
        "        else:\n",
        "            out = self.relu_quadratic_att(qkv)\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        heads_ratio: float = 1.0,\n",
        "        dim=32,\n",
        "        expand_ratio: float = 4,\n",
        "        scales: tuple[int, ...] = (5,),\n",
        "        norm: str = \"bn2d\",\n",
        "        act_func: str = \"hswish\",\n",
        "        context_module: str = \"LiteMLA\",\n",
        "        local_module: str = \"MBConv\",\n",
        "    ):\n",
        "        super(EfficientViTBlock, self).__init__()\n",
        "        if context_module == \"LiteMLA\":\n",
        "            self.context_module = ResidualBlock(\n",
        "                LiteMLA(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    heads_ratio=heads_ratio,\n",
        "                    dim=dim,\n",
        "                    norm=(None, norm),\n",
        "                    scales=scales,\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"context_module {context_module} is not supported\")\n",
        "        if local_module == \"MBConv\":\n",
        "            self.local_module = ResidualBlock(\n",
        "                MBConv(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    expand_ratio=expand_ratio,\n",
        "                    use_bias=(True, True, False),\n",
        "                    norm=(None, None, norm),\n",
        "                    act_func=(act_func, act_func, None),\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        elif local_module == \"GLUMBConv\":\n",
        "            self.local_module = ResidualBlock(\n",
        "                GLUMBConv(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    expand_ratio=expand_ratio,\n",
        "                    use_bias=(True, True, False),\n",
        "                    norm=(None, None, norm),\n",
        "                    act_func=(act_func, act_func, None),\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(f\"local_module {local_module} is not supported\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.context_module(x)\n",
        "        x = self.local_module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Functional Blocks                                 #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        main: Optional[nn.Module],\n",
        "        shortcut: Optional[nn.Module],\n",
        "        post_act=None,\n",
        "        pre_norm: Optional[nn.Module] = None,\n",
        "    ):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        self.main = main\n",
        "        self.shortcut = shortcut\n",
        "        self.post_act = build_act(post_act)\n",
        "\n",
        "    def forward_main(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.pre_norm is None:\n",
        "            return self.main(x)\n",
        "        else:\n",
        "            return self.main(self.pre_norm(x))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.main is None:\n",
        "            res = x\n",
        "        elif self.shortcut is None:\n",
        "            res = self.forward_main(x)\n",
        "        else:\n",
        "            res = self.forward_main(x) + self.shortcut(x)\n",
        "            if self.post_act:\n",
        "                res = self.post_act(res)\n",
        "        return res\n",
        "\n",
        "\n",
        "class DAGBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inputs: dict[str, nn.Module],\n",
        "        merge: str,\n",
        "        post_input: Optional[nn.Module],\n",
        "        middle: nn.Module,\n",
        "        outputs: dict[str, nn.Module],\n",
        "    ):\n",
        "        super(DAGBlock, self).__init__()\n",
        "\n",
        "        self.input_keys = list(inputs.keys())\n",
        "        self.input_ops = nn.ModuleList(list(inputs.values()))\n",
        "        self.merge = merge\n",
        "        self.post_input = post_input\n",
        "\n",
        "        self.middle = middle\n",
        "\n",
        "        self.output_keys = list(outputs.keys())\n",
        "        self.output_ops = nn.ModuleList(list(outputs.values()))\n",
        "\n",
        "    def forward(self, feature_dict: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
        "        feat = [op(feature_dict[key]) for key, op in zip(self.input_keys, self.input_ops)]\n",
        "        if self.merge == \"add\":\n",
        "            feat = list_sum(feat)\n",
        "        elif self.merge == \"cat\":\n",
        "            feat = torch.concat(feat, dim=1)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        if self.post_input is not None:\n",
        "            feat = self.post_input(feat)\n",
        "        feat = self.middle(feat)\n",
        "        for key, op in zip(self.output_keys, self.output_ops):\n",
        "            feature_dict[key] = op(feat)\n",
        "        return feature_dict\n",
        "\n",
        "\n",
        "class OpSequential(nn.Module):\n",
        "    def __init__(self, op_list: list[Optional[nn.Module]]):\n",
        "        super(OpSequential, self).__init__()\n",
        "        valid_op_list = []\n",
        "        for op in op_list:\n",
        "            if op is not None:\n",
        "                valid_op_list.append(op)\n",
        "        self.op_list = nn.ModuleList(valid_op_list)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for op in self.op_list:\n",
        "            x = op(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EfX78Q2-Sqaz"
      },
      "outputs": [],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from omegaconf import MISSING, OmegaConf\n",
        "\n",
        "from efficientvit.models.nn.act import build_act\n",
        "from efficientvit.models.nn.norm import build_norm\n",
        "from efficientvit.models.nn.ops import (\n",
        "    ChannelDuplicatingPixelUnshuffleUpSampleLayer,\n",
        "    ConvLayer,\n",
        "    ConvPixelShuffleUpSampleLayer,\n",
        "    ConvPixelUnshuffleDownSampleLayer,\n",
        "    EfficientViTBlock,\n",
        "    IdentityLayer,\n",
        "    InterpolateConvUpSampleLayer,\n",
        "    OpSequential,\n",
        "    PixelUnshuffleChannelAveragingDownSampleLayer,\n",
        "    ResBlock,\n",
        "    ResidualBlock,\n",
        ")\n",
        "\n",
        "__all__ = [\"DCAE\", \"dc_ae_f32c32\", \"dc_ae_f64c128\", \"dc_ae_f128c512\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EncoderConfig:\n",
        "    in_channels: int = MISSING\n",
        "    latent_channels: int = MISSING\n",
        "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
        "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
        "    block_type: Any = \"ResBlock\"\n",
        "    norm: str = \"trms2d\"\n",
        "    act: str = \"silu\"\n",
        "    downsample_block_type: str = \"ConvPixelUnshuffle\"\n",
        "    downsample_match_channel: bool = True\n",
        "    downsample_shortcut: Optional[str] = \"averaging\"\n",
        "    out_norm: Optional[str] = None\n",
        "    out_act: Optional[str] = None\n",
        "    out_shortcut: Optional[str] = \"averaging\"\n",
        "    double_latent: bool = False\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DecoderConfig:\n",
        "    in_channels: int = MISSING\n",
        "    latent_channels: int = MISSING\n",
        "    in_shortcut: Optional[str] = \"duplicating\"\n",
        "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
        "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
        "    block_type: Any = \"ResBlock\"\n",
        "    norm: Any = \"trms2d\"\n",
        "    act: Any = \"silu\"\n",
        "    upsample_block_type: str = \"ConvPixelShuffle\"\n",
        "    upsample_match_channel: bool = True\n",
        "    upsample_shortcut: str = \"duplicating\"\n",
        "    out_norm: str = \"trms2d\"\n",
        "    out_act: str = \"relu\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DCAEConfig:\n",
        "    in_channels: int = 3\n",
        "    latent_channels: int = 32\n",
        "    encoder: EncoderConfig = field(\n",
        "        default_factory=lambda: EncoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
        "    )\n",
        "    decoder: DecoderConfig = field(\n",
        "        default_factory=lambda: DecoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
        "    )\n",
        "    use_quant_conv: bool = False\n",
        "\n",
        "    pretrained_path: Optional[str] = None\n",
        "    pretrained_source: str = \"dc-ae\"\n",
        "\n",
        "    scaling_factor: Optional[float] = None\n",
        "\n",
        "\n",
        "def build_block(\n",
        "    block_type: str, in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str]\n",
        ") -> nn.Module:\n",
        "    if block_type == \"ResBlock\":\n",
        "        assert in_channels == out_channels\n",
        "        main_block = ResBlock(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            use_bias=(True, False),\n",
        "            norm=(None, norm),\n",
        "            act_func=(act, None),\n",
        "        )\n",
        "        block = ResidualBlock(main_block, IdentityLayer())\n",
        "    elif block_type == \"EViT_GLU\":\n",
        "        assert in_channels == out_channels\n",
        "        block = EfficientViTBlock(in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=())\n",
        "    elif block_type == \"EViTS5_GLU\":\n",
        "        assert in_channels == out_channels\n",
        "        block = EfficientViTBlock(in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(5,))\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_stage_main(\n",
        "    width: int, depth: int, block_type: str | list[str], norm: str, act: str, input_width: int\n",
        ") -> list[nn.Module]:\n",
        "    assert isinstance(block_type, str) or (isinstance(block_type, list) and depth == len(block_type))\n",
        "    stage = []\n",
        "    for d in range(depth):\n",
        "        current_block_type = block_type[d] if isinstance(block_type, list) else block_type\n",
        "        block = build_block(\n",
        "            block_type=current_block_type,\n",
        "            in_channels=width if d > 0 else input_width,\n",
        "            out_channels=width,\n",
        "            norm=norm,\n",
        "            act=act,\n",
        "        )\n",
        "        stage.append(block)\n",
        "    return stage\n",
        "\n",
        "\n",
        "def build_downsample_block(block_type: str, in_channels: int, out_channels: int, shortcut: Optional[str]) -> nn.Module:\n",
        "    if block_type == \"Conv\":\n",
        "        block = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "    elif block_type == \"ConvPixelUnshuffle\":\n",
        "        block = ConvPixelUnshuffleDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported for downsampling\")\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"averaging\":\n",
        "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=2\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for downsample\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_upsample_block(block_type: str, in_channels: int, out_channels: int, shortcut: Optional[str]) -> nn.Module:\n",
        "    if block_type == \"ConvPixelShuffle\":\n",
        "        block = ConvPixelShuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    elif block_type == \"InterpolateConv\":\n",
        "        block = InterpolateConvUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported for upsampling\")\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"duplicating\":\n",
        "        shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=2\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for upsample\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_encoder_project_in_block(in_channels: int, out_channels: int, factor: int, downsample_block_type: str):\n",
        "    if factor == 1:\n",
        "        block = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "    elif factor == 2:\n",
        "        block = build_downsample_block(\n",
        "            block_type=downsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"downsample factor {factor} is not supported for encoder project in\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_encoder_project_out_block(\n",
        "    in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str], shortcut: Optional[str]\n",
        "):\n",
        "    block = OpSequential(\n",
        "        [\n",
        "            build_norm(norm),\n",
        "            build_act(act),\n",
        "            ConvLayer(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                use_bias=True,\n",
        "                norm=None,\n",
        "                act_func=None,\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"averaging\":\n",
        "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for encoder project out\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_decoder_project_in_block(in_channels: int, out_channels: int, shortcut: Optional[str]):\n",
        "    block = ConvLayer(\n",
        "        in_channels=in_channels,\n",
        "        out_channels=out_channels,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        use_bias=True,\n",
        "        norm=None,\n",
        "        act_func=None,\n",
        "    )\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"duplicating\":\n",
        "        shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for decoder project in\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_decoder_project_out_block(\n",
        "    in_channels: int, out_channels: int, factor: int, upsample_block_type: str, norm: Optional[str], act: Optional[str]\n",
        "):\n",
        "    layers: list[nn.Module] = [\n",
        "        build_norm(norm, in_channels),\n",
        "        build_act(act),\n",
        "    ]\n",
        "    if factor == 1:\n",
        "        layers.append(\n",
        "            ConvLayer(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                use_bias=True,\n",
        "                norm=None,\n",
        "                act_func=None,\n",
        "            )\n",
        "        )\n",
        "    elif factor == 2:\n",
        "        layers.append(\n",
        "            build_upsample_block(\n",
        "                block_type=upsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"upsample factor {factor} is not supported for decoder project out\")\n",
        "    return OpSequential(layers)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, cfg: EncoderConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        num_stages = len(cfg.width_list)\n",
        "        self.num_stages = num_stages\n",
        "        assert len(cfg.depth_list) == num_stages\n",
        "        assert len(cfg.width_list) == num_stages\n",
        "        assert isinstance(cfg.block_type, str) or (\n",
        "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
        "        )\n",
        "\n",
        "        self.project_in = build_encoder_project_in_block(\n",
        "            in_channels=cfg.in_channels,\n",
        "            out_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
        "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
        "            downsample_block_type=cfg.downsample_block_type,\n",
        "        )\n",
        "\n",
        "        self.stages: list[OpSequential] = []\n",
        "        for stage_id, (width, depth) in enumerate(zip(cfg.width_list, cfg.depth_list)):\n",
        "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
        "            stage = build_stage_main(\n",
        "                width=width, depth=depth, block_type=block_type, norm=cfg.norm, act=cfg.act, input_width=width\n",
        "            )\n",
        "\n",
        "            if stage_id < num_stages - 1 and depth > 0:\n",
        "                downsample_block = build_downsample_block(\n",
        "                    block_type=cfg.downsample_block_type,\n",
        "                    in_channels=width,\n",
        "                    out_channels=cfg.width_list[stage_id + 1] if cfg.downsample_match_channel else width,\n",
        "                    shortcut=cfg.downsample_shortcut,\n",
        "                )\n",
        "                stage.append(downsample_block)\n",
        "            self.stages.append(OpSequential(stage))\n",
        "        self.stages = nn.ModuleList(self.stages)\n",
        "\n",
        "        self.project_out = build_encoder_project_out_block(\n",
        "            in_channels=cfg.width_list[-1],\n",
        "            out_channels=2 * cfg.latent_channels if cfg.double_latent else cfg.latent_channels,\n",
        "            norm=cfg.out_norm,\n",
        "            act=cfg.out_act,\n",
        "            shortcut=cfg.out_shortcut,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.project_in(x)\n",
        "        for stage in self.stages:\n",
        "            if len(stage.op_list) == 0:\n",
        "                continue\n",
        "            x = stage(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, cfg: DecoderConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        num_stages = len(cfg.width_list)\n",
        "        self.num_stages = num_stages\n",
        "        assert len(cfg.depth_list) == num_stages\n",
        "        assert len(cfg.width_list) == num_stages\n",
        "        assert isinstance(cfg.block_type, str) or (\n",
        "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
        "        )\n",
        "        assert isinstance(cfg.norm, str) or (isinstance(cfg.norm, list) and len(cfg.norm) == num_stages)\n",
        "        assert isinstance(cfg.act, str) or (isinstance(cfg.act, list) and len(cfg.act) == num_stages)\n",
        "\n",
        "        self.project_in = build_decoder_project_in_block(\n",
        "            in_channels=cfg.latent_channels,\n",
        "            out_channels=cfg.width_list[-1],\n",
        "            shortcut=cfg.in_shortcut,\n",
        "        )\n",
        "\n",
        "        self.stages: list[OpSequential] = []\n",
        "        for stage_id, (width, depth) in reversed(list(enumerate(zip(cfg.width_list, cfg.depth_list)))):\n",
        "            stage = []\n",
        "            if stage_id < num_stages - 1 and depth > 0:\n",
        "                upsample_block = build_upsample_block(\n",
        "                    block_type=cfg.upsample_block_type,\n",
        "                    in_channels=cfg.width_list[stage_id + 1],\n",
        "                    out_channels=width if cfg.upsample_match_channel else cfg.width_list[stage_id + 1],\n",
        "                    shortcut=cfg.upsample_shortcut,\n",
        "                )\n",
        "                stage.append(upsample_block)\n",
        "\n",
        "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
        "            norm = cfg.norm[stage_id] if isinstance(cfg.norm, list) else cfg.norm\n",
        "            act = cfg.act[stage_id] if isinstance(cfg.act, list) else cfg.act\n",
        "            stage.extend(\n",
        "                build_stage_main(\n",
        "                    width=width,\n",
        "                    depth=depth,\n",
        "                    block_type=block_type,\n",
        "                    norm=norm,\n",
        "                    act=act,\n",
        "                    input_width=(\n",
        "                        width if cfg.upsample_match_channel else cfg.width_list[min(stage_id + 1, num_stages - 1)]\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "            self.stages.insert(0, OpSequential(stage))\n",
        "        self.stages = nn.ModuleList(self.stages)\n",
        "\n",
        "        self.project_out = build_decoder_project_out_block(\n",
        "            in_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
        "            out_channels=cfg.in_channels,\n",
        "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
        "            upsample_block_type=cfg.upsample_block_type,\n",
        "            norm=cfg.out_norm,\n",
        "            act=cfg.out_act,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.project_in(x)\n",
        "        for stage in reversed(self.stages):\n",
        "            if len(stage.op_list) == 0:\n",
        "                continue\n",
        "            x = stage(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, cfg: DCAEConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.encoder = Encoder(cfg.encoder)\n",
        "        self.decoder = Decoder(cfg.decoder)\n",
        "\n",
        "        if self.cfg.pretrained_path is not None:\n",
        "            self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.cfg.pretrained_source == \"dc-ae\":\n",
        "            state_dict = torch.load(self.cfg.pretrained_path, map_location=\"cpu\", weights_only=True)[\"state_dict\"]\n",
        "            self.load_state_dict(state_dict)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def spatial_compression_ratio(self) -> int:\n",
        "        return 2 ** (self.decoder.num_stages - 1)\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.encoder(x)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor, global_step: int) -> torch.Tensor:\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x, torch.tensor(0), {}\n",
        "\n",
        "\n",
        "def dc_ae_f32c32(name: str, pretrained_path: str) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f32c32-in-1.0\", \"dc-ae-f32c32-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=32 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[0,4,8,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[0,5,10,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu]\"\n",
        "        )\n",
        "    elif name in [\"dc-ae-f32c32-sana-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=32 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[2,2,2,3,3,3] \"\n",
        "            \"encoder.downsample_block_type=Conv \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[3,3,3,3,3,3] \"\n",
        "            \"decoder.upsample_block_type=InterpolateConv \"\n",
        "            \"decoder.norm=trms2d decoder.act=silu \"\n",
        "            \"scaling_factor=0.41407\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def dc_ae_f64c128(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f64c128-in-1.0\", \"dc-ae-f64c128-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=128 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024,2048] encoder.depth_list=[0,4,8,2,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024,2048] decoder.depth_list=[0,5,10,2,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu,silu]\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def dc_ae_f128c512(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f128c512-in-1.0\", \"dc-ae-f128c512-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=512 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024,2048,2048] encoder.depth_list=[0,4,8,2,2,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024,2048,2048] decoder.depth_list=[0,5,10,2,2,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu,silu,silu]\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsgTY2id2HLR"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYMQDoL578HQ",
        "outputId": "c53aa10d-d4e5-4a46-ad6f-9a2cafbdfea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12, 16, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        if in_ch==out_ch: self.func = lambda x: x\n",
        "        if in_ch > out_ch:\n",
        "            # self.func = lambda x: F.interpolate(x.flatten(2).transpose(1,2), size=out_ch, mode='nearest-exact').transpose(1,2).unflatten(-1,(x.shape[-2:])) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "            # self.func = lambda x: F.adaptive_avg_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "            self.func = lambda x: F.adaptive_max_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        elif in_ch < out_ch:\n",
        "            # self.func = lambda x: F.interpolate(x.flatten(2).transpose(1,2), size=out_ch, mode='nearest-exact').transpose(1,2).unflatten(-1,(x.shape[-2:])) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "            self.func = lambda x: F.adaptive_avg_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "            # self.func = lambda x: F.adaptive_max_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "\n",
        "    def forward(self, x): return self.func(x) # [b,c,h,w] -> [b,o,h,w]\n",
        "\n",
        "class PixelShortcut(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(SameCh(in_ch, out_ch*r**2), nn.PixelShuffle(r)) #\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SameCh(in_ch*r**2, out_ch)) #\n",
        "        else: self.net = SameCh(in_ch, out_ch)\n",
        "    def forward(self, x): return self.net(x) # [b,c,h,w] -> [b,o,r*h,r*w]\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.net = nn.Sequential(SeparableConv2d(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.Conv2d(in_ch, out_ch*r**2, 1, bias=False), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(MultiConv(in_ch, out_ch*r**2, (3,7)), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # if self.r>1: self.net = nn.Sequential(MultiConv(in_ch, in_ch, (3,7), groups=in_ch, bias=False), nn.Conv2d(in_ch, out_ch*r**2, 1, bias=False), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, 7, 1, 7//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        if self.r>1: self.net = nn.Sequential(UIB(in_ch, out_ch*r**2, r=r), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(UIB(in_ch, in_ch), UIB(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "\n",
        "\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SeparableConv2d(in_ch*r**2, out_ch)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(SeparableConv2d(in_ch, out_ch//r**2), nn.PixelUnshuffle(r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, 1, bias=False), nn.Conv2d(out_ch, out_ch, 3, 1, 3//2, groups=out_ch, bias=False))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), MultiConv(in_ch*r**2, out_ch, (3,7))) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, 1, bias=False), MultiConv(out_ch, out_ch, (3,7), groups=out_ch, bias=False))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, 7, 1, 7//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), UIB(in_ch*r**2, out_ch, r=r))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), UIB(in_ch*r**2, in_ch), UIB(in_ch, out_ch))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch))\n",
        "\n",
        "\n",
        "\n",
        "        # if self.r>1: self.net = nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), out_r=r), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), in_r=r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        else: self.net = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2)\n",
        "        # self.net.apply(self.init_conv_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1, kernel=3):\n",
        "        super().__init__()\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=r)\n",
        "    def forward(self, x):\n",
        "        # print('UpDownBlock', x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        # print(self.block, self.shortcut_block)\n",
        "        # return self.block(x) #+ self.shortcut_block(x)\n",
        "        return self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1, kernel=3):\n",
        "        super().__init__()\n",
        "        # self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        act = nn.SiLU()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # print('UpDownBlock', x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        # print(self.block, self.shortcut_block)\n",
        "        out = self.block(x)\n",
        "        # shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        return out + shortcut\n",
        "        # return shortcut\n",
        "\n",
        "# if out>in, inter=max=near. ave=ave\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "# model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}