{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/Autoencoder/blob/main/VQ_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LxACli7GdyGq"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "# train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
        "batch_size = 64 # 4\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# dataiter = iter(train_loader) # get some random training images\n",
        "# images, labels = next(dataiter)\n",
        "# print(images.shape) # [batch, 3, 32, 32]\n",
        "# imshow(torchvision.utils.make_grid(images))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghQ8RSExs_A4",
        "outputId": "4867b02a-cb94-4bd8-c341-e94ecc586fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY\n",
            "From (redirected): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY&confirm=t&uuid=e93c21de-18f8-459e-946a-161c7bcf157f\n",
            "To: /content/buffer512.pkl\n",
            "100% 706M/706M [00:09<00:00, 76.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title gdown\n",
        "import pickle\n",
        "!gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "9UKkkuorG_b9"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        # self.transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        state, action, reward = self.data[idx]\n",
        "        state = self.transform(state)\n",
        "        return state\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 128 # 128 512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yfLogBYztcuM"
      },
      "outputs": [],
      "source": [
        "# @title chatgpt quantizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_emb, emb_dim, beta=0.5, decay=0.99):\n",
        "        super().__init__()\n",
        "        self.num_emb, self.emb_dim = num_emb, emb_dim\n",
        "        self.beta, self.decay = beta, decay\n",
        "        self.epsilon = 1e-5\n",
        "\n",
        "        self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim))\n",
        "        # self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim).uniform_(-3**.5, 3**.5))\n",
        "        # self.embeddings = nn.Parameter(torch.randn(num_emb, emb_dim).uniform_(-1./num_emb, 1./num_emb))\n",
        "\n",
        "        # Register buffers for EMA updates.\n",
        "        self.register_buffer('ema_cluster_size', torch.zeros(num_emb))\n",
        "        self.register_buffer('ema_w', self.embeddings.data.clone())\n",
        "        # self.ema_cluster_size = nn.Parameter(torch.zeros(num_emb), requires_grad=False)\n",
        "        # self.ema_w = nn.Parameter(self.embeddings.data.clone(), requires_grad=False)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Save the original shape and flatten the input to (batch_size * ..., emb_dim)\n",
        "        input_shape = z.shape\n",
        "        # flat_z = z.view(-1, self.emb_dim)\n",
        "\n",
        "        # flat_z = z.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        flat_z = z.permute(0,2,3,1).flatten(end_dim=-2) # [b*h*w,c]\n",
        "        # print(flat_z.shape)\n",
        "\n",
        "        # distances = (torch.sum(flat_z ** 2, dim=1, keepdim=True) + torch.sum(self.embeddings ** 2, dim=1) - 2 * torch.matmul(flat_z, self.embeddings.t()))\n",
        "        distances = (torch.sum(flat_z**2, dim=1, keepdim=True) + torch.sum(self.embeddings**2, dim=1) - 2*torch.matmul(flat_z, self.embeddings.T))\n",
        "        enc_ind = torch.argmin(distances, dim=1)\n",
        "        encs = F.one_hot(enc_ind, self.num_emb).type(flat_z.dtype)\n",
        "        # Quantise the input by replacing with the nearest embedding.\n",
        "        z_q = torch.matmul(encs, self.embeddings).view(input_shape)\n",
        "\n",
        "        commitment_loss = self.beta * F.mse_loss(z_q.detach(), z) # commitment loss.\n",
        "        # loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q-z.detach())**2) # comitment, codebook\n",
        "\n",
        "        if self.training:\n",
        "            # EMA update for cluster size.\n",
        "            ema_cluster_size = torch.sum(encs, dim=0)\n",
        "            self.ema_cluster_size = self.ema_cluster_size * self.decay + (1 - self.decay) * ema_cluster_size\n",
        "            # Laplace smoothing to avoid zero counts.\n",
        "            n = torch.sum(self.ema_cluster_size)\n",
        "            self.ema_cluster_size = ((self.ema_cluster_size + self.epsilon) / (n + self.num_emb * self.epsilon)) * n\n",
        "\n",
        "            # EMA update for the embedding weights.\n",
        "            # dw = torch.matmul(encs.t(), flat_z)\n",
        "            dw = encs.T @ flat_z.detach()\n",
        "            # print('vq fwd', encs.T, flat_z)\n",
        "            self.ema_w = self.ema_w * self.decay + (1 - self.decay) * dw\n",
        "\n",
        "            # Update embeddings with the EMA values.\n",
        "            self.embeddings.data = self.ema_w / self.ema_cluster_size.unsqueeze(1)\n",
        "\n",
        "        # # Compute perplexity of the encs.\n",
        "        # avg_probs = torch.mean(encs, dim=0)\n",
        "        # perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + self.epsilon)))\n",
        "\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # print(z_q.shape)\n",
        "        # flat_z = z.permute(0,2,3,1).flatten(end_dim=-2) # [b*h*w,c]\n",
        "\n",
        "        # z_q = z_q.transpose(1,2).reshape(*bchw)\n",
        "        # return loss, z_q, perplexity, enc_ind\n",
        "        return commitment_loss, z_q, enc_ind\n",
        "\n",
        "emb_dim, num_emb = 4,20\n",
        "# x = torch.randn(2, 3, 4)\n",
        "x = torch.randn(2, emb_dim, 5, 7)\n",
        "vq = VectorQuantizerEMA(num_emb, emb_dim, beta=0.5) # chat gpt\n",
        "loss, z_q, enc_ind = vq(x)\n",
        "print(z_q.shape)\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Mje-yFj88WlY"
      },
      "outputs": [],
      "source": [
        "# @title FSQ me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module):\n",
        "    def __init__(self, levels):\n",
        "        super().__init__()\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cumprod(torch.tensor([*levels[1:], 1], device=device).flip(-1), dim=0).flip(-1)\n",
        "        self.half_width = (self.levels-1)/2\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def forward(self, z, beta=1): # beta in (0,1). beta->0 => values more spread out\n",
        "        offset = (self.levels+1) % 2 /2 # .5 if even, 0 if odd\n",
        "        bound = (F.sigmoid(z)-1/2) * (self.levels-beta) + offset\n",
        "        # print('fwd', bound) #\n",
        "        quantized = ste_round(bound)\n",
        "        # print('fwd', quantized) # 4: -1012\n",
        "        return (quantized-offset) / self.half_width # split [-1,1]\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        zhat = (zhat + 1) * self.half_width\n",
        "        return (zhat * self.basis).sum(axis=-1)#.int()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes = torch.remainder(indices//self.basis, self.levels)\n",
        "        # print(\"codes\",codes)\n",
        "        return codes / self.half_width - 1\n",
        "\n",
        "fsq = FSQ(levels = [5,4,3,2])\n",
        "# print(fsq.codebook)\n",
        "batch_size, seq_len = 2, 4\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "x = torch.linspace(-2,2,7, device=device).repeat(4,1).T\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "lact = fsq.codes_to_indexes(la)\n",
        "print(lact)\n",
        "# la = fsq.indexes_to_codes(lact)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4iRZMIxPsMTV"
      },
      "outputs": [],
      "source": [
        "# @title vqvae me\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=32, z_dim=3):\n",
        "        super().__init__()\n",
        "        d_list=[16, 3] # 849126\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        kernel = 3\n",
        "        self.encoder = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, 16, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 14\n",
        "            nn.Conv2d(in_ch, d_model, kernel, 2, kernel//2), nn.BatchNorm2d(d_model), act,# nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(d_model, z_dim, kernel, 2, kernel//2), act\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, d_model, kernel, stride=2, padding=kernel//2, output_padding=1), nn.BatchNorm2d(d_model), act,\n",
        "            # nn.Upsample(scale_factor=2),\n",
        "            nn.ConvTranspose2d(d_model, in_ch, kernel, 2, padding=kernel//2, output_padding=1)\n",
        "        )\n",
        "        # self.vq = VectorQuantizerEMA(num_emb=20, emb_dim=z_dim, beta=0.5) # chat gpt\n",
        "        self.vq = FSQ(levels = z_dim*[8])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # print(x.shape)\n",
        "        commitment_loss, quantised, encoding_indices = self.vq(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(quantised)\n",
        "        return x, commitment_loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.quantise(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.quantise(x)\n",
        "    def decode(self, x):\n",
        "        # _, x, _ = self.vq(x)\n",
        "        x = self.quantise(x)\n",
        "        return self.decoder(x)\n",
        "    def quantise(self, x): # [b,c,h,w]->[b,h,w,c]->[b,c,h,w]\n",
        "        return self.vq(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
        "\n",
        "\n",
        "in_ch=3\n",
        "d_model=32\n",
        "z_dim=3\n",
        "model = VQVAE(in_ch, d_model, z_dim).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 16x16 conv 17651 ; pixel(3)(3)  ; (1)(1)  ; (3,7,15)(3,7)  ; (3,5,7)(3,5) 42706 ; 7,5 70226\n",
        "\n",
        "x = torch.randn((2, in_ch, 64, 64), device=device)\n",
        "# out, _ = model(x)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "CtmsyKatYT7h"
      },
      "outputs": [],
      "source": [
        "# @title res & attn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).view(batch, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K, V = self.kv(cond).view(batch, -1, self.n_heads, 2*self.d_head).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.lin(out) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, ff_dim=None, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.ReLU()\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.self(self.norm1(x))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        return x.transpose(1,2).reshape(*bchw)\n",
        "\n",
        "\n",
        "# class GLUMBConv(nn.Module):\n",
        "#     def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4):\n",
        "#         super().__init__()\n",
        "#         mid_channels = round(in_ch * expand_ratio) if mid_channels is None else mid_channels\n",
        "#         self.inverted_depth_conv = nn.Sequential(\n",
        "#             nn.Conv2d(in_ch, mid_channels*2, 1, 1, 0), nn.SiLU(),\n",
        "#             nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2),\n",
        "#         )\n",
        "#         self.point_conv = nn.Sequential(\n",
        "#             nn.Conv2d(mid_channels, out_ch, 1, 1, 0, bias=False), nn.BatchNorm2d(out_ch), # \"ln2d\"\n",
        "#         )\n",
        "# https://discuss.pytorch.org/t/is-there-a-layer-normalization-for-conv2d/7595/5\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.inverted_depth_conv(x)\n",
        "#         x, gate = torch.chunk(x, 2, dim=1)\n",
        "#         x = x * nn.SiLU()(gate)\n",
        "#         x = self.point_conv(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "    # def forward(self, x):\n",
        "    #     res = self.forward_main(self.pre_norm(x)) + self.shortcut(x)\n",
        "    #     res = self.post_act(res)\n",
        "    #     return res\n",
        "\n",
        "\n",
        "# d_model=8\n",
        "# d_head=4\n",
        "# batch=4\n",
        "# h,w=5,6\n",
        "# x=torch.rand(batch,d_model,h,w)\n",
        "# cond_dim=10\n",
        "# model = AttentionBlock(d_model=d_model, d_head=d_head,cond_dim=cond_dim)\n",
        "# num_tok=1\n",
        "# cond=torch.rand(batch,num_tok,cond_dim)\n",
        "# mask=torch.rand(batch,h*w)>0.5\n",
        "# out = model(x, cond, mask)\n",
        "# print(out.shape)\n",
        "# # print(out)\n",
        "\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.block1 = nn.Sequential(nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "        # self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)))\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        h = self.block1(x)\n",
        "        h = self.block2(h, emb)\n",
        "        return h + self.res_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuXDKUIACl91",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title encoder me\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, depth=4, num_res_blocks=1, n_head=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch # z_channels z_dim\n",
        "        # n_head = d_model // d_head\n",
        "        # self.vq = VectorQuantizerEMA(num_emb=8192, emb_dim=out_ch, beta=0.5) # chat gpt\n",
        "        # self.vq = FSQ(levels = z_dim*[32])\n",
        "\n",
        "        mult = [1,1,1,1]\n",
        "        # mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            # nn.Conv2d(in_ch, ch_list[0], 3, 1, padding=3//2),\n",
        "\n",
        "            ResBlock(in_ch, ch_list[0]),\n",
        "            # ResBlock(ch_list[0], ch_list[1]),\n",
        "            # ResBlock(ch_list[1], ch_list[1]),\n",
        "            AttentionBlock(ch_list[0], d_head),\n",
        "\n",
        "            # nn.PixelUnshuffle(2),\n",
        "            # ResBlock(ch_list[0]*2**2, ch_list[1]),\n",
        "            # nn.AvgPool2d(2,2),\n",
        "            # nn.MaxPool2d(2,2),\n",
        "            UpDownBlock(3, ch_list[0], r=1/2),\n",
        "\n",
        "            ResBlock(ch_list[0], ch_list[1]),\n",
        "            # ResBlock(ch_list[2], ch_list[2]),\n",
        "            AttentionBlock(ch_list[1], d_head),\n",
        "            # ResBlock(ch_list[1], ch_list[2]),\n",
        "\n",
        "            # nn.PixelUnshuffle(2),\n",
        "            # ResBlock(ch_list[2]*2**2, ch_list[3]),\n",
        "            # nn.AvgPool2d(2,2),\n",
        "            UpDownBlock(ch_list[0], out_ch, r=1/2),\n",
        "\n",
        "            ResBlock(ch_list[2], ch_list[3]),\n",
        "            AttentionBlock(ch_list[3], d_head),\n",
        "            # ResBlock(ch_list[3], ch_list[3]),\n",
        "            ResBlock(ch_list[3], out_ch),\n",
        "\n",
        "            # # nn.GroupNorm(32, ch_list[-1]), nn.SiLU(), nn.Conv2d(ch_list[-1], out_ch, 3, 1, padding=3//2)\n",
        "            # nn.BatchNorm2d(ch_list[-1]), nn.SiLU(), nn.Conv2d(ch_list[-1], out_ch, 3, 1, padding=3//2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # nn.Conv2d(out_ch, ch_list[-1], 3, 1, padding=3//2),\n",
        "\n",
        "            ResBlock(out_ch, ch_list[3]),\n",
        "            # ResBlock(ch_list[3], ch_list[3]),\n",
        "            AttentionBlock(ch_list[3], d_head),\n",
        "            ResBlock(ch_list[3], ch_list[2]),\n",
        "            # nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            UpDownBlock(out_ch, ch_list[0], r=2),\n",
        "\n",
        "            # ResBlock(ch_list[3], ch_list[2]*2**2),\n",
        "            # # AttentionBlock(ch_list[2]*2**2, d_head),\n",
        "            # nn.PixelShuffle(2),\n",
        "\n",
        "            ResBlock(ch_list[2], ch_list[1]),\n",
        "            AttentionBlock(ch_list[2], d_head),\n",
        "            # ResBlock(ch_list[1], ch_list[0]),\n",
        "            # nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            UpDownBlock(ch_list[0], in_ch, r=2),\n",
        "            # ResBlock(ch_list[1], ch_list[0]*2**2),\n",
        "            # nn.PixelShuffle(2),\n",
        "\n",
        "            ResBlock(ch_list[1], ch_list[0]),\n",
        "            AttentionBlock(ch_list[0], d_head),\n",
        "            ResBlock(ch_list[0], in_ch),\n",
        "\n",
        "            # # nn.GroupNorm(32, ch_list[0]), nn.SiLU(), nn.Conv2d(ch_list[0], in_ch, 3, 1, padding=3//2)\n",
        "            # nn.BatchNorm2d(ch_list[0]), nn.SiLU(), zero_module(nn.Conv2d(ch_list[0], in_ch, 3, 1, padding=3//2)) # zero\n",
        "        )\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x = self.encoder(x)\n",
        "    #     # print(x.shape)\n",
        "    #     commitment_loss, x, _ = self.vq(x)\n",
        "    #     # print(x.shape)\n",
        "    #     x = self.decoder(x)\n",
        "    #     return x, commitment_loss\n",
        "\n",
        "    # def encode(self, x):\n",
        "    #     x = self.encoder(x)\n",
        "    #     _, x, _ = self.vq(x)\n",
        "    #     return x\n",
        "\n",
        "    # def decode(self, x):\n",
        "    #     _, x, _ = self.vq(x)\n",
        "    #     return self.decoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # x = self.quantise(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # return self.quantise(x)\n",
        "        return x\n",
        "    def decode(self, x):\n",
        "        # _, x, _ = self.vq(x)\n",
        "        # x = self.quantise(x)\n",
        "        return self.decoder(x)\n",
        "    def quantise(self, x): # [b,c,h,w]->[b,h,w,c]->[b,c,h,w]\n",
        "        return self.vq(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
        "\n",
        "\n",
        "\n",
        "batch=2\n",
        "in_ch=3\n",
        "z_dim=3\n",
        "h,w = 64,64\n",
        "model = VQVAE(in_ch, d_model=16, out_ch=z_dim, depth=4, num_res_blocks=1, n_head=-1, d_head=4).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "x = torch.rand((batch, in_ch, h, w), device=device)\n",
        "# out, _ = model(x)\n",
        "# print(out.shape)\n",
        "# x = model.quantise(x)\n",
        "# print(x.shape)\n",
        "# print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SeparableConv2d\n",
        "# Xception: Deep Learning with Depthwise Separable Convolutions https://arxiv.org/pdf/1610.02357\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False)\n",
        "        self.pointwise = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        # self.pointwise = zero_module(nn.Conv2d(in_ch, out_ch, 1, bias=False))\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class SeparableConv1d(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Convd(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False)\n",
        "        self.pointwise = nn.Convd(in_ch, out_ch, 1, bias=False)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "# #  'all Convolution and SeparableConvolution layers are followed by batch normalization'?\n",
        "# in_ch, out_ch = 3, 3\n",
        "# model = SeparableConv2d(in_ch, out_ch)\n",
        "# x = torch.randn(2, in_ch, 7,9)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n"
      ],
      "metadata": {
        "id": "WGM1ASSN4d82",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ConvNeXt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from timm.models.layers import trunc_normal_, DropPath\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "# https://github.com/facebookresearch/ConvNeXt-V2/blob/main/models/utils.py\n",
        "class GRN(nn.Module):\n",
        "    # \"\"\" GRN (Global Response Normalization) layer\"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        # self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
        "        # self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))\n",
        "        self.gamma = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
        "        self.beta = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print('GRN', x.shape)\n",
        "        Gx = torch.norm(x, p=2, dim=(1,2,3), keepdim=True)\n",
        "        # print('GRN', Gx.shape)\n",
        "        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)\n",
        "        # print('GRN', Nx.shape)\n",
        "        # print(self.gamma.shape ,(x * Nx).shape , self.beta.shape , x.shape)\n",
        "        return self.gamma * (x * Nx) + self.beta + x\n",
        "\n",
        "# https://github.com/facebookresearch/ConvNeXt-V2/blob/main/models/convnextv2.py\n",
        "class ConvNeXtV2Block(nn.Module):\n",
        "    \"\"\" ConvNeXtV2 Block \"\"\"\n",
        "    def __init__(self, dim, drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(1e-6 * torch.ones((dim)), requires_grad=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim), LayerNorm2d(dim, eps=1e-6),\n",
        "            nn.Conv2d(dim, 4 * dim, 1), nn.GELU(), #GRN(4 * dim),\n",
        "            nn.Conv2d(4 * dim, dim, 1),# self.scale, DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        )\n",
        "\n",
        "    def scale(self, x): return self.gamma * x\n",
        "    def forward(self, x):\n",
        "        return x + self.conv(x)\n",
        "\n",
        "model = ConvNeXtV2Block(32).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "x = torch.rand((2,32,32,32), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "7goZkapuLoKJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MultiConv\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class MultiConv(nn.Module):\n",
        "    # def __init__(self, in_ch, out_ch=None, kernel_sizes=(3,7), stride=1, groups=1):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel_sizes=(3,7), **kwargs):\n",
        "    # def __init__(self, in_ch, out_ch=None, kernel_sizes=(3,7), *args):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        # self.convs = nn.ModuleList([nn.Conv2d(in_ch, out_ch, kernel, stride, kernel//2, groups=groups, **kwargs) for kernel in kernel_sizes])\n",
        "        # self.convs = nn.ModuleList([nn.Conv2d(in_ch, out_ch, kernel, padding=(kernel-stride)//2, **kwargs) for kernel in kernel_sizes])\n",
        "        # self.convs = nn.ModuleList([nn.Conv2d(in_ch, out_ch, kernel, padding=kernel//2, *args) for kernel in kernel_sizes])\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(in_ch, out_ch, kernel, padding=kernel//2, **kwargs) for kernel in kernel_sizes])\n",
        "    def forward(self, x): return sum(conv(x) for conv in self.convs)\n",
        "# https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338\n",
        "\n",
        "\n",
        "# model = MultiConv(3,3,(3,7),2).to(device)\n",
        "model=MultiConv(4, 16, (3,7), groups=4, bias=False)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "# x = torch.rand((2,3,16,16), device=device)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n",
        "\n",
        "class ConvNeXtV2Block(nn.Module):\n",
        "    \"\"\" ConvNeXtV2 Block \"\"\"\n",
        "    def __init__(self, dim, drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(1e-6 * torch.ones((dim)), requires_grad=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim), LayerNorm2d(dim, eps=1e-6),\n",
        "            nn.Conv2d(dim, 4 * dim, 1), nn.GELU(), #GRN(4 * dim),\n",
        "            nn.Conv2d(4 * dim, dim, 1),# self.scale, DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        )\n",
        "\n",
        "    def scale(self, x): return self.gamma * x\n",
        "    def forward(self, x):\n",
        "        return x + self.conv(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6rLPfpxh7oB",
        "outputId": "a5866f90-8162-425b-ce31-751071227e06",
        "cellView": "form"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pixel UIB\n",
        "class UIB(nn.Module):\n",
        "    # def __init__(self, in_ch, out_ch=None, mult=4):\n",
        "    def __init__(self, in_ch, out_ch=None, r=1):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        out_ch = out_ch or in_ch\n",
        "        # self.gamma = nn.Parameter(1e-5 * torch.ones(out_ch, 1, 1))\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        mult = r**2\n",
        "        # mult = 4\n",
        "        print(mult)\n",
        "        if self.r==1: self.conv = nn.Sequential( # og\n",
        "        # self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.BatchNorm2d(in_ch),\n",
        "            nn.Conv2d(in_ch, mult*in_ch, 1, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.Conv2d(mult*in_ch, mult*in_ch, 3, 1, 3//2, groups=mult*in_ch, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.Conv2d(mult*in_ch, out_ch, 1, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "        if self.r<1: self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.BatchNorm2d(in_ch),\n",
        "            # nn.Conv2d(in_ch, mult*in_ch, 1, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.PixelUnshuffle(r), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.Conv2d(mult*in_ch, mult*in_ch, 3, 1, 3//2, groups=mult*in_ch, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.Conv2d(mult*in_ch, out_ch, 1, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "        if self.r>1: self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.BatchNorm2d(in_ch),\n",
        "            nn.Conv2d(in_ch, mult*in_ch, 1, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            nn.Conv2d(mult*in_ch, mult*in_ch, 3, 1, 3//2, groups=mult*in_ch, bias=False), nn.BatchNorm2d(mult*in_ch), act,\n",
        "            # nn.Conv2d(mult*in_ch, out_ch, 1, bias=False), nn.BatchNorm2d(out_ch),\n",
        "            nn.Conv2d(mult*in_ch, mult*out_ch, 1, bias=False), nn.BatchNorm2d(mult*out_ch),\n",
        "            nn.PixelShuffle(r),\n",
        "            # nn.Conv2d(in_ch//mult, mult*out_ch, 1, bias=False), nn.BatchNorm2d(mult*out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x = self.conv(x) * self.gamma\n",
        "        # return x\n",
        "        return self.conv(x) #* self.gamma\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "model = UIB(in_ch, out_ch, r=1/2)\n",
        "# model = UIB(in_ch, out_ch, r=2)\n",
        "x = torch.rand(128, in_ch, 64, 64)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxGF8bYeGDA2",
        "outputId": "169f437d-580e-4217-84f5-506833342cc6",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "torch.Size([128, 16, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JYMQDoL578HQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae42413-3952-4dc2-f28f-aba861d4e804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 16, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        if in_ch==out_ch: self.func = lambda x: x\n",
        "        if in_ch > out_ch:\n",
        "            # self.func = lambda x: F.interpolate(x.flatten(2).transpose(1,2), size=out_ch, mode='nearest-exact').transpose(1,2).unflatten(-1,(x.shape[-2:])) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "            # self.func = lambda x: F.adaptive_avg_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "            self.func = lambda x: F.adaptive_max_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        elif in_ch < out_ch:\n",
        "            # self.func = lambda x: F.interpolate(x.flatten(2).transpose(1,2), size=out_ch, mode='nearest-exact').transpose(1,2).unflatten(-1,(x.shape[-2:])) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "            self.func = lambda x: F.adaptive_avg_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "            # self.func = lambda x: F.adaptive_max_pool1d(x.flatten(2).transpose(1,2), out_ch).transpose(1,2).unflatten(-1,(x.shape[-2:])) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "\n",
        "    def forward(self, x): return self.func(x) # [b,c,h,w] -> [b,o,h,w]\n",
        "\n",
        "class PixelShortcut(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(SameCh(in_ch, out_ch*r**2), nn.PixelShuffle(r)) #\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SameCh(in_ch*r**2, out_ch)) #\n",
        "        else: self.net = SameCh(in_ch, out_ch)\n",
        "    def forward(self, x): return self.net(x) # [b,c,h,w] -> [b,o,r*h,r*w]\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.ConvTranspose2d(in_ch, out_ch, kernel, 2, kernel//2, output_padding=1))\n",
        "        # if self.r>1: self.net = nn.Sequential(SeparableConv2d(in_ch, out_ch*r**2), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, in_ch, 3, 1, 3//2, groups=in_ch, bias=False), nn.Conv2d(in_ch, out_ch*r**2, 1, bias=False), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(MultiConv(in_ch, out_ch*r**2, (3,7)), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # if self.r>1: self.net = nn.Sequential(MultiConv(in_ch, in_ch, (3,7), groups=in_ch, bias=False), nn.Conv2d(in_ch, out_ch*r**2, 1, bias=False), nn.PixelShuffle(r))\n",
        "        # if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, 7, 1, 7//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # if self.r>1: self.net = UIB(in_ch, out_ch, r=r)\n",
        "\n",
        "\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel, 2, kernel//2))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SeparableConv2d(in_ch*r**2, out_ch)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(SeparableConv2d(in_ch, out_ch//r**2), nn.PixelUnshuffle(r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, 1, bias=False), nn.Conv2d(out_ch, out_ch, 3, 1, 3//2, groups=out_ch, bias=False))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), MultiConv(in_ch*r**2, out_ch, (3,7))) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, 1, bias=False), MultiConv(out_ch, out_ch, (3,7), groups=out_ch, bias=False))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, 7, 1, 7//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        # elif self.r<1: self.net = UIB(in_ch, out_ch, r=r)\n",
        "\n",
        "\n",
        "\n",
        "        # if self.r>1: self.net = nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), out_r=r), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), in_r=r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        else: self.net = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2)\n",
        "        # self.net.apply(self.init_conv_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1, kernel=3):\n",
        "        super().__init__()\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=r)\n",
        "    def forward(self, x):\n",
        "        # print('UpDownBlock', x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        # print(self.block, self.shortcut_block)\n",
        "        # return self.block(x) #+ self.shortcut_block(x)\n",
        "        return self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, r=1, kernel=3):\n",
        "        super().__init__()\n",
        "        # self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        act = nn.SiLU()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_ch), act, PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # print('UpDownBlock', x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        # print(self.block, self.shortcut_block)\n",
        "        out = self.block(x)\n",
        "        shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # shortcut = F.adaptive_max_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        # shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) if out.shape[1]>=x.shape[1] else F.adaptive_max_pool3d(x, out.shape[1:])\n",
        "        return out + shortcut\n",
        "        # return shortcut\n",
        "\n",
        "# if out>in, inter=max=near. ave=ave\n",
        "# if out<in, inter=ave. max=max\n",
        "\n",
        "# in_ch, out_ch = 16,3\n",
        "in_ch, out_ch = 3,16\n",
        "# model = UpDownBlock(in_ch, out_ch, r=1/2).to(device)\n",
        "model = UpDownBlock(in_ch, out_ch, r=2).to(device)\n",
        "\n",
        "x = torch.rand(12, in_ch, 64,64, device=device)\n",
        "out = model(x)\n",
        "\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1,5)\n",
        "# shortcut = F.interpolate(x.unsqueeze(1), size=out.shape[1:], mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "shortcut = F.interpolate(x.unsqueeze(1), size=(3,), mode='nearest-exact').squeeze(1) # pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html\n",
        "print(x)\n",
        "print(shortcut)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2p6MjLUTHmI",
        "outputId": "ae791580-6fe3-489d-d212-5c435e24b9ce"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9318, 0.4083, 0.6760, 0.5489, 0.5880]])\n",
            "tensor([[0.9318, 0.6760, 0.5880]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ae\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, depth=4, num_res_blocks=1, n_head=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch # z_channels z_dim\n",
        "        # n_head = d_model // d_head\n",
        "\n",
        "        mult = [1,1,1,1]\n",
        "        # mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult] # [128, 256, 384, 512]\n",
        "        act = nn.SiLU()\n",
        "        self.encoder = nn.Sequential(\n",
        "            # UpDownBlock(3, ch_list[0], r=1/2), nn.BatchNorm2d(ch_list[0]), act,\n",
        "            UpDownBlock(3, ch_list[0], r=1/2), #act,\n",
        "            UpDownBlock(ch_list[0], out_ch, r=1/2), #act,\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # UpDownBlock(out_ch, ch_list[0], r=2), nn.BatchNorm2d(ch_list[0]), act,\n",
        "            UpDownBlock(out_ch, ch_list[0], r=2), #act,\n",
        "            UpDownBlock(ch_list[0], in_ch, r=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.encoder(x)\n",
        "        # x = self.decoder(x)\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "    def encode(self, x):return self.encoder(x)\n",
        "    def decode(self, x):return self.decoder(x)\n",
        "\n",
        "# split plateaus\n",
        "# conv 3<7\n",
        "# stride2 < pixel\n",
        "\n",
        "batch=64\n",
        "in_ch=3\n",
        "z_dim=3\n",
        "h,w = 64,64\n",
        "model = VQVAE(in_ch, d_model=16, out_ch=z_dim, depth=4, num_res_blocks=1, n_head=-1, d_head=4).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "x = torch.rand((batch, in_ch, h, w), device=device)\n",
        "out = model.encode(x)\n",
        "# print(out.shape)\n",
        "# out = torch.rand((batch, in_ch, h, w), device=device)\n",
        "x_ = model.decode(out)\n",
        "# x = model.quantise(x)\n",
        "print(x_.shape)\n",
        "# # print(x)\n",
        "# x_ = model(x)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     state = buffer[12][40][0]\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     # transform = transforms.Compose([transforms.ToTensor()])\n",
        "#     x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "\n",
        "#     # out, _ = model(x)\n",
        "#     # out = model(x)\n",
        "#     print(x[0,0,0])\n",
        "#     sx = model.encoder(x)\n",
        "#     print(sx.shape)\n",
        "#     print(sx[0,0,0])\n",
        "#     out = model.decoder(sx)\n",
        "#     imshow(torchvision.utils.make_grid(x.cpu()))\n",
        "#     imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "#     imshow(torchvision.utils.make_grid(out.cpu()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O9NnnDvZI37",
        "outputId": "7e56c707-50bf-4ce9-faa2-b4ee21cc3264"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7083\n",
            "torch.Size([64, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-VeKdVamz2w"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2)\n",
        "print(conv.weight.data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7CtfbCdIpCG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # encoder mult=[1,2,4,4,8,8] # depth_list=[0,4,8,2,2,2]\n",
        "        # decoder mult=[1,2,4,4,8,8] # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, width_list[0], 3, 2, padding=3//2),\n",
        "\n",
        "            UpDownBlock(in_ch, width_list[0], r=1/2),\n",
        "            ResBlock(width_list[0]),\n",
        "            UpDownBlock(width_list[0], width_list[-1], r=1/2),\n",
        "            ResBlock(width_list[-1]),\n",
        "            # AttentionBlock(width_list[-1], d_head=4),\n",
        "            UpDownBlock(width_list[-1], out_ch, r=1),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            # UpDownBlock(in_ch, width_list[-1], r=1),\n",
        "            # AttentionBlock(width_list[-1], d_head=4),\n",
        "            ResBlock(width_list[-1]),\n",
        "            UpDownBlock(width_list[-1], width_list[0], r=2),\n",
        "            ResBlock(width_list[0]),\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpDownBlock(width_list[0], in_ch, r=2)\n",
        "            nn.ConvTranspose2d(out_ch, width_list[-1], 3, 2, padding=3//2, output_padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=24, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "sx = model.encoder(x)\n",
        "print(sx.shape)\n",
        "out = model.decoder(sx)\n",
        "# out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rJQDMix9EpYZ",
        "outputId": "cf774156-9d7f-409a-f048-65175d184ff8",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0.6455811262130737\n",
            "0.055170997977256775\n",
            "0.04227474331855774\n",
            "0.03918161243200302\n",
            "0.032305385917425156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2898297..1.5424377].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2027608..1.1266702].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0.03221188485622406\n",
            "0.029472922906279564\n",
            "0.027356229722499847\n",
            "0.026573721319437027\n",
            "0.028334274888038635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2026267..1.2271004].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2093694..1.125956].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "0.027233704924583435\n",
            "0.02537374570965767\n",
            "0.02502419613301754\n",
            "0.025841478258371353\n",
            "0.024988308548927307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.119899..1.0213058].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.156741..1.110583].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0.024112915620207787\n",
            "0.02347622439265251\n",
            "0.023735476657748222\n",
            "0.022642826661467552\n",
            "0.023513399064540863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0634617..0.9329197].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.1587546..1.1567811].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "0.021885430440306664\n",
            "0.022133231163024902\n",
            "0.02204084023833275\n",
            "0.02136155590415001\n",
            "0.021769797429442406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.093503..0.83784056].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARcAAAESCAYAAADXHpFnAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAF7VJREFUeJzt3X1QVOe9B/Dv7uIuiLAGkJeNoCSxGhVJo4Gxtrlm3Al6jdWZNDUZayntpElKYi2tmswUiXkpMc3kkkQH08xt0TuRmulEm8ncmslQjbXxFWLa9DYKhghKFnyDhQUW2D33j9RtN4J4fnue7Fn6/czsjOyeH7+Hw/J12cPzPBZN0zQQERnMGu0BENHYxHAhIiUYLkSkBMOFiJRguBCREgwXIlKC4UJESsRFewBfFAwG0dbWhqSkJFgslmgPh4j+QdM0dHd3w+VywWod/XWJ6cKlra0N2dnZ0R4GEY2gtbUVkydPHvU404VLUlLS5/+o+x0wIVFfcbNH3LcoLV5Ud8YrP4Uf94yXFSaeF/ecZEkR1Z0PCs9tMElWByB/wpCo7sPL8le87rRxorpTPQFxz5b+BFmhtUPcc67Lpbsm4OvFif9c+c+f0VGYLlxCvwpNSNQfLuOFP6wAxiXKwsUWiOAUBnV+fVck+sQtrRZhz4DwByAo/57YEmXhAr88XMYlysLFqsnDBVbhObIKvycAbHp/tv7F9b5dwTd0iUgJZeGydetWTJ06FfHx8SgsLMTRo0dVtSIiE1ISLrt27UJZWRkqKirQ0NCA/Px8FBUVoaND/jsiEcUWJeHy4osv4sEHH0RJSQlmzpyJbdu2Yfz48fj1r3991bF+vx9erzfsRkSxz/BwGRgYQH19Pdxu9z+bWK1wu904dOjQVcdXVlbC6XSGbrwMTTQ2GB4uFy5cQCAQQEZGRtj9GRkZ8Hiuvpz5xBNPoKurK3RrbW01ekhEFAVRvxTtcDjgcDiiPQwiMpjhr1zS0tJgs9nQ3t4edn97ezsyMzONbkdEJmV4uNjtdsydOxd1dXWh+4LBIOrq6jB//nyj2xGRSSn5taisrAzFxcWYN28eCgoKUFVVBZ/Ph5KSEhXtiMiElITLypUrcf78eWzcuBEejwe33XYb9u7de9WbvEQ0dil7Q/fRRx/Fo48+Kv8EHT7Ap3PXk95+cbs2bUBU97egvCd6UmV1Q/KeXrtfVhgn7CmfBoX+G6RzZ+S75Zwa6hbVfWq9LO6Jy1myujjh9xLA2Qm9umuCvj5dx3NuEREpwXAhIiUYLkSkBMOFiJRguBCREgwXIlKC4UJESjBciEgJhgsRKcFwISIlGC5EpATDhYiUYLgQkRIMFyJSIupr6I6oKwgMBvXVnJdnZVOSbMkFXJZvV4r2QVndBPmSAn1W4TlyChtGsFXV38cLt0htsol7ns7Vt6xAyGAEP0oXhF9ngmwLYgC4lKr/uaf59dXwlQsRKcFwISIlGC5EpITh4VJZWYk77rgDSUlJSE9Px4oVK3Dy5Emj2xCRyRkeLu+99x5KS0tx+PBhvPvuuxgcHMTdd98Nny+CxVSJKOYYfrVo7969YR/X1NQgPT0d9fX1uPPOO6863u/3w+//50LD3IieaGxQ/p5LV1cXACAlJWXYx7kRPdHYpDRcgsEg1q5diwULFmD27NnDHsON6InGJqV/RFdaWoqPPvoIBw8eHPEYbkRPNDYp3RTt7bffxoEDBzB58mRVbYjIpAwPF03T8Nhjj2H37t3Yv38/cnNzjW5BRDHA8HApLS3Fzp078fvf/x5JSUnweDwAAKfTiYQE6facRBRrDH9Dt7q6Gl1dXVi4cCGysrJCt127dhndiohMTMmvRYa4HAT6dc6K9g2J2/X6E2WFnRHkc7dwNmyffCN69PXI6lwWWV23sA6AdlF4fnoj+J50Ci8u2CLoeUk4O368/Nz29wp66qzh3CIiUoLhQkRKMFyISAmGCxEpwXAhIiUYLkSkBMOFiJRguBCREgwXIlKC4UJESjBciEgJhgsRKcFwISIlGC5EpIR5N6K/NATE61xCoVvebui8zuUdrrgUwSnsGpDVBYRT9AFgSLh1i32crK4ngiU47MLavgjOT6dwGYOAcMkOALgsXAZjYLy8Z5dgOYs+fTV85UJESjBciEgJ5eHy3HPPwWKxYO3atapbEZGJKA2XY8eO4dVXX8WcOXNUtiEiE1IWLj09PVi1ahVee+013HDDDaraEJFJKQuX0tJSLF26FG63+5rH+f1+eL3esBsRxT4ll6J/+9vfoqGhAceOHRv12MrKSmzatEnFMIgoigx/5dLa2oof//jHeP311xEfHz/q8dyInmhsMvyVS319PTo6OnD77beH7gsEAjhw4AC2bNkCv98Pm80Weowb0RONTYaHy6JFi/DXv/417L6SkhLMmDEDGzZsCAsWIhq7DA+XpKQkzJ49O+y+xMREpKamXnU/EY1d/AtdIlLiS5m4uH///i+jDRGZiIlnRQcBh86ZytIZrQBgk260LpxNDQB9Omd9X3FJOJsaAIaktcI/hBwUbiYPABB+T3yRXCAQzsQe6pW39AqfQ7YIZpxLZkX3c1Y0EZkAw4WIlGC4EJESDBciUoLhQkRKMFyISAmGCxEpwXAhIiUYLkSkBMOFiJRguBCREgwXIlKC4UJESjBciEgJ8y650GkB7Dqn3F+IYAPygHB6/2AEyzz4hEsudArrAKC3T1ZnHX2x9WHZhBvYA4BfupxFBD0DSbI63xl5T5/wxzAYwdcpWUFD5/eDr1yISAmGCxEpoSRczp07h+985ztITU1FQkIC8vLycPz4cRWtiMikDH/P5fLly1iwYAHuuusu/OEPf8CkSZPQ2NjI/aKJ/s0YHi6bN29GdnY2fvOb34Tuy83NNboNEZmc4b8WvfXWW5g3bx7uu+8+pKen46tf/Spee+21EY/nRvREY5Ph4fLJJ5+guroa06ZNwzvvvINHHnkEa9aswfbt24c9vrKyEk6nM3TLzs42ekhEFAUWTdMi2J/gana7HfPmzcP7778fum/NmjU4duwYDh06dNXxfr8ffr8/9LHX6/08YIr/B7CP19f8wiXxuJFil9VF9Hcuwm0+LkfwdUr/ziVzkqwukr9z0bu1TEiivGdCDP2dy/gIvs5bBDX+PuCFh9HV1YXk5ORRDzf8lUtWVhZmzpwZdt+tt96KlpaWYY93OBxITk4OuxFR7DM8XBYsWICTJ0+G3Xfq1ClMmTLF6FZEZGKGh8tPfvITHD58GL/4xS/Q1NSEnTt34le/+hVKS0uNbkVEJmZ4uNxxxx3YvXs3amtrMXv2bDz99NOoqqrCqlWrjG5FRCamZOLiPffcg3vuuUfFpyaiGGHeWdFt54FxCfpqBiL4cvqlG4lHcDVEOrn5vPCKDwD0Cb9Ou/DKVnwEFyMv2IQ9I5g13vI7WZ1FcvnlH6YKaweEVzgBwCN4Huh8CnDiIhEpwXAhIiUYLkSkBMOFiJRguBCREgwXIlKC4UJESjBciEgJhgsRKcFwISIlGC5EpATDhYiUYLgQkRLmnRV9rg+w6ZxRmxDBlzMo3Gd6XAQzU/sCsroLEeyQ0O8f/ZjhOISzqROFa9ICwIBwdvPFTnnPgPD/29MX5D0vnZXVzXXLe34meL7r/BnhKxciUoLhQkRKMFyISAnDwyUQCKC8vBy5ublISEjAzTffjKeffhoGb49ERCanZK/o6upqbN++HbNmzcLx48dRUlICp9OJNWvWGN2OiEzK8HB5//33sXz5cixduhQAMHXqVNTW1uLo0aNGtyIiEzP816Kvfe1rqKurw6lTpwAAH374IQ4ePIglS5YMezw3oicamwx/5fL444/D6/VixowZsNlsCAQCePbZZ0fct6iyshKbNm0yehhEFGWGv3J544038Prrr2Pnzp1oaGjA9u3b8cILL2D79u3DHv/EE0+gq6srdGttbTV6SEQUBYa/clm3bh0ef/xx3H///QCAvLw8nDlzBpWVlSguLr7qeIfDAYfDYfQwiCjKDH/l0tvbC6s1/NPabDYEg0GjWxGRiRn+ymXZsmV49tlnkZOTg1mzZuGDDz7Aiy++iO9///tGtyIiEzM8XF555RWUl5fjRz/6ETo6OuByufDQQw9h48aNRrciIhMzPFySkpJQVVWFqqoqoz81EcUQ8y650OEFrDqXB0hOlvfzdcvqEifIewpXecClHnlPv/DrnJgpq5MuYQAAfTp3Pr8iIUfes0O4tETgL/Kenj2yuu4IfnyDefprhvR9PzhxkYiUYLgQkRIMFyJSguFCREowXIhICYYLESnBcCEiJRguRKQEw4WIlGC4EJESDBciUoLhQkRKMFyISAnzzoruDQAWnRu19/rk/azCTeH7LPKeli5ZnV86nRoAhJueNwtm0QIATgvrAEwXzsT+OFXc0gLZ8yAdn4h7tqcliOriDuwQ9xy69b/0FwX0xQVfuRCREgwXIlKC4UJESugOlwMHDmDZsmVwuVywWCzYs2dP2OOapmHjxo3IyspCQkIC3G43GhsbjRovEcUI3eHi8/mQn5+PrVu3Dvv4888/j5dffhnbtm3DkSNHkJiYiKKiIvT390c8WCKKHbqvFi1ZsmTEfZ81TUNVVRV+/vOfY/ny5QCAHTt2ICMjA3v27AltlEZEY5+h77k0NzfD4/HA7XaH7nM6nSgsLMShQ4eGreFG9ERjk6Hh4vF4AAAZGRlh92dkZIQe+6LKyko4nc7QLTs728ghEVGURP1qETeiJxqbDA2XzMzP/6Kyvb097P729vbQY1/kcDiQnJwcdiOi2GdouOTm5iIzMxN1dXWh+7xeL44cOYL58+cb2YqITE731aKenh40NTWFPm5ubsaJEyeQkpKCnJwcrF27Fs888wymTZuG3NxclJeXw+VyYcWKFUaOm4hMTne4HD9+HHfddVfo47KyMgBAcXExampqsH79evh8Pvzwhz9EZ2cnvv71r2Pv3r2Ij483btREZHq6w2XhwoXQNG3Exy0WC5566ik89dRTEQ2MiGKbiZdcGAJg01kkXDYBEPS6IoIlBewfCwsd8p7SWttxWV3g77I6ADiZKCqzr1gvbjkwU1aX9GbG6AeNoL1VtoTGEIbEPdG6XX+Npq9f1C9FE9HYxHAhIiUYLkSkBMOFiJRguBCREgwXIlKC4UJESjBciEgJhgsRKcFwISIlGC5EpATDhYiUYLgQkRLmnRWNdgB2nTVZEfQTLq858V15y84GYeHD8p7ok5UFmoX9/k9YB6QI6y7tEbcEhNtrNX3aPvpBIxkvrPPJW8L6Z/01I6+0MnwL/R2IiEbHcCEiJRguRKSEoRvRDw4OYsOGDcjLy0NiYiJcLhe++93voq2tzcgxE1EMMHQj+t7eXjQ0NKC8vBwNDQ148803cfLkSXzzm980ZLBEFDsM3Yje6XTi3XfDr55s2bIFBQUFaGlpQU5OjmyURBRzlF+K7urqgsViwcSJE4d93O/3w+/3hz7mRvREY4PSN3T7+/uxYcMGPPDAAyNu08qN6InGJmXhMjg4iG9/+9vQNA3V1dUjHseN6InGJiW/Fl0JljNnzuCPf/zjNTeXdzgccDgi2YeHiMzI8HC5EiyNjY3Yt28fUlNTjW5BRDHA0I3os7Ky8K1vfQsNDQ14++23EQgE4PF4AAApKSmw2/XOFSKiWGXoRvRPPvkk3nrrLQDAbbfdFla3b98+LFy4UD5SIoophm9Ef63HiOjfh4mXXPgAejeHfwh/E3d7FZmywk6PuKf4Wl1wi7wnJoiqCu09orq0W0RlAIAh4WoNS+Ut0fwfsjrPR/KeLW0TRXW3olPcM3GW/hr/ELDt2PUfz4mLRKQEw4WIlGC4EJESDBciUoLhQkRKMFyISAmGCxEpwXAhIiUYLkSkBMOFiJRguBCREgwXIlKC4UJESph2VvS6ez+GY5y+Gsv/yvu9ulg2u/mjLHnP1JdkdZH8j3DjRtnsZpyWld3gH/2Ykfx5vqxuw3/Le97XLKurnS3veXhxp6juL5flPU8L9qG3BPUdz1cuRKQEw4WIlGC4EJEShm5E/0UPP/wwLBYLqqqqIhgiEcUiQzei/1e7d+/G4cOH4XK5xIMjothl6Eb0V5w7dw6PPfYY3nnnHSxdGsmKpkQUqwy/FB0MBrF69WqsW7cOs2aNvgowN6InGpsMf0N38+bNiIuLw5o1a67reG5ETzQ2GRou9fX1eOmll1BTUwOLxXJdNdyInmhsMjRc/vSnP6GjowM5OTmIi4tDXFwczpw5g5/+9KeYOnXqsDUOhwPJyclhNyKKfYa+57J69Wq43e6w+4qKirB69WqUlJQY2YqITM7QjehzcnKQmpoadvy4ceOQmZmJ6dOnRz5aIooZhm5EX1NTY9jAiCi2Gb4R/Rd9+umnelsQ0Rhg0fQkxZfA6/XC6XTiXgA6V1xAYZ68r90hq/Omy3v2No1+zHAS++U944XnyDoo7CcrAwD0dsrqWoTLJgBAQHhub79T3rM9IKvrj+BPwhJmCvoNAOU7gK6uruu68MKJi0SkBMOFiJRguBCREgwXIlKC4UJESjBciEgJhgsRKcFwISIlGC5EpATDhYiUYLgQkRIMFyJSwnR7RV+ZRymZJ9cvnAAGAAHp5DHhhD4A8At72nTu2RtGOF7LkKwuklmx0u+nP4LzExAOuC+C50G/cLz9wu8JAFgGBP3+UXO9c51NNyv67NmzXKSbyMRaW1sxefLkUY8zXbgEg0G0tbUhKSlp2EW+vV4vsrOz0drayvV2h8Hzc208P9d2rfOjaRq6u7vhcrlgtY7+jorpfi2yWq3XlYpczPvaeH6ujefn2kY6P06n87o/B9/QJSIlGC5EpETMhYvD4UBFRQUcDuG6lGMcz8+18fxcm5Hnx3Rv6BLR2BBzr1yIKDYwXIhICYYLESnBcCEiJRguRKRETIXL1q1bMXXqVMTHx6OwsBBHjx6N9pBM4cknn4TFYgm7zZgxI9rDiqoDBw5g2bJlcLlcsFgs2LNnT9jjmqZh48aNyMrKQkJCAtxuNxobG6Mz2CgY7fx873vfu+o5tXjxYl09YiZcdu3ahbKyMlRUVKChoQH5+fkoKipCR0dHtIdmCrNmzcJnn30Wuh08eDDaQ4oqn8+H/Px8bN26ddjHn3/+ebz88svYtm0bjhw5gsTERBQVFaG/P4K9cmPIaOcHABYvXhz2nKqtrdXXRIsRBQUFWmlpaejjQCCguVwurbKyMoqjMoeKigotPz8/2sMwLQDa7t27Qx8Hg0EtMzNT++Uvfxm6r7OzU3M4HFptbW0URhhdXzw/mqZpxcXF2vLlyyP6vDHxymVgYAD19fVwu92h+6xWK9xuNw4dOhTFkZlHY2MjXC4XbrrpJqxatQotLS3RHpJpNTc3w+PxhD2fnE4nCgsL+Xz6F/v370d6ejqmT5+ORx55BBcvXtRVHxPhcuHCBQQCAWRkZITdn5GRAY/HE6VRmUdhYSFqamqwd+9eVFdXo7m5Gd/4xjfQ3d0d7aGZ0pXnDJ9PI1u8eDF27NiBuro6bN68Ge+99x6WLFmCgI5V1Uy35ALpt2TJktC/58yZg8LCQkyZMgVvvPEGfvCDH0RxZBSr7r///tC/8/LyMGfOHNx8883Yv38/Fi1adF2fIyZeuaSlpcFms6G9vT3s/vb2dmRmZkZpVOY1ceJEfOUrX0FTU1O0h2JKV54zfD5dv5tuuglpaWm6nlMxES52ux1z585FXV1d6L5gMIi6ujrMnz8/iiMzp56eHpw+fRpZWVnRHoop5ebmIjMzM+z55PV6ceTIET6fRnD27FlcvHhR13MqZn4tKisrQ3FxMebNm4eCggJUVVXB5/OhpKQk2kOLup/97GdYtmwZpkyZgra2NlRUVMBms+GBBx6I9tCipqenJ+x/2ebmZpw4cQIpKSnIycnB2rVr8cwzz2DatGnIzc1FeXk5XC4XVqxYEb1Bf4mudX5SUlKwadMm3HvvvcjMzMTp06exfv163HLLLSgqKrr+JhFda/qSvfLKK1pOTo5mt9u1goIC7fDhw9EekimsXLlSy8rK0ux2u3bjjTdqK1eu1JqamqI9rKjat2+fhs83Hwi7FRcXa5r2+eXo8vJyLSMjQ3M4HNqiRYu0kydPRnfQX6JrnZ/e3l7t7rvv1iZNmqSNGzdOmzJlivbggw9qHo9HVw+u50JESsTEey5EFHsYLkSkBMOFiJRguBCREgwXIlKC4UJESjBciEgJhgsRKcFwISIlGC5EpATDhYiU+H+PhAX7SlG65AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2987136..1.2116321].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "0.021503549069166183\n",
            "0.022002702578902245\n",
            "0.020659834146499634\n",
            "0.01912136748433113\n",
            "0.020938223227858543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2308143..0.7601467].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.3927681..1.2525964].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "0.019610293209552765\n",
            "0.019222192466259003\n",
            "0.020670272409915924\n",
            "0.019518166780471802\n"
          ]
        }
      ],
      "source": [
        "# @title train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler(device)\n",
        "\n",
        "def train(model, optim, dataloader, scheduler=None):\n",
        "    model.train()\n",
        "    # for i, (x, _) in enumerate(dataloader):\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        # x1 = F.interpolate(x1, size=(16,16)).repeat(1,3,1,1)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x_, commitment_loss = model(x)\n",
        "            # loss = commitment_loss + F.mse_loss(x, x_)\n",
        "\n",
        "            x_ = model(x)\n",
        "            loss = F.mse_loss(x, x_)\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        # if i%10 == 0:\n",
        "            # with torch.no_grad():\n",
        "            #     state = buffer[12][40][0]\n",
        "            #     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "            #     # transform = transforms.Compose([transforms.ToTensor()])\n",
        "            #     x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "            #     out = model(x).squeeze(0)\n",
        "            #     sx = model.encoder(x).squeeze(0)\n",
        "            #     out = model.decoder(sx).squeeze(0)\n",
        "            #     imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "            #     imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "        if i % 100 == 0: print(loss.item())\n",
        "\n",
        "\n",
        "# for i in range(1):\n",
        "for i in range(20): # 10\n",
        "    print(i)\n",
        "    train(model, optim, train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # z = torch.randn(1,z_dim,8,8).to(device)\n",
        "        # _, z, _ = model.vq(z)\n",
        "        # z = model.quantise(z)\n",
        "\n",
        "        # out = model.decode(z)\n",
        "        # imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "        state = buffer[12][40][0]\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        # transform = transforms.Compose([transforms.ToTensor()])\n",
        "        x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "\n",
        "\n",
        "        # out, _ = model(x)\n",
        "        # out = model(x)\n",
        "        sx = model.encoder(x)\n",
        "        out = model.decoder(sx)\n",
        "        imshow(torchvision.utils.make_grid(x.cpu()))\n",
        "        imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "        imshow(torchvision.utils.make_grid(out.cpu()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "f90a91b1-b9ab-44c4-9a61-91e71020438e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.01763</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">noble-lake-25</strong> at: <a href='https://wandb.ai/bobdole/vqvae/runs/0blaafrg' target=\"_blank\">https://wandb.ai/bobdole/vqvae/runs/0blaafrg</a><br> View project at: <a href='https://wandb.ai/bobdole/vqvae' target=\"_blank\">https://wandb.ai/bobdole/vqvae</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250323_004757-0blaafrg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250323_010235-8lm5ntvq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/vqvae/runs/8lm5ntvq' target=\"_blank\">brisk-disco-26</a></strong> to <a href='https://wandb.ai/bobdole/vqvae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/vqvae' target=\"_blank\">https://wandb.ai/bobdole/vqvae</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/vqvae/runs/8lm5ntvq' target=\"_blank\">https://wandb.ai/bobdole/vqvae/runs/8lm5ntvq</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"vqvae\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXLAI-bVQ6G9"
      },
      "outputs": [],
      "source": [
        "# z = torch.randn(1,z_dim,16,16).to(device)\n",
        "# with torch.no_grad(): out = model.decode(z)\n",
        "# imshow(torchvision.utils.make_grid(out))\n",
        "\n",
        "with torch.no_grad():\n",
        "    state = buffer[12][40][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    # transform = transforms.Compose([transforms.ToTensor()])\n",
        "    x = transform(state).unsqueeze(0).to(device)#[0]\n",
        "    # out = model(x).squeeze(0)\n",
        "    sx = model.encoder(x)#.squeeze(0)\n",
        "    out = model.decoder(sx)#.squeeze(0)\n",
        "    imshow(torchvision.utils.make_grid(sx.cpu()))\n",
        "    imshow(torchvision.utils.make_grid(out.cpu()))\n",
        "\n",
        "# out, _ = model(x)\n",
        "# imshow(torchvision.utils.make_grid(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI3uaeX-r73O"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "psJQyxGNkOlE"
      },
      "outputs": [],
      "source": [
        "# @title vqvae from CompVis\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/models/autoencoder.py\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from contextlib import contextmanager\n",
        "\n",
        "from ldm.modules.diffusionmodules.model import Encoder, Decoder\n",
        "from ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "class VQModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 n_embed,\n",
        "                 embed_dim,\n",
        "                 remap=None,\n",
        "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "                 use_ema=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25, remap=remap, sane_index_shape=sane_index_shape)\n",
        "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "\n",
        "        self.use_ema = use_ema\n",
        "        if self.use_ema:\n",
        "            self.model_ema = LitEma(self)\n",
        "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
        "\n",
        "    @contextmanager\n",
        "    def ema_scope(self, context=None):\n",
        "        if self.use_ema:\n",
        "            self.model_ema.store(self.parameters())\n",
        "            self.model_ema.copy_to(self)\n",
        "            if context is not None:\n",
        "                print(f\"{context}: Switched to EMA weights\")\n",
        "        try:\n",
        "            yield None\n",
        "        finally:\n",
        "            if self.use_ema:\n",
        "                self.model_ema.restore(self.parameters())\n",
        "                if context is not None:\n",
        "                    print(f\"{context}: Restored training weights\")\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        if self.use_ema:\n",
        "            self.model_ema(self)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        # https://github.com/pytorch/pytorch/issues/37142\n",
        "        # try not to fool the heuristics\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # autoencode\n",
        "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
        "                                            predicted_indices=ind)\n",
        "\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # discriminator\n",
        "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        log_dict = self._validation_step(batch, batch_idx)\n",
        "        with self.ema_scope():\n",
        "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
        "        return log_dict\n",
        "\n",
        "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
        "                                        self.global_step,\n",
        "                                        last_layer=self.get_last_layer(),\n",
        "                                        split=\"val\"+suffix,\n",
        "                                        predicted_indices=ind\n",
        "                                        )\n",
        "\n",
        "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
        "                                            self.global_step,\n",
        "                                            last_layer=self.get_last_layer(),\n",
        "                                            split=\"val\"+suffix,\n",
        "                                            predicted_indices=ind\n",
        "                                            )\n",
        "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
        "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "\n",
        "class VQModelInterface(VQModel):\n",
        "    def __init__(self, embed_dim, *args, **kwargs):\n",
        "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, h, force_not_quantize=False):\n",
        "        # also go through quantization layer\n",
        "        if not force_not_quantize:\n",
        "            quant, emb_loss, info = self.quantize(h)\n",
        "        else:\n",
        "            quant = h\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "\n",
        "class AutoencoderKL(pl.LightningModule):\n",
        "    def __init__(self, ddconfig, lossconfig, embed_dim):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        assert ddconfig[\"double_z\"]\n",
        "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, sample_posterior=True):\n",
        "        posterior = self.encode(input)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        return dec, posterior\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # train encoder+decoder+logvar\n",
        "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # train the discriminator\n",
        "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "\n",
        "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "\n",
        "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XgDIoz8Jm8B6"
      },
      "outputs": [],
      "source": [
        "# @title encoder from CompVis\n",
        "\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/cin-ldm-vq-f8.yaml\n",
        "# ddconfig:\n",
        "#     double_z: false\n",
        "#     z_channels: 4\n",
        "#     resolution: 256\n",
        "#     in_channels: 3\n",
        "#     out_ch: 3\n",
        "#     ch: 128\n",
        "#     ch_mult: 1,2,3,4\n",
        "#     num_res_blocks: 2\n",
        "#     attn_resolutions: 32\n",
        "\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/ffhq-ldm-vq-4.yaml\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/configs/latent-diffusion/celebahq-ldm-vq-4.yaml\n",
        "# embed_dim: 3\n",
        "# n_embed: 8192 = 2^13 ~ 16^3\n",
        "# ddconfig:\n",
        "#   double_z: false\n",
        "#   z_channels: 3\n",
        "#   resolution: 256\n",
        "#   in_channels: 3\n",
        "#   out_ch: 3\n",
        "#   ch: 128\n",
        "#   ch_mult: 1,2,4\n",
        "#   num_res_blocks: 2\n",
        "#   attn_resolutions: []\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch=128, out_ch=3, ch_mult=(1,2,3,4), num_res_blocks=2,\n",
        "                 attn_resolutions=32, dropout=0.0, resamp_with_conv=True, in_channels=3,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult) # 1,1,2,3,4\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level] # 128 * 1,1,2,3,4\n",
        "            block_out = ch*ch_mult[i_level] # 128 * 1,2,3,4\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1: # downsample at all except last\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "encoder:\n",
        "convin k3s1\n",
        "res attn (),\n",
        "norm act convout k3s1\n",
        "\n",
        "self.conv_in = torch.nn.Conv2d(in_channels, self.ch, 3, 1, padding=3//2)\n",
        "\n",
        "res res down res res down\n",
        "res att res\n",
        "\n",
        "self.conv_out = nn.Sequential(\n",
        "    nn.GroupNorm(32, ch), nn.SiLU(), torch.nn.Conv2d(block_in, z_channels, 3, 1, padding=3//2)\n",
        ")\n",
        "\n",
        "me: down res res down res res\n",
        "down res att down res att = lvl lvl\n",
        "\n",
        "decoder:\n",
        "conv_in\n",
        "res att res\n",
        "res res up res res up\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nWb4L-uhmL4u"
      },
      "outputs": [],
      "source": [
        "# @title CompVis model.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L368\n",
        "\n",
        "# pytorch_diffusion + derived encoder decoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.modules.attention import LinearAttention\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
        "    From Fairseq.\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in tensor2tensor, but differs slightly\n",
        "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    # swish\n",
        "    return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def Normalize(in_channels, num_groups=32):\n",
        "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0,1,0,1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if temb_channels > 0:\n",
        "            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        if temb is not None:\n",
        "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class LinAttnBlock(LinearAttention):\n",
        "    \"\"\"to match AttnBlock usage\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = q.reshape(b,c,h*w)\n",
        "        q = q.permute(0,2,1)   # b,hw,c\n",
        "        k = k.reshape(b,c,h*w) # b,c,hw\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b,c,h*w)\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
        "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
        "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
        "    if attn_type == \"vanilla\":\n",
        "        return AttnBlock(in_channels)\n",
        "    elif attn_type == \"none\":\n",
        "        return nn.Identity(in_channels)\n",
        "    else:\n",
        "        return LinAttnBlock(in_channels)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.use_timestep = use_timestep\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            self.temb = nn.Module()\n",
        "            self.temb.dense = nn.ModuleList([\n",
        "                torch.nn.Linear(self.ch,\n",
        "                                self.temb_ch),\n",
        "                torch.nn.Linear(self.temb_ch,\n",
        "                                self.temb_ch),\n",
        "            ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, t=None, context=None):\n",
        "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
        "        if context is not None:\n",
        "            # assume aligned context, cat along channel axis\n",
        "            x = torch.cat((x, context), dim=1)\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            assert t is not None\n",
        "            temb = get_timestep_embedding(t, self.ch)\n",
        "            temb = self.temb.dense[0](temb)\n",
        "            temb = nonlinearity(temb)\n",
        "            temb = self.temb.dense[1](temb)\n",
        "        else:\n",
        "            temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.conv_out.weight\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
        "                 attn_type=\"vanilla\", **ignorekwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "        self.give_pre_end = give_pre_end\n",
        "        self.tanh_out = tanh_out\n",
        "\n",
        "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
        "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
        "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
        "        print(\"Working with z of shape {} = {} dimensions.\".format(self.z_shape, np.prod(self.z_shape)))\n",
        "\n",
        "        # z to block_in\n",
        "        self.conv_in = torch.nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #assert z.shape[1:] == self.z_shape[1:]\n",
        "        self.last_z_shape = z.shape\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # z to block_in\n",
        "        h = self.conv_in(z)\n",
        "\n",
        "        # middle\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](h, temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        if self.give_pre_end:\n",
        "            return h\n",
        "\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        if self.tanh_out:\n",
        "            h = torch.tanh(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
        "                                     ResnetBlock(in_channels=in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=2 * in_channels, out_channels=4 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=4 * in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
        "                                     Upsample(in_channels, with_conv=True)])\n",
        "        # end\n",
        "        self.norm_out = Normalize(in_channels)\n",
        "        self.conv_out = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.model):\n",
        "            if i in [1,2,3]:\n",
        "                x = layer(x, None)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        h = self.norm_out(x)\n",
        "        h = nonlinearity(h)\n",
        "        x = self.conv_out(h)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsampleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
        "                 ch_mult=(2,2), dropout=0.0):\n",
        "        super().__init__()\n",
        "        # upsampling\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        block_in = in_channels\n",
        "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            res_block = []\n",
        "            block_out = ch * ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                res_block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "            self.res_blocks.append(nn.ModuleList(res_block))\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                self.upsample_blocks.append(Upsample(block_in, True))\n",
        "                curr_res = curr_res * 2\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # upsampling\n",
        "        h = x\n",
        "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                h = self.res_blocks[i_level][i_block](h, None)\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                h = self.upsample_blocks[k](h)\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class LatentRescaler(nn.Module):\n",
        "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
        "        super().__init__()\n",
        "        # residual block, interpolate, residual block\n",
        "        self.factor = factor\n",
        "        self.conv_in = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "        self.attn = AttnBlock(mid_channels)\n",
        "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "\n",
        "        self.conv_out = nn.Conv2d(mid_channels, out_channels, kernel_size=1,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "        for block in self.res_block1:\n",
        "            x = block(x, None)\n",
        "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
        "        x = self.attn(x)\n",
        "        for block in self.res_block2:\n",
        "            x = block(x, None)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
        "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        intermediate_chn = ch * ch_mult[-1]\n",
        "        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
        "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
        "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
        "                               out_ch=None)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
        "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.rescaler(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleDecoder(nn.Module):\n",
        "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
        "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        tmp_chn = z_channels*ch_mult[-1]\n",
        "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
        "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
        "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
        "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampler(nn.Module):\n",
        "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
        "        super().__init__()\n",
        "        assert out_size >= in_size\n",
        "        num_blocks = int(np.log2(out_size//in_size))+1\n",
        "        factor_up = 1.+ (out_size % in_size)\n",
        "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
        "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
        "                                       out_channels=in_channels)\n",
        "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
        "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
        "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Resize(nn.Module):\n",
        "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
        "        super().__init__()\n",
        "        self.with_conv = learned\n",
        "        self.mode = mode\n",
        "        if self.with_conv:\n",
        "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
        "            raise NotImplementedError()\n",
        "            assert in_channels is not None\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, scale_factor=1.0):\n",
        "        if scale_factor==1.0:\n",
        "            return x\n",
        "        else:\n",
        "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
        "        return x\n",
        "\n",
        "class FirstStagePostProcessor(nn.Module):\n",
        "    def __init__(self, ch_mult:list, in_channels,\n",
        "                 pretrained_model:nn.Module=None,\n",
        "                 reshape=False,\n",
        "                 n_channels=None,\n",
        "                 dropout=0.,\n",
        "                 pretrained_config=None):\n",
        "        super().__init__()\n",
        "        if pretrained_config is None:\n",
        "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.pretrained_model = pretrained_model\n",
        "        else:\n",
        "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.instantiate_pretrained(pretrained_config)\n",
        "        self.do_reshape = reshape\n",
        "        if n_channels is None:\n",
        "            n_channels = self.pretrained_model.encoder.ch\n",
        "\n",
        "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
        "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3, stride=1,padding=1)\n",
        "        blocks = []\n",
        "        downs = []\n",
        "        ch_in = n_channels\n",
        "        for m in ch_mult:\n",
        "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
        "            ch_in = m * n_channels\n",
        "            downs.append(Downsample(ch_in, with_conv=False))\n",
        "        self.model = nn.ModuleList(blocks)\n",
        "        self.downsampler = nn.ModuleList(downs)\n",
        "\n",
        "\n",
        "    def instantiate_pretrained(self, config):\n",
        "        model = instantiate_from_config(config)\n",
        "        self.pretrained_model = model.eval()\n",
        "        # self.pretrained_model.train = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_with_pretrained(self,x):\n",
        "        c = self.pretrained_model.encode(x)\n",
        "        if isinstance(c, DiagonalGaussianDistribution):\n",
        "            c = c.mode()\n",
        "        return  c\n",
        "\n",
        "    def forward(self,x):\n",
        "        z_fs = self.encode_with_pretrained(x)\n",
        "        z = self.proj_norm(z_fs)\n",
        "        z = self.proj(z)\n",
        "        z = nonlinearity(z)\n",
        "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
        "            z = submodel(z,temb=None)\n",
        "            z = downmodel(z)\n",
        "        if self.do_reshape:\n",
        "            z = rearrange(z,'b c h w -> b (h w) c')\n",
        "        return z\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5bYiW-RnBdAi"
      },
      "outputs": [],
      "source": [
        "# @title quantizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# airalcorn2\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "        self.decay = decay\n",
        "        # Small constant to avoid numerical instability in embedding updates.\n",
        "        self.epsilon = 1e-5\n",
        "\n",
        "        # Dictionary embeddings.\n",
        "        limit = 3 ** 0.5\n",
        "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_( -limit, limit)\n",
        "        if use_ema: self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else: self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        # Exponential moving average of the cluster counts.\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
        "        distances = ((flat_x ** 2).sum(1, keepdim=True) - 2 * flat_x @ self.e_i_ts + (self.e_i_ts ** 2).sum(0, keepdim=True))\n",
        "        encoding_indices = distances.argmin(1)\n",
        "        quantized_x = F.embedding(encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        # See second term of Equation (3).\n",
        "        if not self.use_ema: dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else: dictionary_loss = None\n",
        "\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
        "                # Cluster counts.\n",
        "                encoding_one_hots = F.one_hot(encoding_indices, self.num_embeddings).type(flat_x.dtype)\n",
        "                n_i_ts = encoding_one_hots.sum(0)\n",
        "                # Updated exponential moving average of the cluster counts.\n",
        "                # See Equation (6).\n",
        "                self.N_i_ts(n_i_ts)\n",
        "\n",
        "                # Exponential moving average of the embeddings. See Equation (7).\n",
        "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
        "                self.m_i_ts(embed_sums)\n",
        "\n",
        "                # This is kind of weird.\n",
        "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
        "                # and Equation (8).\n",
        "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
        "                N_i_ts_stable = ((self.N_i_ts.average + self.epsilon) / (N_i_ts_sum + self.num_embeddings * self.epsilon) * N_i_ts_sum)\n",
        "\n",
        "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
        "\n",
        "        return (quantized_x, dictionary_loss, commitment_loss, encoding_indices.view(x.shape[0], -1),)\n",
        "\n",
        "\n",
        "# rosinality\n",
        "class Quantize(nn.Module):\n",
        "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.n_embed = n_embed\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        embed = torch.randn(dim, n_embed)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, input):\n",
        "        flatten = input.reshape(-1, self.dim)\n",
        "        dist = (flatten.pow(2).sum(1, keepdim=True) - 2 * flatten @ self.embed + self.embed.pow(2).sum(0, keepdim=True))\n",
        "        _, embed_ind = (-dist).max(1)\n",
        "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
        "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
        "        quantize = self.embed_code(embed_ind)\n",
        "\n",
        "        if self.training:\n",
        "            embed_onehot_sum = embed_onehot.sum(0)\n",
        "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
        "            dist_fn.all_reduce(embed_onehot_sum)\n",
        "            dist_fn.all_reduce(embed_sum)\n",
        "            self.cluster_size.data.mul_(self.decay).add_(embed_onehot_sum, alpha=1 - self.decay)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = ((self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n)\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "        diff = (quantize.detach() - input).pow(2).mean()\n",
        "        quantize = input + (quantize - input).detach()\n",
        "        return quantize, diff, embed_ind\n",
        "\n",
        "    def embed_code(self, embed_id):\n",
        "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
        "\n",
        "# CompVis\n",
        "from einops import rearrange\n",
        "class VectorQuantizer2(nn.Module): # https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py#L213\n",
        "    def __init__(self, n_e, e_dim, beta, sane_index_shape=False): # sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + torch.sum(self.embedding.weight**2, dim=1) - 2 * torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        return z_q, loss, min_encoding_indices\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "\n",
        "emb_dim, num_emb = 4,20\n",
        "# x = torch.randn(2, 3, 4)\n",
        "# x = torch.randn(2, 3, 4, 4)\n",
        "# vq = VectorQuantizer(emb_dim, num_emb, use_ema, decay)\n",
        "# vq = Quantize(emb_dim, num_emb, decay=0.99, eps=1e-5)\n",
        "vq = VectorQuantizer2(num_emb, emb_dim, beta=0.5, sane_index_shape=False) # CompVis\n",
        "out = vq(x)\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6gGjNCKog8za"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5)\n",
        "        shift = torch.tan(offset / half_l)\n",
        "        return torch.tanh(z + shift) * half_l - offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "print(fsq.codebook)\n",
        "\n",
        "batch_size, seq_len = 1, 1\n",
        "x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "lact = fsq.codes_to_indexes(la)\n",
        "print(lact)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bhCqoI0_fX1B"
      },
      "outputs": [],
      "source": [
        "# @title tried no round, LogitNormalCDF\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def logit(x): return torch.log(x/(1-x)) # x in (0,1)\n",
        "def LogitNormalCDF(x, mu=0, std=.5): # _/- for std<1.8; /-/ for std>1.8\n",
        "    cdf = 1/2 * (1 + torch.erf((logit(x)-mu)/(2**.5*std)))\n",
        "    return cdf\n",
        "\n",
        "# class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "#     def __init__(self, levels):\n",
        "#         super().__init__()\n",
        "#         self.eps = eps\n",
        "#         self.levels = torch.tensor(levels, device=device)\n",
        "#         # level in levels\n",
        "\n",
        "    # linear, normal,\n",
        "    # center = LogitNormalCDF(torch.linspace(0,1,level), mu=0, std=3)\n",
        "    # def forward(self, z):\n",
        "    #     z = F.sigmoid(z)\n",
        "    #     ind = torch.argmin((z-center).abs())\n",
        "\n",
        "\n",
        "    # threshold = LogitNormalCDF(torch.linspace(0,1,level+1), mu=0, std=3)[1:-1]\n",
        "\n",
        "    # def forward(self, z):\n",
        "    #     z = F.sigmoid(z)\n",
        "    #     center[ind]\n",
        "\n",
        "\n",
        "# linear, normal,\n",
        "\n",
        "center = [LogitNormalCDF(torch.linspace(0,1,level), mu=0, std=3) for level in levels]\n",
        "# print(center)\n",
        "# def forward(self, z):\n",
        "z = torch.linspace(-2,2,7).repeat(3,1).T\n",
        "\n",
        "z = F.sigmoid(z)\n",
        "# ind = [torch.argmin((z-c).abs()) for c in center]\n",
        "# print(ind)\n",
        "\n",
        "threshold = [LogitNormalCDF(torch.linspace(0,1,level+1), mu=0, std=1)[1:-1] for level in levels]\n",
        "print(threshold)\n",
        "\n",
        "def get_vjp(v):\n",
        "    return torch.autograd.grad(y, x, v)\n",
        "out = torch.vmap(get_vjp)(I_N)\n",
        "\n",
        "\n",
        "# def forward(self, z):\n",
        "#     z = F.sigmoid(z)\n",
        "#     center[ind]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "axZFsNiThSmT"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains vector_quantize_pytorch.py GroupedResidualVQ\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/vector_quantize_pytorch.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "from math import ceil\n",
        "from functools import partial, cache\n",
        "from itertools import zip_longest\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import Module, ModuleList\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from vector_quantize_pytorch.vector_quantize_pytorch import VectorQuantize\n",
        "\n",
        "from einops import rearrange, repeat, reduce, pack, unpack\n",
        "\n",
        "from einx import get_at\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def first(it):\n",
        "    return it[0]\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def cast_tuple(t, length = 1):\n",
        "    return t if isinstance(t, tuple) else ((t,) * length)\n",
        "\n",
        "def unique(arr):\n",
        "    return list({*arr})\n",
        "\n",
        "def round_up_multiple(num, mult):\n",
        "    return ceil(num / mult) * mult\n",
        "\n",
        "# distributed helpers\n",
        "\n",
        "def is_distributed():\n",
        "    return dist.is_initialized() and dist.get_world_size() > 1\n",
        "\n",
        "def get_maybe_sync_seed(device, max_size = 10_000):\n",
        "    rand_int = torch.randint(0, max_size, (), device = device)\n",
        "\n",
        "    if is_distributed():\n",
        "        dist.all_reduce(rand_int)\n",
        "\n",
        "    return rand_int.item()\n",
        "\n",
        "# the mlp for generating the neural implicit codebook\n",
        "# from Huijben et al. https://arxiv.org/abs/2401.14732\n",
        "\n",
        "class MLP(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_hidden = None,\n",
        "        depth = 4,             # they used 4 layers in the paper\n",
        "        l2norm_output = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim_hidden = default(dim_hidden, dim)\n",
        "\n",
        "        self.proj_in = nn.Linear(2 * dim, dim)\n",
        "\n",
        "        layers = ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            layers.append(nn.Sequential(\n",
        "                nn.Linear(dim, dim_hidden),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(dim_hidden, dim)\n",
        "            ))\n",
        "\n",
        "        self.layers = layers\n",
        "        self.l2norm_output = l2norm_output\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        codes,\n",
        "        *,\n",
        "        condition\n",
        "    ):\n",
        "        one_headed = codes.ndim == 2\n",
        "\n",
        "        if one_headed:\n",
        "            codes = rearrange(codes, 'c d -> 1 c d')\n",
        "\n",
        "        heads, num_codes, batch, seq_len = codes.shape[0], codes.shape[-2], condition.shape[0], condition.shape[-2]\n",
        "\n",
        "        codes = repeat(codes, 'h c d -> h b n c d', n = seq_len, b = batch)\n",
        "        condition = repeat(condition, 'b n d -> h b n c d', c = num_codes, h = heads)\n",
        "\n",
        "        x = torch.cat((condition, codes), dim = -1)\n",
        "        x = self.proj_in(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x) + x\n",
        "\n",
        "        if self.l2norm_output:\n",
        "            x = F.normalize(x, dim = -1)\n",
        "\n",
        "        if not one_headed:\n",
        "            return x\n",
        "\n",
        "        return rearrange(x, '1 ... -> ...')\n",
        "\n",
        "# main class\n",
        "\n",
        "class ResidualVQ(Module):\n",
        "    \"\"\" Follows Algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        num_quantizers: int | None = None,\n",
        "        codebook_size: int | tuple[int, ...],\n",
        "        codebook_dim = None,\n",
        "        shared_codebook = False,\n",
        "        heads = 1,\n",
        "        quantize_dropout = False,\n",
        "        quantize_dropout_cutoff_index = 0,\n",
        "        quantize_dropout_multiple_of = 1,\n",
        "        accept_image_fmap = False,\n",
        "        implicit_neural_codebook = False, # QINCo from https://arxiv.org/abs/2401.14732\n",
        "        mlp_kwargs: dict = dict(),\n",
        "        **vq_kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert heads == 1, 'residual vq is not compatible with multi-headed codes'\n",
        "        assert exists(num_quantizers) or isinstance(codebook_size, tuple)\n",
        "\n",
        "        codebook_dim = default(codebook_dim, dim)\n",
        "        codebook_input_dim = codebook_dim * heads\n",
        "\n",
        "        requires_projection = codebook_input_dim != dim\n",
        "        self.project_in = nn.Linear(dim, codebook_input_dim) if requires_projection else nn.Identity()\n",
        "        self.project_out = nn.Linear(codebook_input_dim, dim) if requires_projection else nn.Identity()\n",
        "        self.has_projections = requires_projection\n",
        "\n",
        "        self.accept_image_fmap = accept_image_fmap\n",
        "\n",
        "        self.implicit_neural_codebook = implicit_neural_codebook\n",
        "\n",
        "        if implicit_neural_codebook:\n",
        "            vq_kwargs.update(\n",
        "                learnable_codebook = True,\n",
        "                ema_update = False\n",
        "            )\n",
        "\n",
        "        if shared_codebook:\n",
        "            vq_kwargs.update(\n",
        "                manual_ema_update = True,\n",
        "                manual_in_place_optimizer_update = True\n",
        "            )\n",
        "\n",
        "        # take care of maybe different codebook sizes across depth\n",
        "\n",
        "        codebook_sizes = cast_tuple(codebook_size, num_quantizers)\n",
        "\n",
        "        num_quantizers = default(num_quantizers, len(codebook_sizes))\n",
        "        assert len(codebook_sizes) == num_quantizers\n",
        "\n",
        "        self.num_quantizers = num_quantizers\n",
        "\n",
        "        self.codebook_sizes = codebook_sizes\n",
        "        self.uniform_codebook_size = len(unique(codebook_sizes)) == 1\n",
        "\n",
        "        # define vq across layers\n",
        "\n",
        "        self.layers = ModuleList([VectorQuantize(dim = codebook_dim, codebook_size = layer_codebook_size, codebook_dim = codebook_dim, accept_image_fmap = accept_image_fmap, **vq_kwargs) for layer_codebook_size in codebook_sizes])\n",
        "\n",
        "        assert all([not vq.has_projections for vq in self.layers])\n",
        "\n",
        "        self.quantize_dropout = quantize_dropout and num_quantizers > 1\n",
        "\n",
        "        assert quantize_dropout_cutoff_index >= 0\n",
        "\n",
        "        self.quantize_dropout_cutoff_index = quantize_dropout_cutoff_index\n",
        "        self.quantize_dropout_multiple_of = quantize_dropout_multiple_of  # encodec paper proposes structured dropout, believe this was set to 4\n",
        "\n",
        "        # setting up the MLPs for implicit neural codebooks\n",
        "\n",
        "        self.mlps = None\n",
        "\n",
        "        if implicit_neural_codebook:\n",
        "            self.mlps = ModuleList([MLP(dim = codebook_dim, l2norm_output = first(self.layers).use_cosine_sim, **mlp_kwargs) for _ in range(num_quantizers - 1)])\n",
        "        else:\n",
        "            self.mlps = (None,) * (num_quantizers - 1)\n",
        "\n",
        "        # sharing codebook logic\n",
        "\n",
        "        self.shared_codebook = shared_codebook\n",
        "\n",
        "        if not shared_codebook:\n",
        "            return\n",
        "\n",
        "        assert self.uniform_codebook_size\n",
        "\n",
        "        first_vq, *rest_vq = self.layers\n",
        "        codebook = first_vq._codebook\n",
        "\n",
        "        for vq in rest_vq:\n",
        "            vq._codebook = codebook\n",
        "\n",
        "    @property\n",
        "    def codebook_size(self):\n",
        "        return self.layers[0].codebook_size\n",
        "\n",
        "    @property\n",
        "    def codebook_dim(self):\n",
        "        return self.layers[0].codebook_dim\n",
        "\n",
        "    @property\n",
        "    def codebooks(self):\n",
        "        codebooks = [layer._codebook.embed for layer in self.layers]\n",
        "\n",
        "        codebooks = tuple(rearrange(codebook, '1 ... -> ...') for codebook in codebooks)\n",
        "\n",
        "        if not self.uniform_codebook_size:\n",
        "            return codebooks\n",
        "\n",
        "        codebooks = torch.stack(codebooks)\n",
        "        return codebooks\n",
        "\n",
        "    def get_codes_from_indices(self, indices):\n",
        "\n",
        "        batch, quantize_dim = indices.shape[0], indices.shape[-1]\n",
        "\n",
        "        # may also receive indices in the shape of 'b h w q' (accept_image_fmap)\n",
        "\n",
        "        indices, ps = pack([indices], 'b * q')\n",
        "\n",
        "        # because of quantize dropout, one can pass in indices that are coarse\n",
        "        # and the network should be able to reconstruct\n",
        "\n",
        "        if quantize_dim < self.num_quantizers:\n",
        "            assert self.quantize_dropout > 0., 'quantize dropout must be greater than 0 if you wish to reconstruct from a signal with less fine quantizations'\n",
        "            indices = F.pad(indices, (0, self.num_quantizers - quantize_dim), value = -1)\n",
        "\n",
        "        # take care of quantizer dropout\n",
        "\n",
        "        mask = indices == -1.\n",
        "        indices = indices.masked_fill(mask, 0) # have it fetch a dummy code to be masked out later\n",
        "\n",
        "        if not self.implicit_neural_codebook and self.uniform_codebook_size:\n",
        "\n",
        "            all_codes = get_at('q [c] d, b n q -> q b n d', self.codebooks, indices)\n",
        "\n",
        "        else:\n",
        "            # else if using implicit neural codebook, or non uniform codebook sizes, codes will need to be derived layer by layer\n",
        "\n",
        "            code_transform_mlps = (None, *self.mlps)\n",
        "\n",
        "            all_codes = []\n",
        "            quantized_out = 0.\n",
        "\n",
        "            for codes, indices, maybe_transform_mlp in zip(self.codebooks, indices.unbind(dim = -1), code_transform_mlps):\n",
        "\n",
        "                if exists(maybe_transform_mlp):\n",
        "                    codes = maybe_transform_mlp(codes, condition = quantized_out)\n",
        "                    layer_codes = get_at('b n [c] d, b n -> b n d', codes, indices)\n",
        "                else:\n",
        "                    layer_codes = get_at('[c] d, b n -> b n d', codes, indices)\n",
        "\n",
        "                all_codes.append(layer_codes)\n",
        "                quantized_out += layer_codes\n",
        "\n",
        "            all_codes = torch.stack(all_codes)\n",
        "\n",
        "        # mask out any codes that were dropout-ed\n",
        "\n",
        "        all_codes = all_codes.masked_fill(rearrange(mask, 'b n q -> q b n 1'), 0.)\n",
        "\n",
        "        # if (accept_image_fmap = True) then return shape (quantize, batch, height, width, dimension)\n",
        "\n",
        "        all_codes, = unpack(all_codes, ps, 'q b * d')\n",
        "\n",
        "        return all_codes\n",
        "\n",
        "    def get_output_from_indices(self, indices):\n",
        "        codes = self.get_codes_from_indices(indices)\n",
        "        codes_summed = reduce(codes, 'q ... -> ...', 'sum')\n",
        "        return self.project_out(codes_summed)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        mask = None,\n",
        "        indices: Tensor | list[Tensor] | None = None,\n",
        "        return_all_codes = False,\n",
        "        sample_codebook_temp = None,\n",
        "        freeze_codebook = False,\n",
        "        rand_quantize_dropout_fixed_seed = None\n",
        "    ):\n",
        "        num_quant, quant_dropout_multiple_of, return_loss, device = self.num_quantizers, self.quantize_dropout_multiple_of, exists(indices), x.device\n",
        "\n",
        "        x = self.project_in(x)\n",
        "\n",
        "        assert not (self.accept_image_fmap and exists(indices))\n",
        "\n",
        "        quantized_out = 0.\n",
        "        residual = x\n",
        "\n",
        "        all_losses = []\n",
        "        all_indices = []\n",
        "\n",
        "        if isinstance(indices, list):\n",
        "            indices = torch.stack(indices)\n",
        "\n",
        "        if return_loss:\n",
        "            assert not torch.any(indices == -1), 'some of the residual vq indices were dropped out. please use indices derived when the module is in eval mode to derive cross entropy loss'\n",
        "            ce_losses = []\n",
        "\n",
        "        should_quantize_dropout = self.training and self.quantize_dropout and not return_loss\n",
        "\n",
        "        # sample a layer index at which to dropout further residual quantization\n",
        "        # also prepare null indices and loss\n",
        "\n",
        "        if should_quantize_dropout:\n",
        "\n",
        "            # check if seed is manually passed in\n",
        "\n",
        "            if not exists(rand_quantize_dropout_fixed_seed):\n",
        "                rand_quantize_dropout_fixed_seed = get_maybe_sync_seed(device)\n",
        "\n",
        "            rand = random.Random(rand_quantize_dropout_fixed_seed)\n",
        "\n",
        "            rand_quantize_dropout_index = rand.randrange(self.quantize_dropout_cutoff_index, num_quant)\n",
        "\n",
        "            if quant_dropout_multiple_of != 1:\n",
        "                rand_quantize_dropout_index = round_up_multiple(rand_quantize_dropout_index + 1, quant_dropout_multiple_of) - 1\n",
        "\n",
        "            null_indices_shape = (x.shape[0], *x.shape[-2:]) if self.accept_image_fmap else tuple(x.shape[:2])\n",
        "            null_indices = torch.full(null_indices_shape, -1., device = device, dtype = torch.long)\n",
        "            null_loss = torch.full((1,), 0., device = device, dtype = x.dtype)\n",
        "\n",
        "        # setup the mlps for implicit neural codebook\n",
        "\n",
        "        maybe_code_transforms = (None,) * len(self.layers)\n",
        "\n",
        "        if self.implicit_neural_codebook:\n",
        "            maybe_code_transforms = (None, *self.mlps)\n",
        "\n",
        "        # save all inputs across layers, for use during expiration at end under shared codebook setting\n",
        "\n",
        "        all_residuals = []\n",
        "\n",
        "        # go through the layers\n",
        "\n",
        "        for quantizer_index, (vq, maybe_mlp) in enumerate(zip(self.layers, maybe_code_transforms)):\n",
        "\n",
        "            if should_quantize_dropout and quantizer_index > rand_quantize_dropout_index:\n",
        "                all_indices.append(null_indices)\n",
        "                all_losses.append(null_loss)\n",
        "                continue\n",
        "\n",
        "            layer_indices = None\n",
        "            if return_loss:\n",
        "                layer_indices = indices[..., quantizer_index]\n",
        "\n",
        "            # setup the transform code function to be passed into VectorQuantize forward\n",
        "\n",
        "            if exists(maybe_mlp):\n",
        "                maybe_mlp = partial(maybe_mlp, condition = quantized_out)\n",
        "\n",
        "            # save for expiration\n",
        "\n",
        "            all_residuals.append(residual)\n",
        "\n",
        "            # vector quantize forward\n",
        "\n",
        "            quantized, *rest = vq(\n",
        "                residual,\n",
        "                mask = mask,\n",
        "                indices = layer_indices,\n",
        "                sample_codebook_temp = sample_codebook_temp,\n",
        "                freeze_codebook = freeze_codebook,\n",
        "                codebook_transform_fn = maybe_mlp\n",
        "            )\n",
        "\n",
        "            residual = residual - quantized.detach()\n",
        "            quantized_out = quantized_out + quantized\n",
        "\n",
        "            if return_loss:\n",
        "                ce_loss = rest[0]\n",
        "                ce_losses.append(ce_loss)\n",
        "                continue\n",
        "\n",
        "            embed_indices, loss = rest\n",
        "\n",
        "            all_indices.append(embed_indices)\n",
        "            all_losses.append(loss)\n",
        "\n",
        "        # if shared codebook, update ema only at end\n",
        "\n",
        "        if self.training and self.shared_codebook:\n",
        "            shared_layer = first(self.layers)\n",
        "            shared_layer._codebook.update_ema()\n",
        "            shared_layer.update_in_place_optimizer()\n",
        "            shared_layer.expire_codes_(torch.cat(all_residuals, dim = -2))\n",
        "\n",
        "        # project out, if needed\n",
        "\n",
        "        quantized_out = self.project_out(quantized_out)\n",
        "\n",
        "        # whether to early return the cross entropy loss\n",
        "\n",
        "        if return_loss:\n",
        "            return quantized_out, sum(ce_losses)\n",
        "\n",
        "        # stack all losses and indices\n",
        "\n",
        "        all_losses, all_indices = map(partial(torch.stack, dim = -1), (all_losses, all_indices))\n",
        "\n",
        "        ret = (quantized_out, all_indices, all_losses)\n",
        "\n",
        "        if return_all_codes:\n",
        "            # whether to return all codes from all codebooks across layers\n",
        "            all_codes = self.get_codes_from_indices(all_indices)\n",
        "\n",
        "            # will return all codes in shape (quantizer, batch, sequence length, codebook dimension)\n",
        "            ret = (*ret, all_codes)\n",
        "\n",
        "        return ret\n",
        "\n",
        "# grouped residual vq\n",
        "\n",
        "class GroupedResidualVQ(Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        groups = 1,\n",
        "        accept_image_fmap = False,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.groups = groups\n",
        "        assert (dim % groups) == 0\n",
        "        dim_per_group = dim // groups\n",
        "\n",
        "        self.accept_image_fmap = accept_image_fmap\n",
        "\n",
        "        self.rvqs = ModuleList([])\n",
        "\n",
        "        for _ in range(groups):\n",
        "            self.rvqs.append(ResidualVQ(\n",
        "                dim = dim_per_group,\n",
        "                accept_image_fmap = accept_image_fmap,\n",
        "                **kwargs\n",
        "            ))\n",
        "\n",
        "    @property\n",
        "    def codebooks(self):\n",
        "        return torch.stack(tuple(rvq.codebooks for rvq in self.rvqs))\n",
        "\n",
        "    @property\n",
        "    def split_dim(self):\n",
        "        return 1 if self.accept_image_fmap else -1\n",
        "\n",
        "    def get_codes_from_indices(self, indices):\n",
        "        codes = tuple(rvq.get_codes_from_indices(chunk_indices) for rvq, chunk_indices in zip(self.rvqs, indices))\n",
        "        return torch.stack(codes)\n",
        "\n",
        "    def get_output_from_indices(self, indices):\n",
        "        outputs = tuple(rvq.get_output_from_indices(chunk_indices) for rvq, chunk_indices in zip(self.rvqs, indices))\n",
        "        return torch.cat(outputs, dim = self.split_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        indices = None,\n",
        "        return_all_codes = False,\n",
        "        sample_codebook_temp = None,\n",
        "        freeze_codebook = False,\n",
        "        mask = None,\n",
        "    ):\n",
        "        shape, split_dim, device = x.shape, self.split_dim, x.device\n",
        "        assert shape[split_dim] == self.dim\n",
        "\n",
        "        # split the feature dimension into groups\n",
        "\n",
        "        x = x.chunk(self.groups, dim = split_dim)\n",
        "\n",
        "        indices = default(indices, tuple())\n",
        "        return_ce_loss = len(indices) > 0\n",
        "        assert len(indices) == 0 or len(indices) == self.groups\n",
        "\n",
        "        forward_kwargs = dict(\n",
        "            return_all_codes = return_all_codes,\n",
        "            sample_codebook_temp = sample_codebook_temp,\n",
        "            mask = mask,\n",
        "            freeze_codebook = freeze_codebook,\n",
        "            rand_quantize_dropout_fixed_seed = get_maybe_sync_seed(device) if self.training else None\n",
        "        )\n",
        "\n",
        "        # invoke residual vq on each group\n",
        "\n",
        "        out = tuple(rvq(chunk, indices = chunk_indices, **forward_kwargs) for rvq, chunk, chunk_indices in zip_longest(self.rvqs, x, indices))\n",
        "        out = tuple(zip(*out))\n",
        "\n",
        "        # if returning cross entropy loss to rvq codebooks\n",
        "\n",
        "        if return_ce_loss:\n",
        "            quantized, ce_losses = out\n",
        "            return torch.cat(quantized, dim = split_dim), sum(ce_losses)\n",
        "\n",
        "        # otherwise, get all the zipped outputs and combine them\n",
        "\n",
        "        quantized, all_indices, commit_losses, *maybe_all_codes = out\n",
        "\n",
        "        quantized = torch.cat(quantized, dim = split_dim)\n",
        "        all_indices = torch.stack(all_indices)\n",
        "        commit_losses = torch.stack(commit_losses)\n",
        "\n",
        "        ret = (quantized, all_indices, commit_losses, *maybe_all_codes)\n",
        "        return ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dr2bajYVyjbN"
      },
      "outputs": [],
      "source": [
        "# @title CompVis vqvae\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VectorQuantizer2(nn.Module): # https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py#L213\n",
        "    def __init__(self, n_e, e_dim, beta, sane_index_shape=False): # sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        return z_q, loss, min_encoding_indices\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "\n",
        "\n",
        "# class VQModel(pl.LightningModule):\n",
        "class VQModel(nn.Module):\n",
        "    def __init__(self, n_embed, embed_dim,):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        z_channels = 4 # 4?\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25)\n",
        "        self.quant_conv = torch.nn.Conv2d(z_channels, embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, z_channels, 1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cS-HV67C8S_m"
      },
      "outputs": [],
      "source": [
        "# @title airalcorn2 vqvae\n",
        "# https://github.com/airalcorn2/vqvae-pytorch/blob/master/vqvae.py\n",
        "# Ported from: https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb.\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        layers = []\n",
        "        for i in range(num_residual_layers):\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_hiddens,\n",
        "                        out_channels=num_residual_hiddens,\n",
        "                        kernel_size=3,\n",
        "                        padding=1,\n",
        "                    ),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_residual_hiddens,\n",
        "                        out_channels=num_hiddens,\n",
        "                        kernel_size=1,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        for layer in self.layers:\n",
        "            h = h + layer(h)\n",
        "\n",
        "        # ResNet V1-style.\n",
        "        return torch.relu(h)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        # The last ReLU from the Sonnet example is omitted because ResidualStack starts\n",
        "        # off with a ReLU.\n",
        "        conv = nn.Sequential()\n",
        "        for downsampling_layer in range(num_downsampling_layers):\n",
        "            if downsampling_layer == 0:\n",
        "                out_channels = num_hiddens // 2\n",
        "            elif downsampling_layer == 1:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, num_hiddens)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            conv.add_module(\n",
        "                f\"down{downsampling_layer}\",\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            conv.add_module(f\"relu{downsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        conv.add_module(\n",
        "            \"final_conv\",\n",
        "            nn.Conv2d(\n",
        "                in_channels=num_hiddens,\n",
        "                out_channels=num_hiddens,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "        )\n",
        "        self.conv = conv\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        return self.residual_stack(h)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        num_hiddens,\n",
        "        num_upsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=embedding_dim,\n",
        "            out_channels=num_hiddens,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self.residual_stack = ResidualStack(\n",
        "            num_hiddens, num_residual_layers, num_residual_hiddens\n",
        "        )\n",
        "        upconv = nn.Sequential()\n",
        "        for upsampling_layer in range(num_upsampling_layers):\n",
        "            if upsampling_layer < num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            elif upsampling_layer == num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens // 2)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, 3)\n",
        "\n",
        "            upconv.add_module(\n",
        "                f\"up{upsampling_layer}\",\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    kernel_size=4,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                ),\n",
        "            )\n",
        "            if upsampling_layer < num_upsampling_layers - 1:\n",
        "                upconv.add_module(f\"relu{upsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        self.upconv = upconv\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h = self.residual_stack(h)\n",
        "        x_recon = self.upconv(h)\n",
        "        return x_recon\n",
        "\n",
        "\n",
        "class SonnetExponentialMovingAverage(nn.Module):\n",
        "    # See: https://github.com/deepmind/sonnet/blob/5cbfdc356962d9b6198d5b63f0826a80acfdf35b/sonnet/src/moving_averages.py#L25.\n",
        "    # They do *not* use the exponential moving average updates described in Appendix A.1\n",
        "    # of \"Neural Discrete Representation Learning\".\n",
        "    def __init__(self, decay, shape):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.counter = 0\n",
        "        self.register_buffer(\"hidden\", torch.zeros(*shape))\n",
        "        self.register_buffer(\"average\", torch.zeros(*shape))\n",
        "\n",
        "    def update(self, value):\n",
        "        self.counter += 1\n",
        "        with torch.no_grad():\n",
        "            self.hidden -= (self.hidden - value) * (1 - self.decay)\n",
        "            self.average = self.hidden / (1 - self.decay ** self.counter)\n",
        "\n",
        "    def __call__(self, value):\n",
        "        self.update(value)\n",
        "        return self.average\n",
        "\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, use_ema, decay, epsilon):\n",
        "        super().__init__()\n",
        "        # See Section 3 of \"Neural Discrete Representation Learning\" and:\n",
        "        # https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L142.\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.use_ema = use_ema\n",
        "        # Weight for the exponential moving average.\n",
        "        self.decay = decay\n",
        "        # Small constant to avoid numerical instability in embedding updates.\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Dictionary embeddings.\n",
        "        limit = 3 ** 0.5\n",
        "        e_i_ts = torch.FloatTensor(embedding_dim, num_embeddings).uniform_( -limit, limit)\n",
        "        if use_ema: self.register_buffer(\"e_i_ts\", e_i_ts)\n",
        "        else: self.register_parameter(\"e_i_ts\", nn.Parameter(e_i_ts))\n",
        "\n",
        "        # Exponential moving average of the cluster counts.\n",
        "        self.N_i_ts = SonnetExponentialMovingAverage(decay, (num_embeddings,))\n",
        "        # Exponential moving average of the embeddings.\n",
        "        self.m_i_ts = SonnetExponentialMovingAverage(decay, e_i_ts.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flat_x = x.permute(0, 2, 3, 1).reshape(-1, self.embedding_dim)\n",
        "        distances = ((flat_x ** 2).sum(1, keepdim=True) - 2 * flat_x @ self.e_i_ts + (self.e_i_ts ** 2).sum(0, keepdim=True))\n",
        "        encoding_indices = distances.argmin(1)\n",
        "        quantized_x = F.embedding(encoding_indices.view(x.shape[0], *x.shape[2:]), self.e_i_ts.transpose(0, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        # See second term of Equation (3).\n",
        "        if not self.use_ema:\n",
        "            dictionary_loss = ((x.detach() - quantized_x) ** 2).mean()\n",
        "        else:\n",
        "            dictionary_loss = None\n",
        "\n",
        "        # See third term of Equation (3).\n",
        "        commitment_loss = ((x - quantized_x.detach()) ** 2).mean()\n",
        "        # Straight-through gradient. See Section 3.2.\n",
        "        quantized_x = x + (quantized_x - x).detach()\n",
        "\n",
        "        if self.use_ema and self.training:\n",
        "            with torch.no_grad():\n",
        "                # See Appendix A.1 of \"Neural Discrete Representation Learning\".\n",
        "                # Cluster counts.\n",
        "                encoding_one_hots = F.one_hot(encoding_indices, self.num_embeddings).type(flat_x.dtype)\n",
        "                n_i_ts = encoding_one_hots.sum(0)\n",
        "                # Updated exponential moving average of the cluster counts.\n",
        "                # See Equation (6).\n",
        "                self.N_i_ts(n_i_ts)\n",
        "\n",
        "                # Exponential moving average of the embeddings. See Equation (7).\n",
        "                embed_sums = flat_x.transpose(0, 1) @ encoding_one_hots\n",
        "                self.m_i_ts(embed_sums)\n",
        "\n",
        "                # This is kind of weird.\n",
        "                # Compare: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py#L270\n",
        "                # and Equation (8).\n",
        "                N_i_ts_sum = self.N_i_ts.average.sum()\n",
        "                N_i_ts_stable = ((self.N_i_ts.average + self.epsilon) / (N_i_ts_sum + self.num_embeddings * self.epsilon) * N_i_ts_sum)\n",
        "                self.e_i_ts = self.m_i_ts.average / N_i_ts_stable.unsqueeze(0)\n",
        "        return (quantized_x, dictionary_loss, commitment_loss, encoding_indices.view(x.shape[0], -1),)\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "        embedding_dim,\n",
        "        num_embeddings,\n",
        "        use_ema,\n",
        "        decay,\n",
        "        epsilon,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            in_channels,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens,\n",
        "        )\n",
        "        self.pre_vq_conv = nn.Conv2d(\n",
        "            in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1\n",
        "        )\n",
        "        self.vq = VectorQuantizer(\n",
        "            embedding_dim, num_embeddings, use_ema, decay, epsilon\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            embedding_dim,\n",
        "            num_hiddens,\n",
        "            num_downsampling_layers,\n",
        "            num_residual_layers,\n",
        "            num_residual_hiddens,\n",
        "        )\n",
        "\n",
        "    def quantize(self, x):\n",
        "        z = self.pre_vq_conv(self.encoder(x))\n",
        "        (z_quantized, dictionary_loss, commitment_loss, encoding_indices) = self.vq(z)\n",
        "        return (z_quantized, dictionary_loss, commitment_loss, encoding_indices)\n",
        "\n",
        "    def forward(self, x):\n",
        "        (z_quantized, dictionary_loss, commitment_loss, _) = self.quantize(x)\n",
        "        x_recon = self.decoder(z_quantized)\n",
        "        return {\n",
        "            \"dictionary_loss\": dictionary_loss,\n",
        "            \"commitment_loss\": commitment_loss,\n",
        "            \"x_recon\": x_recon,\n",
        "        }\n",
        "\n",
        "# https://github.com/airalcorn2/vqvae-pytorch/blob/master/train_vqvae.py\n",
        "        # out = model(imgs)\n",
        "        # recon_error = criterion(out[\"x_recon\"], imgs) / train_data_variance\n",
        "        # total_recon_error += recon_error.item()\n",
        "        # loss = recon_error + beta * out[\"commitment_loss\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "15v0h5A74X2Z"
      },
      "outputs": [],
      "source": [
        "# @title rosinality from sonet\n",
        "# https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import distributed as dist_fn\n",
        "\n",
        "\n",
        "# Copyright 2018 The Sonnet Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Borrowed from https://github.com/deepmind/sonnet and ported it to PyTorch\n",
        "\n",
        "\n",
        "class Quantize(nn.Module):\n",
        "    def __init__(self, dim, n_embed, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.n_embed = n_embed\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        embed = torch.randn(dim, n_embed)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(n_embed))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, input):\n",
        "        flatten = input.reshape(-1, self.dim)\n",
        "        dist = (flatten.pow(2).sum(1, keepdim=True) - 2 * flatten @ self.embed + self.embed.pow(2).sum(0, keepdim=True))\n",
        "        _, embed_ind = (-dist).max(1)\n",
        "        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(flatten.dtype)\n",
        "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
        "        quantize = self.embed_code(embed_ind)\n",
        "\n",
        "        if self.training:\n",
        "            embed_onehot_sum = embed_onehot.sum(0)\n",
        "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
        "            dist_fn.all_reduce(embed_onehot_sum)\n",
        "            dist_fn.all_reduce(embed_sum)\n",
        "            self.cluster_size.data.mul_(self.decay).add_(embed_onehot_sum, alpha=1 - self.decay)\n",
        "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = ((self.cluster_size + self.eps) / (n + self.n_embed * self.eps) * n)\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "        diff = (quantize.detach() - input).pow(2).mean()\n",
        "        quantize = input + (quantize - input).detach()\n",
        "        return quantize, diff, embed_ind\n",
        "\n",
        "    def embed_code(self, embed_id):\n",
        "        return F.embedding(embed_id, self.embed.transpose(0, 1))\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channel, channel, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel, in_channel, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv(input)\n",
        "        out += input\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channel, channel, n_res_block, n_res_channel, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        if stride == 4:\n",
        "            blocks = [\n",
        "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 2, channel, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel, channel, 3, padding=1),\n",
        "            ]\n",
        "\n",
        "        elif stride == 2:\n",
        "            blocks = [\n",
        "                nn.Conv2d(in_channel, channel // 2, 4, stride=2, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(channel // 2, channel, 3, padding=1),\n",
        "            ]\n",
        "\n",
        "        for i in range(n_res_block):\n",
        "            blocks.append(ResBlock(channel, n_res_channel))\n",
        "\n",
        "        blocks.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.blocks(input)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channel, out_channel, channel, n_res_block, n_res_channel, stride\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        blocks = [nn.Conv2d(in_channel, channel, 3, padding=1)]\n",
        "\n",
        "        for i in range(n_res_block):\n",
        "            blocks.append(ResBlock(channel, n_res_channel))\n",
        "\n",
        "        blocks.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        if stride == 4:\n",
        "            blocks.extend(\n",
        "                [\n",
        "                    nn.ConvTranspose2d(channel, channel // 2, 4, stride=2, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.ConvTranspose2d(\n",
        "                        channel // 2, out_channel, 4, stride=2, padding=1\n",
        "                    ),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        elif stride == 2:\n",
        "            blocks.append(\n",
        "                nn.ConvTranspose2d(channel, out_channel, 4, stride=2, padding=1)\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.blocks(input)\n",
        "\n",
        "\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel=3,\n",
        "        channel=128,\n",
        "        n_res_block=2,\n",
        "        n_res_channel=32,\n",
        "        embed_dim=64,\n",
        "        n_embed=512,\n",
        "        decay=0.99,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc_b = Encoder(in_channel, channel, n_res_block, n_res_channel, stride=4)\n",
        "        self.enc_t = Encoder(channel, channel, n_res_block, n_res_channel, stride=2)\n",
        "        self.quantize_conv_t = nn.Conv2d(channel, embed_dim, 1)\n",
        "        self.quantize_t = Quantize(embed_dim, n_embed)\n",
        "        self.dec_t = Decoder(\n",
        "            embed_dim, embed_dim, channel, n_res_block, n_res_channel, stride=2\n",
        "        )\n",
        "        self.quantize_conv_b = nn.Conv2d(embed_dim + channel, embed_dim, 1)\n",
        "        self.quantize_b = Quantize(embed_dim, n_embed)\n",
        "        self.upsample_t = nn.ConvTranspose2d(\n",
        "            embed_dim, embed_dim, 4, stride=2, padding=1\n",
        "        )\n",
        "        self.dec = Decoder(\n",
        "            embed_dim + embed_dim,\n",
        "            in_channel,\n",
        "            channel,\n",
        "            n_res_block,\n",
        "            n_res_channel,\n",
        "            stride=4,\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        quant_t, quant_b, diff, _, _ = self.encode(input)\n",
        "        dec = self.decode(quant_t, quant_b)\n",
        "\n",
        "        return dec, diff\n",
        "\n",
        "    def encode(self, input):\n",
        "        enc_b = self.enc_b(input)\n",
        "        enc_t = self.enc_t(enc_b)\n",
        "\n",
        "        quant_t = self.quantize_conv_t(enc_t).permute(0, 2, 3, 1)\n",
        "        quant_t, diff_t, id_t = self.quantize_t(quant_t)\n",
        "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
        "        diff_t = diff_t.unsqueeze(0)\n",
        "\n",
        "        dec_t = self.dec_t(quant_t)\n",
        "        enc_b = torch.cat([dec_t, enc_b], 1)\n",
        "\n",
        "        quant_b = self.quantize_conv_b(enc_b).permute(0, 2, 3, 1)\n",
        "        quant_b, diff_b, id_b = self.quantize_b(quant_b)\n",
        "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
        "        diff_b = diff_b.unsqueeze(0)\n",
        "\n",
        "        return quant_t, quant_b, diff_t + diff_b, id_t, id_b\n",
        "\n",
        "    def decode(self, quant_t, quant_b):\n",
        "        upsample_t = self.upsample_t(quant_t)\n",
        "        quant = torch.cat([upsample_t, quant_b], 1)\n",
        "        dec = self.dec(quant)\n",
        "\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_t, code_b):\n",
        "        quant_t = self.quantize_t.embed_code(code_t)\n",
        "        quant_t = quant_t.permute(0, 3, 1, 2)\n",
        "        quant_b = self.quantize_b.embed_code(code_b)\n",
        "        quant_b = quant_b.permute(0, 3, 1, 2)\n",
        "\n",
        "        dec = self.decode(quant_t, quant_b)\n",
        "\n",
        "        return dec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GRVHtJAs0bXp"
      },
      "outputs": [],
      "source": [
        "# @title CompVis stable-diffusion model.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/model.py#L368\n",
        "\n",
        "# pytorch_diffusion + derived encoder decoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.modules.attention import LinearAttention\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
        "    From Fairseq.\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in tensor2tensor, but differs slightly\n",
        "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    # swish\n",
        "    return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def Normalize(in_channels, num_groups=32):\n",
        "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0,1,0,1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if temb_channels > 0:\n",
        "            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        if temb is not None:\n",
        "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class LinAttnBlock(LinearAttention):\n",
        "    \"\"\"to match AttnBlock usage\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = q.reshape(b,c,h*w)\n",
        "        q = q.permute(0,2,1)   # b,hw,c\n",
        "        k = k.reshape(b,c,h*w) # b,c,hw\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b,c,h*w)\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
        "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
        "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
        "    if attn_type == \"vanilla\":\n",
        "        return AttnBlock(in_channels)\n",
        "    elif attn_type == \"none\":\n",
        "        return nn.Identity(in_channels)\n",
        "    else:\n",
        "        return LinAttnBlock(in_channels)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.use_timestep = use_timestep\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            self.temb = nn.Module()\n",
        "            self.temb.dense = nn.ModuleList([\n",
        "                torch.nn.Linear(self.ch,\n",
        "                                self.temb_ch),\n",
        "                torch.nn.Linear(self.temb_ch,\n",
        "                                self.temb_ch),\n",
        "            ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, t=None, context=None):\n",
        "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
        "        if context is not None:\n",
        "            # assume aligned context, cat along channel axis\n",
        "            x = torch.cat((x, context), dim=1)\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            assert t is not None\n",
        "            temb = get_timestep_embedding(t, self.ch)\n",
        "            temb = self.temb.dense[0](temb)\n",
        "            temb = nonlinearity(temb)\n",
        "            temb = self.temb.dense[1](temb)\n",
        "        else:\n",
        "            temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.conv_out.weight\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, 2*z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
        "                 attn_type=\"vanilla\", **ignorekwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "        self.give_pre_end = give_pre_end\n",
        "        self.tanh_out = tanh_out\n",
        "\n",
        "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
        "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
        "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
        "        print(\"Working with z of shape {} = {} dimensions.\".format(self.z_shape, np.prod(self.z_shape)))\n",
        "\n",
        "        # z to block_in\n",
        "        self.conv_in = torch.nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #assert z.shape[1:] == self.z_shape[1:]\n",
        "        self.last_z_shape = z.shape\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # z to block_in\n",
        "        h = self.conv_in(z)\n",
        "\n",
        "        # middle\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](h, temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        if self.give_pre_end:\n",
        "            return h\n",
        "\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        if self.tanh_out:\n",
        "            h = torch.tanh(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
        "                                     ResnetBlock(in_channels=in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=2 * in_channels, out_channels=4 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=4 * in_channels, out_channels=2 * in_channels, temb_channels=0, dropout=0.0),\n",
        "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
        "                                     Upsample(in_channels, with_conv=True)])\n",
        "        # end\n",
        "        self.norm_out = Normalize(in_channels)\n",
        "        self.conv_out = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.model):\n",
        "            if i in [1,2,3]:\n",
        "                x = layer(x, None)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        h = self.norm_out(x)\n",
        "        h = nonlinearity(h)\n",
        "        x = self.conv_out(h)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsampleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
        "                 ch_mult=(2,2), dropout=0.0):\n",
        "        super().__init__()\n",
        "        # upsampling\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        block_in = in_channels\n",
        "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            res_block = []\n",
        "            block_out = ch * ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                res_block.append(ResnetBlock(in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                block_in = block_out\n",
        "            self.res_blocks.append(nn.ModuleList(res_block))\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                self.upsample_blocks.append(Upsample(block_in, True))\n",
        "                curr_res = curr_res * 2\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # upsampling\n",
        "        h = x\n",
        "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                h = self.res_blocks[i_level][i_block](h, None)\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                h = self.upsample_blocks[k](h)\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class LatentRescaler(nn.Module):\n",
        "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
        "        super().__init__()\n",
        "        # residual block, interpolate, residual block\n",
        "        self.factor = factor\n",
        "        self.conv_in = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "        self.attn = AttnBlock(mid_channels)\n",
        "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels, out_channels=mid_channels, temb_channels=0, dropout=0.0) for _ in range(depth)])\n",
        "\n",
        "        self.conv_out = nn.Conv2d(mid_channels, out_channels, kernel_size=1,)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "        for block in self.res_block1:\n",
        "            x = block(x, None)\n",
        "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
        "        x = self.attn(x)\n",
        "        for block in self.res_block2:\n",
        "            x = block(x, None)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
        "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        intermediate_chn = ch * ch_mult[-1]\n",
        "        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
        "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
        "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
        "                               out_ch=None)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
        "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.rescaler(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleDecoder(nn.Module):\n",
        "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
        "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        tmp_chn = z_channels*ch_mult[-1]\n",
        "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
        "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
        "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
        "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampler(nn.Module):\n",
        "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
        "        super().__init__()\n",
        "        assert out_size >= in_size\n",
        "        num_blocks = int(np.log2(out_size//in_size))+1\n",
        "        factor_up = 1.+ (out_size % in_size)\n",
        "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
        "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
        "                                       out_channels=in_channels)\n",
        "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
        "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
        "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Resize(nn.Module):\n",
        "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
        "        super().__init__()\n",
        "        self.with_conv = learned\n",
        "        self.mode = mode\n",
        "        if self.with_conv:\n",
        "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
        "            raise NotImplementedError()\n",
        "            assert in_channels is not None\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, scale_factor=1.0):\n",
        "        if scale_factor==1.0:\n",
        "            return x\n",
        "        else:\n",
        "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
        "        return x\n",
        "\n",
        "class FirstStagePostProcessor(nn.Module):\n",
        "    def __init__(self, ch_mult:list, in_channels,\n",
        "                 pretrained_model:nn.Module=None,\n",
        "                 reshape=False,\n",
        "                 n_channels=None,\n",
        "                 dropout=0.,\n",
        "                 pretrained_config=None):\n",
        "        super().__init__()\n",
        "        if pretrained_config is None:\n",
        "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.pretrained_model = pretrained_model\n",
        "        else:\n",
        "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.instantiate_pretrained(pretrained_config)\n",
        "        self.do_reshape = reshape\n",
        "        if n_channels is None:\n",
        "            n_channels = self.pretrained_model.encoder.ch\n",
        "\n",
        "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
        "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3, stride=1,padding=1)\n",
        "        blocks = []\n",
        "        downs = []\n",
        "        ch_in = n_channels\n",
        "        for m in ch_mult:\n",
        "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
        "            ch_in = m * n_channels\n",
        "            downs.append(Downsample(ch_in, with_conv=False))\n",
        "        self.model = nn.ModuleList(blocks)\n",
        "        self.downsampler = nn.ModuleList(downs)\n",
        "\n",
        "\n",
        "    def instantiate_pretrained(self, config):\n",
        "        model = instantiate_from_config(config)\n",
        "        self.pretrained_model = model.eval()\n",
        "        # self.pretrained_model.train = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_with_pretrained(self,x):\n",
        "        c = self.pretrained_model.encode(x)\n",
        "        if isinstance(c, DiagonalGaussianDistribution):\n",
        "            c = c.mode()\n",
        "        return  c\n",
        "\n",
        "    def forward(self,x):\n",
        "        z_fs = self.encode_with_pretrained(x)\n",
        "        z = self.proj_norm(z_fs)\n",
        "        z = self.proj(z)\n",
        "        z = nonlinearity(z)\n",
        "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
        "            z = submodel(z,temb=None)\n",
        "            z = downmodel(z)\n",
        "        if self.do_reshape:\n",
        "            z = rearrange(z,'b c h w -> b (h w) c')\n",
        "        return z\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BwCLJCY_dFD6"
      },
      "outputs": [],
      "source": [
        "# @title CompVis taming-transformers vqvae quantize.py\n",
        "# https://github.com/CompVis/taming-transformers/blob/master/taming/modules/vqvae/quantize.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import einsum\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    \"\"\"\n",
        "    see https://github.com/MishaLaskin/vqvae/blob/d761a999e2267766400dc646d82d3ac3657771d4/models/quantizer.py\n",
        "    ____________________________________________\n",
        "    Discretization bottleneck part of the VQ-VAE.\n",
        "    Inputs:\n",
        "    - n_e : number of embeddings\n",
        "    - e_dim : dimension of embedding\n",
        "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
        "    _____________________________________________\n",
        "    \"\"\"\n",
        "\n",
        "    # NOTE: this class contains a bug regarding beta; see VectorQuantizer2 for\n",
        "    # a fix and use legacy=False to apply that fix. VectorQuantizer2 can be\n",
        "    # used wherever VectorQuantizer has been used before and is additionally\n",
        "    # more efficient.\n",
        "    def __init__(self, n_e, e_dim, beta):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Inputs the output of the encoder network z and maps it to a discrete\n",
        "        one-hot vector that is the index of the closest embedding vector e_j\n",
        "        z (continuous) -> z_q (discrete)\n",
        "        z.shape = (batch, channel, height, width)\n",
        "        quantization pipeline:\n",
        "            1. get encoder input (B,C,H,W)\n",
        "            2. flatten input to (B*H*W,C)\n",
        "        \"\"\"\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = z.permute(0, 2, 3, 1).contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.matmul(z_flattened, self.embedding.weight.t())\n",
        "\n",
        "        ## could possible replace this here\n",
        "        # #\\start...\n",
        "        # find closest encodings\n",
        "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
        "\n",
        "        min_encodings = torch.zeros(\n",
        "            min_encoding_indices.shape[0], self.n_e).to(z)\n",
        "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
        "\n",
        "        # dtype min encodings: torch.float32\n",
        "        # min_encodings shape: torch.Size([2048, 512])\n",
        "        # min_encoding_indices.shape: torch.Size([2048, 1])\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
        "        #.........\\end\n",
        "\n",
        "        # with:\n",
        "        # .........\\start\n",
        "        #min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        #z_q = self.embedding(min_encoding_indices)\n",
        "        # ......\\end......... (TODO)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
        "            torch.mean((z_q - z.detach()) ** 2)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # perplexity\n",
        "        e_mean = torch.mean(min_encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # shape specifying (batch, height, width, channel)\n",
        "        # TODO: check for more easy handling with nn.Embedding\n",
        "        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n",
        "        min_encodings.scatter_(1, indices[:,None], 1)\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n",
        "\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q\n",
        "\n",
        "\n",
        "class GumbelQuantize(nn.Module):\n",
        "    \"\"\"\n",
        "    credit to @karpathy: https://github.com/karpathy/deep-vector-quantization/blob/main/model.py (thanks!)\n",
        "    Gumbel Softmax trick quantizer\n",
        "    Categorical Reparameterization with Gumbel-Softmax, Jang et al. 2016\n",
        "    https://arxiv.org/abs/1611.01144\n",
        "    \"\"\"\n",
        "    def __init__(self, num_hiddens, embedding_dim, n_embed, straight_through=True, kl_weight=5e-4, temp_init=1.0, use_vqinterface=True, remap=None, unknown_index=\"random\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_embed = n_embed\n",
        "\n",
        "        self.straight_through = straight_through\n",
        "        self.temperature = temp_init\n",
        "        self.kl_weight = kl_weight\n",
        "\n",
        "        self.proj = nn.Conv2d(num_hiddens, n_embed, 1)\n",
        "        self.embed = nn.Embedding(n_embed, embedding_dim)\n",
        "\n",
        "        self.use_vqinterface = use_vqinterface\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_embed} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_embed\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z, temp=None, return_logits=False):\n",
        "        # force hard = True when we are in eval mode, as we must quantize. actually, always true seems to work\n",
        "        hard = self.straight_through if self.training else True\n",
        "        temp = self.temperature if temp is None else temp\n",
        "\n",
        "        logits = self.proj(z)\n",
        "        if self.remap is not None:\n",
        "            # continue only with used logits\n",
        "            full_zeros = torch.zeros_like(logits)\n",
        "            logits = logits[:,self.used,...]\n",
        "\n",
        "        soft_one_hot = F.gumbel_softmax(logits, tau=temp, dim=1, hard=hard)\n",
        "        if self.remap is not None:\n",
        "            # go back to all entries but unused set to zero\n",
        "            full_zeros[:,self.used,...] = soft_one_hot\n",
        "            soft_one_hot = full_zeros\n",
        "        z_q = einsum('b n h w, n d -> b d h w', soft_one_hot, self.embed.weight)\n",
        "\n",
        "        # + kl divergence to the prior loss\n",
        "        qy = F.softmax(logits, dim=1)\n",
        "        diff = self.kl_weight * torch.sum(qy * torch.log(qy * self.n_embed + 1e-10), dim=1).mean()\n",
        "\n",
        "        ind = soft_one_hot.argmax(dim=1)\n",
        "        if self.remap is not None:\n",
        "            ind = self.remap_to_used(ind)\n",
        "        if self.use_vqinterface:\n",
        "            if return_logits:\n",
        "                return z_q, diff, (None, None, ind), logits\n",
        "            return z_q, diff, (None, None, ind)\n",
        "        return z_q, diff, ind\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        b, h, w, c = shape\n",
        "        assert b*h*w == indices.shape[0]\n",
        "        indices = rearrange(indices, '(b h w) -> b h w', b=b, h=h, w=w)\n",
        "        if self.remap is not None:\n",
        "            indices = self.unmap_to_all(indices)\n",
        "        one_hot = F.one_hot(indices, num_classes=self.n_embed).permute(0, 3, 1, 2).float()\n",
        "        z_q = einsum('b n h w, n d -> b d h w', one_hot, self.embed.weight)\n",
        "        return z_q\n",
        "\n",
        "\n",
        "class VectorQuantizer2(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly\n",
        "    avoids costly matrix multiplications and allows for post-hoc remapping of indices.\n",
        "    \"\"\"\n",
        "    # NOTE: due to a bug the beta term was applied to the wrong term. for\n",
        "    # backwards compatibility we use the buggy version by default, but you can\n",
        "    # specify legacy=False to fix it.\n",
        "    def __init__(self, n_e, e_dim, beta, remap=None, unknown_index=\"random\", sane_index_shape=False, legacy=True):\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "        self.legacy = legacy\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_e\n",
        "\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z, temp=None, rescale_logits=False, return_logits=False):\n",
        "        assert temp is None or temp==1.0, \"Only for interface compatible with Gumbel\"\n",
        "        assert rescale_logits==False, \"Only for interface compatible with Gumbel\"\n",
        "        assert return_logits==False, \"Only for interface compatible with Gumbel\"\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "        perplexity = None\n",
        "        min_encodings = None\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * torch.mean((z_q.detach()-z)**2) + torch.mean((z_q - z.detach()) ** 2)\n",
        "        # loss = torch.mean((z_q.detach()-z)**2) + self.beta * torch.mean((z_q - z.detach()) ** 2) # legacy\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "        if self.remap is not None:\n",
        "            min_encoding_indices = min_encoding_indices.reshape(z.shape[0],-1) # add batch axis\n",
        "            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n",
        "            min_encoding_indices = min_encoding_indices.reshape(-1,1) # flatten\n",
        "        if self.sane_index_shape:\n",
        "            min_encoding_indices = min_encoding_indices.reshape(\n",
        "                z_q.shape[0], z_q.shape[2], z_q.shape[3])\n",
        "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # shape specifying (batch, height, width, channel)\n",
        "        if self.remap is not None:\n",
        "            indices = indices.reshape(shape[0],-1) # add batch axis\n",
        "            indices = self.unmap_to_all(indices)\n",
        "            indices = indices.reshape(-1) # flatten again\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "        return z_q\n",
        "\n",
        "class EmbeddingEMA(nn.Module):\n",
        "    def __init__(self, num_tokens, codebook_dim, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        weight = torch.randn(num_tokens, codebook_dim)\n",
        "        self.weight = nn.Parameter(weight, requires_grad = False)\n",
        "        self.cluster_size = nn.Parameter(torch.zeros(num_tokens), requires_grad = False)\n",
        "        self.embed_avg = nn.Parameter(weight.clone(), requires_grad = False)\n",
        "        self.update = True\n",
        "\n",
        "    def forward(self, embed_id):\n",
        "        return F.embedding(embed_id, self.weight)\n",
        "\n",
        "    def cluster_size_ema_update(self, new_cluster_size):\n",
        "        self.cluster_size.data.mul_(self.decay).add_(new_cluster_size, alpha=1 - self.decay)\n",
        "\n",
        "    def embed_avg_ema_update(self, new_embed_avg):\n",
        "        self.embed_avg.data.mul_(self.decay).add_(new_embed_avg, alpha=1 - self.decay)\n",
        "\n",
        "    def weight_update(self, num_tokens):\n",
        "        n = self.cluster_size.sum()\n",
        "        smoothed_cluster_size = (\n",
        "                (self.cluster_size + self.eps) / (n + num_tokens * self.eps) * n\n",
        "            )\n",
        "        #normalize embedding average with smoothed cluster size\n",
        "        embed_normalized = self.embed_avg / smoothed_cluster_size.unsqueeze(1)\n",
        "        self.weight.data.copy_(embed_normalized)\n",
        "\n",
        "\n",
        "class EMAVectorQuantizer(nn.Module):\n",
        "    def __init__(self, n_embed, embedding_dim, beta, decay=0.99, eps=1e-5,\n",
        "                remap=None, unknown_index=\"random\"):\n",
        "        super().__init__()\n",
        "        self.codebook_dim = codebook_dim\n",
        "        self.num_tokens = num_tokens\n",
        "        self.beta = beta\n",
        "        self.embedding = EmbeddingEMA(self.num_tokens, self.codebook_dim, decay, eps)\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_embed} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_embed\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        #z, 'b c h w -> b h w c'\n",
        "        z = rearrange(z, 'b c h w -> b h w c')\n",
        "        z_flattened = z.reshape(-1, self.codebook_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = z_flattened.pow(2).sum(dim=1, keepdim=True) + \\\n",
        "            self.embedding.weight.pow(2).sum(dim=1) - 2 * \\\n",
        "            torch.einsum('bd,nd->bn', z_flattened, self.embedding.weight) # 'n d -> d n'\n",
        "\n",
        "\n",
        "        encoding_indices = torch.argmin(d, dim=1)\n",
        "\n",
        "        z_q = self.embedding(encoding_indices).view(z.shape)\n",
        "        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        if self.training and self.embedding.update:\n",
        "            #EMA cluster size\n",
        "            encodings_sum = encodings.sum(0)\n",
        "            self.embedding.cluster_size_ema_update(encodings_sum)\n",
        "            #EMA embedding average\n",
        "            embed_sum = encodings.transpose(0,1) @ z_flattened\n",
        "            self.embedding.embed_avg_ema_update(embed_sum)\n",
        "            #normalize embed_avg and update weight\n",
        "            self.embedding.weight_update(self.num_tokens)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * F.mse_loss(z_q.detach(), z)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        #z_q, 'b h w c -> b c h w'\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w')\n",
        "        return z_q, loss, (perplexity, encodings, encoding_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7CSqxzfG_2rb"
      },
      "outputs": [],
      "source": [
        "# @title CompVis stable-diffusion autoencoder.py\n",
        "# https://github.com/CompVis/stable-diffusion/blob/main/ldm/models/autoencoder.py\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# from taming.modules.vqvae.quantize import VectorQuantizer2 as VectorQuantizer\n",
        "\n",
        "from ldm.modules.diffusionmodules.model import Encoder, Decoder\n",
        "from ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "class VQModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 n_embed,\n",
        "                 embed_dim,\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 image_key=\"image\",\n",
        "                 colorize_nlabels=None,\n",
        "                 monitor=None,\n",
        "                 batch_resize_range=None,\n",
        "                 scheduler_config=None,\n",
        "                 lr_g_factor=1.0,\n",
        "                 remap=None,\n",
        "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "                 use_ema=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.image_key = image_key\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n",
        "                                        remap=remap,\n",
        "                                        sane_index_shape=sane_index_shape)\n",
        "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        if colorize_nlabels is not None:\n",
        "            assert type(colorize_nlabels)==int\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        self.batch_resize_range = batch_resize_range\n",
        "        if self.batch_resize_range is not None:\n",
        "            print(f\"{self.__class__.__name__}: Using per-batch resizing in range {batch_resize_range}.\")\n",
        "\n",
        "        self.use_ema = use_ema\n",
        "        if self.use_ema:\n",
        "            self.model_ema = LitEma(self)\n",
        "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
        "\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
        "        self.scheduler_config = scheduler_config\n",
        "        self.lr_g_factor = lr_g_factor\n",
        "\n",
        "    @contextmanager\n",
        "    def ema_scope(self, context=None):\n",
        "        if self.use_ema:\n",
        "            self.model_ema.store(self.parameters())\n",
        "            self.model_ema.copy_to(self)\n",
        "            if context is not None:\n",
        "                print(f\"{context}: Switched to EMA weights\")\n",
        "        try:\n",
        "            yield None\n",
        "        finally:\n",
        "            if self.use_ema:\n",
        "                self.model_ema.restore(self.parameters())\n",
        "                if context is not None:\n",
        "                    print(f\"{context}: Restored training weights\")\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
        "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        missing, unexpected = self.load_state_dict(sd, strict=False)\n",
        "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
        "        if len(missing) > 0:\n",
        "            print(f\"Missing Keys: {missing}\")\n",
        "            print(f\"Unexpected Keys: {unexpected}\")\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        if self.use_ema:\n",
        "            self.model_ema(self)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        if self.batch_resize_range is not None:\n",
        "            lower_size = self.batch_resize_range[0]\n",
        "            upper_size = self.batch_resize_range[1]\n",
        "            if self.global_step <= 4:\n",
        "                # do the first few batches with max size to avoid later oom\n",
        "                new_resize = upper_size\n",
        "            else:\n",
        "                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n",
        "            if new_resize != x.shape[2]:\n",
        "                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n",
        "            x = x.detach()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        # https://github.com/pytorch/pytorch/issues/37142\n",
        "        # try not to fool the heuristics\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # autoencode\n",
        "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
        "                                            predicted_indices=ind)\n",
        "\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # discriminator\n",
        "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        log_dict = self._validation_step(batch, batch_idx)\n",
        "        with self.ema_scope():\n",
        "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
        "        return log_dict\n",
        "\n",
        "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
        "                                        self.global_step,\n",
        "                                        last_layer=self.get_last_layer(),\n",
        "                                        split=\"val\"+suffix,\n",
        "                                        predicted_indices=ind\n",
        "                                        )\n",
        "\n",
        "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
        "                                            self.global_step,\n",
        "                                            last_layer=self.get_last_layer(),\n",
        "                                            split=\"val\"+suffix,\n",
        "                                            predicted_indices=ind\n",
        "                                            )\n",
        "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
        "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr_d = self.learning_rate\n",
        "        lr_g = self.lr_g_factor*self.learning_rate\n",
        "        print(\"lr_d\", lr_d)\n",
        "        print(\"lr_g\", lr_g)\n",
        "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
        "                                  list(self.decoder.parameters())+\n",
        "                                  list(self.quantize.parameters())+\n",
        "                                  list(self.quant_conv.parameters())+\n",
        "                                  list(self.post_quant_conv.parameters()),\n",
        "                                  lr=lr_g, betas=(0.5, 0.9))\n",
        "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
        "                                    lr=lr_d, betas=(0.5, 0.9))\n",
        "\n",
        "        if self.scheduler_config is not None:\n",
        "            scheduler = instantiate_from_config(self.scheduler_config)\n",
        "\n",
        "            print(\"Setting up LambdaLR scheduler...\")\n",
        "            scheduler = [\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                },\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                },\n",
        "            ]\n",
        "            return [opt_ae, opt_disc], scheduler\n",
        "        return [opt_ae, opt_disc], []\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "    def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n",
        "        log = dict()\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        x = x.to(self.device)\n",
        "        if only_inputs:\n",
        "            log[\"inputs\"] = x\n",
        "            return log\n",
        "        xrec, _ = self(x)\n",
        "        if x.shape[1] > 3:\n",
        "            # colorize with random projection\n",
        "            assert xrec.shape[1] > 3\n",
        "            x = self.to_rgb(x)\n",
        "            xrec = self.to_rgb(xrec)\n",
        "        log[\"inputs\"] = x\n",
        "        log[\"reconstructions\"] = xrec\n",
        "        if plot_ema:\n",
        "            with self.ema_scope():\n",
        "                xrec_ema, _ = self(x)\n",
        "                if x.shape[1] > 3: xrec_ema = self.to_rgb(xrec_ema)\n",
        "                log[\"reconstructions_ema\"] = xrec_ema\n",
        "        return log\n",
        "\n",
        "    def to_rgb(self, x):\n",
        "        assert self.image_key == \"segmentation\"\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
        "        x = F.conv2d(x, weight=self.colorize)\n",
        "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
        "        return x\n",
        "\n",
        "\n",
        "class VQModelInterface(VQModel):\n",
        "    def __init__(self, embed_dim, *args, **kwargs):\n",
        "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, h, force_not_quantize=False):\n",
        "        # also go through quantization layer\n",
        "        if not force_not_quantize:\n",
        "            quant, emb_loss, info = self.quantize(h)\n",
        "        else:\n",
        "            quant = h\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "\n",
        "class AutoencoderKL(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 embed_dim,\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 image_key=\"image\",\n",
        "                 colorize_nlabels=None,\n",
        "                 monitor=None,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.image_key = image_key\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        assert ddconfig[\"double_z\"]\n",
        "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        self.embed_dim = embed_dim\n",
        "        if colorize_nlabels is not None:\n",
        "            assert type(colorize_nlabels)==int\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
        "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        self.load_state_dict(sd, strict=False)\n",
        "        print(f\"Restored from {path}\")\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, sample_posterior=True):\n",
        "        posterior = self.encode(input)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        return dec, posterior\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # train encoder+decoder+logvar\n",
        "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # train the discriminator\n",
        "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step, last_layer=self.get_last_layer(), split=\"train\")\n",
        "\n",
        "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step, last_layer=self.get_last_layer(), split=\"val\")\n",
        "\n",
        "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.learning_rate\n",
        "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
        "                                  list(self.decoder.parameters())+\n",
        "                                  list(self.quant_conv.parameters())+\n",
        "                                  list(self.post_quant_conv.parameters()),\n",
        "                                  lr=lr, betas=(0.5, 0.9))\n",
        "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "        return [opt_ae, opt_disc], []\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_images(self, batch, only_inputs=False, **kwargs):\n",
        "        log = dict()\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        x = x.to(self.device)\n",
        "        if not only_inputs:\n",
        "            xrec, posterior = self(x)\n",
        "            if x.shape[1] > 3:\n",
        "                # colorize with random projection\n",
        "                assert xrec.shape[1] > 3\n",
        "                x = self.to_rgb(x)\n",
        "                xrec = self.to_rgb(xrec)\n",
        "            log[\"samples\"] = self.decode(torch.randn_like(posterior.sample()))\n",
        "            log[\"reconstructions\"] = xrec\n",
        "        log[\"inputs\"] = x\n",
        "        return log\n",
        "\n",
        "    def to_rgb(self, x):\n",
        "        assert self.image_key == \"segmentation\"\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
        "        x = F.conv2d(x, weight=self.colorize)\n",
        "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XOuGUdMZaxB6"
      },
      "outputs": [],
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ConvLayer\n",
        "# nn.Sequential(\n",
        "#     nn.Dropout2d(dropout), nn.Conv2d(in_ch, out_ch, 3, 1, 3//2, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU()\n",
        "# )\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.repeats = out_ch//in_ch\n",
        "        if out_ch//in_ch > 1:\n",
        "            self.func = lambda x: x.repeat_interleave(out_ch//in_ch, dim=1) # [b,i,h,w] -> [b,o,h,w]\n",
        "        elif in_ch//out_ch > 1:\n",
        "            self.func = lambda x: torch.unflatten(x, 1, (out_ch, in_ch//out_ch)).mean(dim=2) # [b,i,h,w] -> [b,o,i/o,h,w] -> [b,o,h,w]\n",
        "        else: print('err SameCh', in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        return self.func(x)\n",
        "\n",
        "class PixelShortcut(nn.Module): # up shortcut [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(SameCh(in_ch, out_ch*r**2), nn.PixelShuffle(r)) #\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), SameCh(in_ch*r**2, out_ch)) #\n",
        "        else: self.net = SameCh(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): #\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module): # down main [b,i,2h,2w] -> [b,o,h,w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch//r**2, kernel_size, 1, kernel_size//2)\n",
        "\n",
        "    def forward(self, x): # [b,i,2h,2w] -> [b,o/4,2h,2w] -> [b,o,h,w]\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module): # up main [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch*r**2, kernel_size, 1, kernel_size//2)\n",
        "        # self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, 1, kernel_size//2) # InterpolateConvUpSampleLayer\n",
        "\n",
        "    def forward(self, x): # [b,i,h,w] -> [b,4o,h,w] -> [b,o,2h,2w]\n",
        "        # x = torch.nn.functional.interpolate(x, scale_r=self.r, mode=\"nearest\")\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "        # if self.r>1: self.net = nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), out_r=r), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), in_r=r)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        else: self.net = nn.Conv2d(in_ch, out_ch, kernel, 1, kernel//2)\n",
        "        # self.net.apply(self.init_conv_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# # https://arxiv.org/pdf/1707.02937\n",
        "# nn.Sequential(init_conv(nn.Conv2d(in_ch, out_ch*r**2, kernel, 1, kernel//2), 'out'), nn.PixelShuffle(r))\n",
        "# nn.Sequential(nn.PixelUnshuffle(r), init_conv(nn.Conv2d(in_ch*r**2, out_ch, kernel, 1, kernel//2), 'in'))\n",
        "\n",
        "# https://github.com/fastai/fastai/blob/main/fastai/layers.py#L368\n",
        "def icnr_init(x, scale=2, init=nn.init.kaiming_normal_):\n",
        "    \"ICNR init of `x`, with `scale` and `init` function\"\n",
        "    ni,nf,h,w = x.shape\n",
        "    ni2 = int(ni/(scale**2))\n",
        "    k = init(x.new_zeros([ni2,nf,h,w])).transpose(0, 1)\n",
        "    k = k.contiguous().view(ni2, nf, -1)\n",
        "    k = k.repeat(1, 1, scale**2)\n",
        "    return k.contiguous().view([nf,ni,h,w]).transpose(0, 1)\n",
        "\n",
        "class PixelShuffle_ICNR(nn.Sequential):\n",
        "    \"Upsample by `scale` from `ni` filters to `nf` (default `ni`), using `nn.PixelShuffle`.\"\n",
        "    def __init__(self, ni, nf=None, scale=2, blur=False, norm_type=NormType.Weight, act_cls=defaults.activation):\n",
        "        super().__init__()\n",
        "        nf = ifnone(nf, ni)\n",
        "        layers = [ConvLayer(ni, nf*(scale**2), ks=1, norm_type=norm_type, act_cls=act_cls, bias_std=0),\n",
        "                  nn.PixelShuffle(scale)]\n",
        "        if norm_type == NormType.Weight:\n",
        "            layers[0][0].weight_v.data.copy_(icnr_init(layers[0][0].weight_v.data))\n",
        "            layers[0][0].weight_g.data.copy_(((layers[0][0].weight_v.data**2).sum(dim=[1,2,3])**0.5)[:,None,None,None])\n",
        "        else:\n",
        "            layers[0][0].weight.data.copy_(icnr_init(layers[0][0].weight.data))\n",
        "        if blur: layers += [nn.ReplicationPad2d((1,0,1,0)), nn.AvgPool2d(2, stride=1)]\n",
        "        super().__init__(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# block = EfficientViTBlock(in_ch, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU\n",
        "# self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio,\n",
        "#     use_bias=(True, True, False), norm=(None, None, norm), act_func=(act_func, act_func, None))\n",
        "class GLUMBConv(nn.Module):\n",
        "    # def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4, use_bias=False, norm=(None, None, \"ln2d\"), act_func=(\"silu\", \"silu\", None)):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4):\n",
        "        super().__init__()\n",
        "        mid_channels = round(in_ch * expand_ratio) if mid_channels is None else mid_channels\n",
        "        # self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        # self.inverted_conv = ConvLayer(in_ch, mid_channels * 2, 1, use_bias=use_bias[0], norm=norm[0], act_func=act_func[0],)\n",
        "        # self.depth_conv = ConvLayer(mid_channels * 2, mid_channels * 2, kernel_size, stride=stride, groups=mid_channels * 2, use_bias=use_bias[1], norm=norm[1], act_func=None,)\n",
        "        self.inverted_depth_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_channels*2, 1, 1, 0), nn.SiLU(),\n",
        "            nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2),\n",
        "        )\n",
        "        # self.point_conv = ConvLayer(mid_channels, out_ch, 1, use_bias=use_bias[2], norm=norm[2], act_func=act_func[2],)\n",
        "        self.point_conv = nn.Sequential(\n",
        "            nn.Conv2d(mid_channels, out_ch, 1, 1, 0, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.inverted_conv(x)\n",
        "        # x = self.depth_conv(x)\n",
        "        x = self.inverted_depth_conv(x)\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        x = x * nn.SiLU()(gate)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# main_block = ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel_size=3, stride=1, d_model=None,\n",
        "        use_bias=False, norm=(\"bn2d\", \"bn2d\"), act_func=(\"relu6\", None)):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_ch\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, d_model, kernel_size, stride, kernel_size//2), nn.SiLU(),\n",
        "            nn.Conv2d(d_model, out_ch, kernel_size, 1, kernel_size//2, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(self, in_ch, heads_ratio = 1.0, dim=32, expand_ratio=1, # expand_ratio=4\n",
        "        # scales: tuple[int, ...] = (5,), # (5,): sana\n",
        "        # act_func = \"hswish\", # nn.Hardswish()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # self.context_module = LiteMLA(in_ch, in_ch, heads_ratio=heads_ratio, dim=dim, norm=(None, norm), scales=scales,)\n",
        "        self.context_module = AttentionBlock(in_ch, d_head=8)\n",
        "        # self.local_module = MBConv(\n",
        "        self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.context_module(x)\n",
        "        x = x + self.local_module(x)\n",
        "        return x\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "    # def forward(self, x):\n",
        "    #     res = self.forward_main(self.pre_norm(x)) + self.shortcut(x)\n",
        "    #     res = self.post_act(res)\n",
        "    #     return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_FyFDkua0lA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_block(block_type, d_model, norm=None, act=None):\n",
        "    if block_type == \"ResBlock\": return ResBlock(d_model) # ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "    # ResBlock: bn2d, relu ; EViT_GLU: trms2d, silu\n",
        "    elif block_type == \"EViT_GLU\": return EfficientViTBlock(d_model) # EfficientViTBlock(d_model, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU:scales=() ; EViTS5_GLU sana:scales=(5,)\n",
        "\n",
        "class LevelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, depth, block_type, norm=None, act=None, updown=None):\n",
        "        super().__init__()\n",
        "        stage = []\n",
        "        if updown=='up': stage.append(UpsampleBlock(in_ch, out_ch))\n",
        "        for d in range(depth):\n",
        "            # block = build_block(block_type=block_type, in_ch=d_model if d > 0 else in_ch, out_ch=d_model, norm=norm, act=act,)\n",
        "            block = build_block(block_type, out_ch if updown=='up' else in_ch, norm=norm, act=act,)\n",
        "            stage.append(block)\n",
        "        if updown=='down': stage.append(DownsampleBlock(in_ch, out_ch))\n",
        "        self.block = nn.Sequential(*stage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# stage = build_stage_main(width, depth, block_type)\n",
        "# downsample_block = DownsampleBlock(width, width_list[stage_id + 1])\n",
        "\n",
        "# upsample_block = UpsampleBlock(width_list[stage_id + 1], width)\n",
        "# stage.extend(build_stage_main(width, depth, block_type, \"bn2d\", \"silu\", input_width=width))\n",
        "\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = nn.Conv2d(in_ch, out_ch, 3, 2, 3//2)\n",
        "        # self.block = ConvPixelUnshuffleDownSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=3, r=1/2)\n",
        "        # self.shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(in_ch, out_ch, r=2)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=1/2)\n",
        "    def forward(self, x):\n",
        "        # print(\"DownsampleBlock fwd\", x.shape, self.block(x).shape + self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,4,8,2,2,2]\n",
        "\n",
        "        # # self.project_in = nn.Conv2d(in_ch, width_list[0], 3, 1, 3//2) # if depth_list[0] > 0:\n",
        "        self.project_in = DownsampleBlock(in_ch, width_list[0]) # shortcut=None # self.project_in = ConvPixelUnshuffleDownSampleLayer(in_ch, width_list[0], kernel_size=3, r=2)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[0], width_list[-1], depth=depth_list[0], block_type='ResBlock', updown='down'),\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "        )\n",
        "\n",
        "        self.out_block = nn.Conv2d(width_list[-1], out_ch, 3, 1, 3//2)\n",
        "        # self.out_shortcut = PixelUnshuffleChannelAveragingDownSampleLayer(width_list[-1], out_ch, r=1)\n",
        "        self.out_shortcut = PixelShortcut(width_list[-1], out_ch, r=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x = self.stages(x)\n",
        "        # print(\"Encoder fwd\", x.shape, self.out_block, self.out_shortcut(x).shape)\n",
        "        x = self.out_block(x) + self.out_shortcut(x)\n",
        "        return x\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = ConvPixelShuffleUpSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=3, r=2)\n",
        "        # self.block = InterpolateConvUpSampleLayer(in_ch=in_ch, out_ch=out_ch, kernel_size=3, r=2)\n",
        "        # self.shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, out_ch, r=2)\n",
        "        self.shortcut_block = PixelShortcut(in_ch, out_ch, r=2)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,o,2h,2w]\n",
        "        # print(\"UpsampleBlock fwd\", x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.in_block = nn.Conv2d(in_ch, width_list[-1], 3, 1, 3//2)\n",
        "        # self.in_shortcut = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, width_list[-1], r=1)\n",
        "        self.in_shortcut = PixelShortcut(in_ch, width_list[-1], r=1)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "            LevelBlock(width_list[-1], width_list[0], depth=depth_list[0], block_type='ResBlock', updown='up'),\n",
        "        )\n",
        "\n",
        "        # if depth_list[0] > 0:\n",
        "        # self.project_out = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(width_list[0]), nn.ReLU(), nn.Conv2d(width_list[0], out_ch, 3, 1, 3//2) # norm=\"trms2d\"\n",
        "        #     )\n",
        "        # else:\n",
        "        self.project_out = nn.Sequential(\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpsampleBlock(width_list[0], out_ch) # shortcut=None ; norm=\"trms2d\"\n",
        "            # nn.BatchNorm2d(width_list[0]), nn.ReLU(), ConvPixelShuffleUpSampleLayer(width_list[0], out_ch, kernel_size=3, r=2) # shortcut=None ; norm=\"trms2d\"\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.in_block(x) + self.in_shortcut(x)\n",
        "        x = self.stages(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(in_ch, out_ch, d_model, mult, depth_list)\n",
        "        self.decoder = Decoder(out_ch, in_ch, d_model, mult, depth_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# https://discuss.pytorch.org/t/is-there-a-layer-normalization-for-conv2d/7595/5\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "class LayerNorm2d(nn.LayerNorm):\n",
        "    def __init__(self, num_channels, eps=1e-6, affine=True):\n",
        "        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=24, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# model = Encoder(in_ch, out_ch, d_model=32, mult=[1,1], depth_list=[2,2])\n",
        "# print(sum(p.numel() for p in model.project_in.parameters() if p.requires_grad)) # 896\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "# print(sum(p.numel() for p in model.out_shortcut.parameters() if p.requires_grad)) # 0\n",
        "# print(sum(p.numel() for p in model.out_block.parameters() if p.requires_grad)) # 18436\n",
        "# model = Decoder(out_ch, in_ch)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rkrEVsXxSs_C"
      },
      "outputs": [],
      "source": [
        "# @title efficientvit nn/ops.py\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from efficientvit.models.nn.act import build_act\n",
        "from efficientvit.models.nn.norm import build_norm\n",
        "from efficientvit.models.utils import get_same_padding, list_sum, resize, val2list, val2tuple\n",
        "\n",
        "__all__ = [\n",
        "    \"ConvLayer\",\n",
        "    \"UpSampleLayer\",\n",
        "    \"ConvPixelUnshuffleDownSampleLayer\",\n",
        "    \"PixelUnshuffleChannelAveragingDownSampleLayer\",\n",
        "    \"ConvPixelShuffleUpSampleLayer\",\n",
        "    \"ChannelDuplicatingPixelUnshuffleUpSampleLayer\",\n",
        "    \"LinearLayer\",\n",
        "    \"IdentityLayer\",\n",
        "    \"DSConv\",\n",
        "    \"MBConv\",\n",
        "    \"FusedMBConv\",\n",
        "    \"ResBlock\",\n",
        "    \"LiteMLA\",\n",
        "    \"EfficientViTBlock\",\n",
        "    \"ResidualBlock\",\n",
        "    \"DAGBlock\",\n",
        "    \"OpSequential\",\n",
        "]\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Basic Layers                                      #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        dilation=1,\n",
        "        groups=1,\n",
        "        use_bias=False,\n",
        "        dropout=0,\n",
        "        norm=\"bn2d\",\n",
        "        act_func=\"relu\",\n",
        "    ):\n",
        "        super(ConvLayer, self).__init__()\n",
        "\n",
        "        padding = get_same_padding(kernel_size)\n",
        "        padding *= dilation\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropout, inplace=False) if dropout > 0 else None\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=(kernel_size, kernel_size),\n",
        "            stride=(stride, stride),\n",
        "            padding=padding,\n",
        "            dilation=(dilation, dilation),\n",
        "            groups=groups,\n",
        "            bias=use_bias,\n",
        "        )\n",
        "        self.norm = build_norm(norm, num_features=out_channels)\n",
        "        self.act = build_act(act_func)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x)\n",
        "        x = self.conv(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.act:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mode=\"bicubic\",\n",
        "        size: Optional[int | tuple[int, int] | list[int]] = None,\n",
        "        factor=2,\n",
        "        align_corners=False,\n",
        "    ):\n",
        "        super(UpSampleLayer, self).__init__()\n",
        "        self.mode = mode\n",
        "        self.size = val2list(size, 2) if size is not None else None\n",
        "        self.factor = None if self.size is not None else factor\n",
        "        self.align_corners = align_corners\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if (self.size is not None and tuple(x.shape[-2:]) == self.size) or self.factor == 1:\n",
        "            return x\n",
        "        if x.dtype in [torch.float16, torch.bfloat16]:\n",
        "            x = x.float()\n",
        "        return resize(x, self.size, self.factor, self.mode, self.align_corners)\n",
        "\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        out_ratio = factor**2\n",
        "        assert out_channels % out_ratio == 0\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels // out_ratio,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.factor = factor\n",
        "        assert in_channels * factor**2 % out_channels == 0\n",
        "        self.group_size = in_channels * factor**2 // out_channels\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.pixel_unshuffle(x, self.factor)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, self.out_channels, self.group_size, H, W)\n",
        "        x = x.mean(dim=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        out_ratio = factor**2\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels * out_ratio,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class InterpolateConvUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int,\n",
        "        factor: int,\n",
        "        mode: str = \"nearest\",\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.factor = factor\n",
        "        self.mode = mode\n",
        "        self.conv = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=self.factor, mode=self.mode)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ChannelDuplicatingPixelUnshuffleUpSampleLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        factor: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.factor = factor\n",
        "        assert out_channels * factor**2 % in_channels == 0\n",
        "        self.repeats = out_channels * factor**2 // in_channels\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.repeat_interleave(self.repeats, dim=1)\n",
        "        x = F.pixel_shuffle(x, self.factor)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LinearLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        out_features: int,\n",
        "        use_bias=True,\n",
        "        dropout=0,\n",
        "        norm=None,\n",
        "        act_func=None,\n",
        "    ):\n",
        "        super(LinearLayer, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout, inplace=False) if dropout > 0 else None\n",
        "        self.linear = nn.Linear(in_features, out_features, use_bias)\n",
        "        self.norm = build_norm(norm, num_features=out_features)\n",
        "        self.act = build_act(act_func)\n",
        "\n",
        "    def _try_squeeze(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() > 2:\n",
        "            x = torch.flatten(x, start_dim=1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self._try_squeeze(x)\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        if self.norm:\n",
        "            x = self.norm(x)\n",
        "        if self.act:\n",
        "            x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class IdentityLayer(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Basic Blocks                                      #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class DSConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super(DSConv, self).__init__()\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        self.depth_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            in_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            groups=in_channels,\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "            use_bias=use_bias[0],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "            use_bias=use_bias[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.depth_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", \"relu6\", None),\n",
        "    ):\n",
        "        super(MBConv, self).__init__()\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 3)\n",
        "        norm = val2tuple(norm, 3)\n",
        "        act_func = val2tuple(act_func, 3)\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.inverted_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            1,\n",
        "            stride=1,\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "            use_bias=use_bias[0],\n",
        "        )\n",
        "        self.depth_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            groups=mid_channels,\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "            use_bias=use_bias[1],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            norm=norm[2],\n",
        "            act_func=act_func[2],\n",
        "            use_bias=use_bias[2],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.inverted_conv(x)\n",
        "        x = self.depth_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FusedMBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        groups=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.spatial_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            groups=groups,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.spatial_conv(x)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GLUMBConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=6,\n",
        "        use_bias=False,\n",
        "        norm=(None, None, \"ln2d\"),\n",
        "        act_func=(\"silu\", \"silu\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 3)\n",
        "        norm = val2tuple(norm, 3)\n",
        "        act_func = val2tuple(act_func, 3)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        self.inverted_conv = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels * 2,\n",
        "            1,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.depth_conv = ConvLayer(\n",
        "            mid_channels * 2,\n",
        "            mid_channels * 2,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            groups=mid_channels * 2,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=None,\n",
        "        )\n",
        "        self.point_conv = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[2],\n",
        "            norm=norm[2],\n",
        "            act_func=act_func[2],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.inverted_conv(x)\n",
        "        x = self.depth_conv(x)\n",
        "\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        gate = self.glu_act(gate)\n",
        "        x = x * gate\n",
        "\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mid_channels=None,\n",
        "        expand_ratio=1,\n",
        "        use_bias=False,\n",
        "        norm=(\"bn2d\", \"bn2d\"),\n",
        "        act_func=(\"relu6\", None),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        mid_channels = round(in_channels * expand_ratio) if mid_channels is None else mid_channels\n",
        "\n",
        "        self.conv1 = ConvLayer(\n",
        "            in_channels,\n",
        "            mid_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.conv2 = ConvLayer(\n",
        "            mid_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LiteMLA(nn.Module):\n",
        "    r\"\"\"Lightweight multi-scale linear attention\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        heads: Optional[int] = None,\n",
        "        heads_ratio: float = 1.0,\n",
        "        dim=8,\n",
        "        use_bias=False,\n",
        "        norm=(None, \"bn2d\"),\n",
        "        act_func=(None, None),\n",
        "        kernel_func=\"relu\",\n",
        "        scales: tuple[int, ...] = (5,),\n",
        "        eps=1.0e-15,\n",
        "    ):\n",
        "        super(LiteMLA, self).__init__()\n",
        "        self.eps = eps\n",
        "        heads = int(in_channels // dim * heads_ratio) if heads is None else heads\n",
        "\n",
        "        total_dim = heads * dim\n",
        "\n",
        "        use_bias = val2tuple(use_bias, 2)\n",
        "        norm = val2tuple(norm, 2)\n",
        "        act_func = val2tuple(act_func, 2)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.qkv = ConvLayer(\n",
        "            in_channels,\n",
        "            3 * total_dim,\n",
        "            1,\n",
        "            use_bias=use_bias[0],\n",
        "            norm=norm[0],\n",
        "            act_func=act_func[0],\n",
        "        )\n",
        "        self.aggreg = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        3 * total_dim,\n",
        "                        3 * total_dim,\n",
        "                        scale,\n",
        "                        padding=get_same_padding(scale),\n",
        "                        groups=3 * total_dim,\n",
        "                        bias=use_bias[0],\n",
        "                    ),\n",
        "                    nn.Conv2d(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0]),\n",
        "                )\n",
        "                for scale in scales\n",
        "            ]\n",
        "        )\n",
        "        self.kernel_func = build_act(kernel_func, inplace=False)\n",
        "\n",
        "        self.proj = ConvLayer(\n",
        "            total_dim * (1 + len(scales)),\n",
        "            out_channels,\n",
        "            1,\n",
        "            use_bias=use_bias[1],\n",
        "            norm=norm[1],\n",
        "            act_func=act_func[1],\n",
        "        )\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def relu_linear_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
        "        B, _, H, W = list(qkv.size())\n",
        "\n",
        "        if qkv.dtype == torch.float16:\n",
        "            qkv = qkv.float()\n",
        "\n",
        "        qkv = torch.reshape(\n",
        "            qkv,\n",
        "            (\n",
        "                B,\n",
        "                -1,\n",
        "                3 * self.dim,\n",
        "                H * W,\n",
        "            ),\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[:, :, 0 : self.dim],\n",
        "            qkv[:, :, self.dim : 2 * self.dim],\n",
        "            qkv[:, :, 2 * self.dim :],\n",
        "        )\n",
        "\n",
        "        # lightweight linear attention\n",
        "        q = self.kernel_func(q)\n",
        "        k = self.kernel_func(k)\n",
        "\n",
        "        # linear matmul\n",
        "        trans_k = k.transpose(-1, -2)\n",
        "\n",
        "        v = F.pad(v, (0, 0, 0, 1), mode=\"constant\", value=1)\n",
        "        vk = torch.matmul(v, trans_k)\n",
        "        out = torch.matmul(vk, q)\n",
        "        if out.dtype == torch.bfloat16:\n",
        "            out = out.float()\n",
        "        out = out[:, :, :-1] / (out[:, :, -1:] + self.eps)\n",
        "\n",
        "        out = torch.reshape(out, (B, -1, H, W))\n",
        "        return out\n",
        "\n",
        "    @torch.autocast(device_type=\"cuda\", enabled=False)\n",
        "    def relu_quadratic_att(self, qkv: torch.Tensor) -> torch.Tensor:\n",
        "        B, _, H, W = list(qkv.size())\n",
        "\n",
        "        qkv = torch.reshape(\n",
        "            qkv,\n",
        "            (\n",
        "                B,\n",
        "                -1,\n",
        "                3 * self.dim,\n",
        "                H * W,\n",
        "            ),\n",
        "        )\n",
        "        q, k, v = (\n",
        "            qkv[:, :, 0 : self.dim],\n",
        "            qkv[:, :, self.dim : 2 * self.dim],\n",
        "            qkv[:, :, 2 * self.dim :],\n",
        "        )\n",
        "\n",
        "        q = self.kernel_func(q)\n",
        "        k = self.kernel_func(k)\n",
        "\n",
        "        att_map = torch.matmul(k.transpose(-1, -2), q)  # b h n n\n",
        "        original_dtype = att_map.dtype\n",
        "        if original_dtype in [torch.float16, torch.bfloat16]:\n",
        "            att_map = att_map.float()\n",
        "        att_map = att_map / (torch.sum(att_map, dim=2, keepdim=True) + self.eps)  # b h n n\n",
        "        att_map = att_map.to(original_dtype)\n",
        "        out = torch.matmul(v, att_map)  # b h d n\n",
        "\n",
        "        out = torch.reshape(out, (B, -1, H, W))\n",
        "        return out\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # generate multi-scale q, k, v\n",
        "        qkv = self.qkv(x)\n",
        "        multi_scale_qkv = [qkv]\n",
        "        for op in self.aggreg:\n",
        "            multi_scale_qkv.append(op(qkv))\n",
        "        qkv = torch.cat(multi_scale_qkv, dim=1)\n",
        "\n",
        "        H, W = list(qkv.size())[-2:]\n",
        "        if H * W > self.dim:\n",
        "            out = self.relu_linear_att(qkv).to(qkv.dtype)\n",
        "        else:\n",
        "            out = self.relu_quadratic_att(qkv)\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        heads_ratio: float = 1.0,\n",
        "        dim=32,\n",
        "        expand_ratio: float = 4,\n",
        "        scales: tuple[int, ...] = (5,),\n",
        "        norm: str = \"bn2d\",\n",
        "        act_func: str = \"hswish\",\n",
        "        context_module: str = \"LiteMLA\",\n",
        "        local_module: str = \"MBConv\",\n",
        "    ):\n",
        "        super(EfficientViTBlock, self).__init__()\n",
        "        if context_module == \"LiteMLA\":\n",
        "            self.context_module = ResidualBlock(\n",
        "                LiteMLA(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    heads_ratio=heads_ratio,\n",
        "                    dim=dim,\n",
        "                    norm=(None, norm),\n",
        "                    scales=scales,\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"context_module {context_module} is not supported\")\n",
        "        if local_module == \"MBConv\":\n",
        "            self.local_module = ResidualBlock(\n",
        "                MBConv(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    expand_ratio=expand_ratio,\n",
        "                    use_bias=(True, True, False),\n",
        "                    norm=(None, None, norm),\n",
        "                    act_func=(act_func, act_func, None),\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        elif local_module == \"GLUMBConv\":\n",
        "            self.local_module = ResidualBlock(\n",
        "                GLUMBConv(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=in_channels,\n",
        "                    expand_ratio=expand_ratio,\n",
        "                    use_bias=(True, True, False),\n",
        "                    norm=(None, None, norm),\n",
        "                    act_func=(act_func, act_func, None),\n",
        "                ),\n",
        "                IdentityLayer(),\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(f\"local_module {local_module} is not supported\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.context_module(x)\n",
        "        x = self.local_module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#                             Functional Blocks                                 #\n",
        "#################################################################################\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        main: Optional[nn.Module],\n",
        "        shortcut: Optional[nn.Module],\n",
        "        post_act=None,\n",
        "        pre_norm: Optional[nn.Module] = None,\n",
        "    ):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.pre_norm = pre_norm\n",
        "        self.main = main\n",
        "        self.shortcut = shortcut\n",
        "        self.post_act = build_act(post_act)\n",
        "\n",
        "    def forward_main(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.pre_norm is None:\n",
        "            return self.main(x)\n",
        "        else:\n",
        "            return self.main(self.pre_norm(x))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.main is None:\n",
        "            res = x\n",
        "        elif self.shortcut is None:\n",
        "            res = self.forward_main(x)\n",
        "        else:\n",
        "            res = self.forward_main(x) + self.shortcut(x)\n",
        "            if self.post_act:\n",
        "                res = self.post_act(res)\n",
        "        return res\n",
        "\n",
        "\n",
        "class DAGBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        inputs: dict[str, nn.Module],\n",
        "        merge: str,\n",
        "        post_input: Optional[nn.Module],\n",
        "        middle: nn.Module,\n",
        "        outputs: dict[str, nn.Module],\n",
        "    ):\n",
        "        super(DAGBlock, self).__init__()\n",
        "\n",
        "        self.input_keys = list(inputs.keys())\n",
        "        self.input_ops = nn.ModuleList(list(inputs.values()))\n",
        "        self.merge = merge\n",
        "        self.post_input = post_input\n",
        "\n",
        "        self.middle = middle\n",
        "\n",
        "        self.output_keys = list(outputs.keys())\n",
        "        self.output_ops = nn.ModuleList(list(outputs.values()))\n",
        "\n",
        "    def forward(self, feature_dict: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
        "        feat = [op(feature_dict[key]) for key, op in zip(self.input_keys, self.input_ops)]\n",
        "        if self.merge == \"add\":\n",
        "            feat = list_sum(feat)\n",
        "        elif self.merge == \"cat\":\n",
        "            feat = torch.concat(feat, dim=1)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        if self.post_input is not None:\n",
        "            feat = self.post_input(feat)\n",
        "        feat = self.middle(feat)\n",
        "        for key, op in zip(self.output_keys, self.output_ops):\n",
        "            feature_dict[key] = op(feat)\n",
        "        return feature_dict\n",
        "\n",
        "\n",
        "class OpSequential(nn.Module):\n",
        "    def __init__(self, op_list: list[Optional[nn.Module]]):\n",
        "        super(OpSequential, self).__init__()\n",
        "        valid_op_list = []\n",
        "        for op in op_list:\n",
        "            if op is not None:\n",
        "                valid_op_list.append(op)\n",
        "        self.op_list = nn.ModuleList(valid_op_list)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for op in self.op_list:\n",
        "            x = op(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EfX78Q2-Sqaz"
      },
      "outputs": [],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from omegaconf import MISSING, OmegaConf\n",
        "\n",
        "from efficientvit.models.nn.act import build_act\n",
        "from efficientvit.models.nn.norm import build_norm\n",
        "from efficientvit.models.nn.ops import (\n",
        "    ChannelDuplicatingPixelUnshuffleUpSampleLayer,\n",
        "    ConvLayer,\n",
        "    ConvPixelShuffleUpSampleLayer,\n",
        "    ConvPixelUnshuffleDownSampleLayer,\n",
        "    EfficientViTBlock,\n",
        "    IdentityLayer,\n",
        "    InterpolateConvUpSampleLayer,\n",
        "    OpSequential,\n",
        "    PixelUnshuffleChannelAveragingDownSampleLayer,\n",
        "    ResBlock,\n",
        "    ResidualBlock,\n",
        ")\n",
        "\n",
        "__all__ = [\"DCAE\", \"dc_ae_f32c32\", \"dc_ae_f64c128\", \"dc_ae_f128c512\"]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EncoderConfig:\n",
        "    in_channels: int = MISSING\n",
        "    latent_channels: int = MISSING\n",
        "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
        "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
        "    block_type: Any = \"ResBlock\"\n",
        "    norm: str = \"trms2d\"\n",
        "    act: str = \"silu\"\n",
        "    downsample_block_type: str = \"ConvPixelUnshuffle\"\n",
        "    downsample_match_channel: bool = True\n",
        "    downsample_shortcut: Optional[str] = \"averaging\"\n",
        "    out_norm: Optional[str] = None\n",
        "    out_act: Optional[str] = None\n",
        "    out_shortcut: Optional[str] = \"averaging\"\n",
        "    double_latent: bool = False\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DecoderConfig:\n",
        "    in_channels: int = MISSING\n",
        "    latent_channels: int = MISSING\n",
        "    in_shortcut: Optional[str] = \"duplicating\"\n",
        "    width_list: tuple[int, ...] = (128, 256, 512, 512, 1024, 1024)\n",
        "    depth_list: tuple[int, ...] = (2, 2, 2, 2, 2, 2)\n",
        "    block_type: Any = \"ResBlock\"\n",
        "    norm: Any = \"trms2d\"\n",
        "    act: Any = \"silu\"\n",
        "    upsample_block_type: str = \"ConvPixelShuffle\"\n",
        "    upsample_match_channel: bool = True\n",
        "    upsample_shortcut: str = \"duplicating\"\n",
        "    out_norm: str = \"trms2d\"\n",
        "    out_act: str = \"relu\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DCAEConfig:\n",
        "    in_channels: int = 3\n",
        "    latent_channels: int = 32\n",
        "    encoder: EncoderConfig = field(\n",
        "        default_factory=lambda: EncoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
        "    )\n",
        "    decoder: DecoderConfig = field(\n",
        "        default_factory=lambda: DecoderConfig(in_channels=\"${..in_channels}\", latent_channels=\"${..latent_channels}\")\n",
        "    )\n",
        "    use_quant_conv: bool = False\n",
        "\n",
        "    pretrained_path: Optional[str] = None\n",
        "    pretrained_source: str = \"dc-ae\"\n",
        "\n",
        "    scaling_factor: Optional[float] = None\n",
        "\n",
        "\n",
        "def build_block(\n",
        "    block_type: str, in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str]\n",
        ") -> nn.Module:\n",
        "    if block_type == \"ResBlock\":\n",
        "        assert in_channels == out_channels\n",
        "        main_block = ResBlock(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            use_bias=(True, False),\n",
        "            norm=(None, norm),\n",
        "            act_func=(act, None),\n",
        "        )\n",
        "        block = ResidualBlock(main_block, IdentityLayer())\n",
        "    elif block_type == \"EViT_GLU\":\n",
        "        assert in_channels == out_channels\n",
        "        block = EfficientViTBlock(in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=())\n",
        "    elif block_type == \"EViTS5_GLU\":\n",
        "        assert in_channels == out_channels\n",
        "        block = EfficientViTBlock(in_channels, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=(5,))\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_stage_main(\n",
        "    width: int, depth: int, block_type: str | list[str], norm: str, act: str, input_width: int\n",
        ") -> list[nn.Module]:\n",
        "    assert isinstance(block_type, str) or (isinstance(block_type, list) and depth == len(block_type))\n",
        "    stage = []\n",
        "    for d in range(depth):\n",
        "        current_block_type = block_type[d] if isinstance(block_type, list) else block_type\n",
        "        block = build_block(\n",
        "            block_type=current_block_type,\n",
        "            in_channels=width if d > 0 else input_width,\n",
        "            out_channels=width,\n",
        "            norm=norm,\n",
        "            act=act,\n",
        "        )\n",
        "        stage.append(block)\n",
        "    return stage\n",
        "\n",
        "\n",
        "def build_downsample_block(block_type: str, in_channels: int, out_channels: int, shortcut: Optional[str]) -> nn.Module:\n",
        "    if block_type == \"Conv\":\n",
        "        block = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=2,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "    elif block_type == \"ConvPixelUnshuffle\":\n",
        "        block = ConvPixelUnshuffleDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported for downsampling\")\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"averaging\":\n",
        "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=2\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for downsample\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_upsample_block(block_type: str, in_channels: int, out_channels: int, shortcut: Optional[str]) -> nn.Module:\n",
        "    if block_type == \"ConvPixelShuffle\":\n",
        "        block = ConvPixelShuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    elif block_type == \"InterpolateConv\":\n",
        "        block = InterpolateConvUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=3, factor=2\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"block_type {block_type} is not supported for upsampling\")\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"duplicating\":\n",
        "        shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=2\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for upsample\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_encoder_project_in_block(in_channels: int, out_channels: int, factor: int, downsample_block_type: str):\n",
        "    if factor == 1:\n",
        "        block = ConvLayer(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            use_bias=True,\n",
        "            norm=None,\n",
        "            act_func=None,\n",
        "        )\n",
        "    elif factor == 2:\n",
        "        block = build_downsample_block(\n",
        "            block_type=downsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"downsample factor {factor} is not supported for encoder project in\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_encoder_project_out_block(\n",
        "    in_channels: int, out_channels: int, norm: Optional[str], act: Optional[str], shortcut: Optional[str]\n",
        "):\n",
        "    block = OpSequential(\n",
        "        [\n",
        "            build_norm(norm),\n",
        "            build_act(act),\n",
        "            ConvLayer(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                use_bias=True,\n",
        "                norm=None,\n",
        "                act_func=None,\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"averaging\":\n",
        "        shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for encoder project out\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_decoder_project_in_block(in_channels: int, out_channels: int, shortcut: Optional[str]):\n",
        "    block = ConvLayer(\n",
        "        in_channels=in_channels,\n",
        "        out_channels=out_channels,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        use_bias=True,\n",
        "        norm=None,\n",
        "        act_func=None,\n",
        "    )\n",
        "    if shortcut is None:\n",
        "        pass\n",
        "    elif shortcut == \"duplicating\":\n",
        "        shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(\n",
        "            in_channels=in_channels, out_channels=out_channels, factor=1\n",
        "        )\n",
        "        block = ResidualBlock(block, shortcut_block)\n",
        "    else:\n",
        "        raise ValueError(f\"shortcut {shortcut} is not supported for decoder project in\")\n",
        "    return block\n",
        "\n",
        "\n",
        "def build_decoder_project_out_block(\n",
        "    in_channels: int, out_channels: int, factor: int, upsample_block_type: str, norm: Optional[str], act: Optional[str]\n",
        "):\n",
        "    layers: list[nn.Module] = [\n",
        "        build_norm(norm, in_channels),\n",
        "        build_act(act),\n",
        "    ]\n",
        "    if factor == 1:\n",
        "        layers.append(\n",
        "            ConvLayer(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                use_bias=True,\n",
        "                norm=None,\n",
        "                act_func=None,\n",
        "            )\n",
        "        )\n",
        "    elif factor == 2:\n",
        "        layers.append(\n",
        "            build_upsample_block(\n",
        "                block_type=upsample_block_type, in_channels=in_channels, out_channels=out_channels, shortcut=None\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"upsample factor {factor} is not supported for decoder project out\")\n",
        "    return OpSequential(layers)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, cfg: EncoderConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        num_stages = len(cfg.width_list)\n",
        "        self.num_stages = num_stages\n",
        "        assert len(cfg.depth_list) == num_stages\n",
        "        assert len(cfg.width_list) == num_stages\n",
        "        assert isinstance(cfg.block_type, str) or (\n",
        "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
        "        )\n",
        "\n",
        "        self.project_in = build_encoder_project_in_block(\n",
        "            in_channels=cfg.in_channels,\n",
        "            out_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
        "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
        "            downsample_block_type=cfg.downsample_block_type,\n",
        "        )\n",
        "\n",
        "        self.stages: list[OpSequential] = []\n",
        "        for stage_id, (width, depth) in enumerate(zip(cfg.width_list, cfg.depth_list)):\n",
        "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
        "            stage = build_stage_main(\n",
        "                width=width, depth=depth, block_type=block_type, norm=cfg.norm, act=cfg.act, input_width=width\n",
        "            )\n",
        "\n",
        "            if stage_id < num_stages - 1 and depth > 0:\n",
        "                downsample_block = build_downsample_block(\n",
        "                    block_type=cfg.downsample_block_type,\n",
        "                    in_channels=width,\n",
        "                    out_channels=cfg.width_list[stage_id + 1] if cfg.downsample_match_channel else width,\n",
        "                    shortcut=cfg.downsample_shortcut,\n",
        "                )\n",
        "                stage.append(downsample_block)\n",
        "            self.stages.append(OpSequential(stage))\n",
        "        self.stages = nn.ModuleList(self.stages)\n",
        "\n",
        "        self.project_out = build_encoder_project_out_block(\n",
        "            in_channels=cfg.width_list[-1],\n",
        "            out_channels=2 * cfg.latent_channels if cfg.double_latent else cfg.latent_channels,\n",
        "            norm=cfg.out_norm,\n",
        "            act=cfg.out_act,\n",
        "            shortcut=cfg.out_shortcut,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.project_in(x)\n",
        "        for stage in self.stages:\n",
        "            if len(stage.op_list) == 0:\n",
        "                continue\n",
        "            x = stage(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, cfg: DecoderConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        num_stages = len(cfg.width_list)\n",
        "        self.num_stages = num_stages\n",
        "        assert len(cfg.depth_list) == num_stages\n",
        "        assert len(cfg.width_list) == num_stages\n",
        "        assert isinstance(cfg.block_type, str) or (\n",
        "            isinstance(cfg.block_type, list) and len(cfg.block_type) == num_stages\n",
        "        )\n",
        "        assert isinstance(cfg.norm, str) or (isinstance(cfg.norm, list) and len(cfg.norm) == num_stages)\n",
        "        assert isinstance(cfg.act, str) or (isinstance(cfg.act, list) and len(cfg.act) == num_stages)\n",
        "\n",
        "        self.project_in = build_decoder_project_in_block(\n",
        "            in_channels=cfg.latent_channels,\n",
        "            out_channels=cfg.width_list[-1],\n",
        "            shortcut=cfg.in_shortcut,\n",
        "        )\n",
        "\n",
        "        self.stages: list[OpSequential] = []\n",
        "        for stage_id, (width, depth) in reversed(list(enumerate(zip(cfg.width_list, cfg.depth_list)))):\n",
        "            stage = []\n",
        "            if stage_id < num_stages - 1 and depth > 0:\n",
        "                upsample_block = build_upsample_block(\n",
        "                    block_type=cfg.upsample_block_type,\n",
        "                    in_channels=cfg.width_list[stage_id + 1],\n",
        "                    out_channels=width if cfg.upsample_match_channel else cfg.width_list[stage_id + 1],\n",
        "                    shortcut=cfg.upsample_shortcut,\n",
        "                )\n",
        "                stage.append(upsample_block)\n",
        "\n",
        "            block_type = cfg.block_type[stage_id] if isinstance(cfg.block_type, list) else cfg.block_type\n",
        "            norm = cfg.norm[stage_id] if isinstance(cfg.norm, list) else cfg.norm\n",
        "            act = cfg.act[stage_id] if isinstance(cfg.act, list) else cfg.act\n",
        "            stage.extend(\n",
        "                build_stage_main(\n",
        "                    width=width,\n",
        "                    depth=depth,\n",
        "                    block_type=block_type,\n",
        "                    norm=norm,\n",
        "                    act=act,\n",
        "                    input_width=(\n",
        "                        width if cfg.upsample_match_channel else cfg.width_list[min(stage_id + 1, num_stages - 1)]\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "            self.stages.insert(0, OpSequential(stage))\n",
        "        self.stages = nn.ModuleList(self.stages)\n",
        "\n",
        "        self.project_out = build_decoder_project_out_block(\n",
        "            in_channels=cfg.width_list[0] if cfg.depth_list[0] > 0 else cfg.width_list[1],\n",
        "            out_channels=cfg.in_channels,\n",
        "            factor=1 if cfg.depth_list[0] > 0 else 2,\n",
        "            upsample_block_type=cfg.upsample_block_type,\n",
        "            norm=cfg.out_norm,\n",
        "            act=cfg.out_act,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.project_in(x)\n",
        "        for stage in reversed(self.stages):\n",
        "            if len(stage.op_list) == 0:\n",
        "                continue\n",
        "            x = stage(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, cfg: DCAEConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.encoder = Encoder(cfg.encoder)\n",
        "        self.decoder = Decoder(cfg.decoder)\n",
        "\n",
        "        if self.cfg.pretrained_path is not None:\n",
        "            self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.cfg.pretrained_source == \"dc-ae\":\n",
        "            state_dict = torch.load(self.cfg.pretrained_path, map_location=\"cpu\", weights_only=True)[\"state_dict\"]\n",
        "            self.load_state_dict(state_dict)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def spatial_compression_ratio(self) -> int:\n",
        "        return 2 ** (self.decoder.num_stages - 1)\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.encoder(x)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor, global_step: int) -> torch.Tensor:\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x, torch.tensor(0), {}\n",
        "\n",
        "\n",
        "def dc_ae_f32c32(name: str, pretrained_path: str) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f32c32-in-1.0\", \"dc-ae-f32c32-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=32 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[0,4,8,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[0,5,10,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu]\"\n",
        "        )\n",
        "    elif name in [\"dc-ae-f32c32-sana-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=32 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024] encoder.depth_list=[2,2,2,3,3,3] \"\n",
        "            \"encoder.downsample_block_type=Conv \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViTS5_GLU,EViTS5_GLU,EViTS5_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024] decoder.depth_list=[3,3,3,3,3,3] \"\n",
        "            \"decoder.upsample_block_type=InterpolateConv \"\n",
        "            \"decoder.norm=trms2d decoder.act=silu \"\n",
        "            \"scaling_factor=0.41407\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def dc_ae_f64c128(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f64c128-in-1.0\", \"dc-ae-f64c128-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=128 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024,2048] encoder.depth_list=[0,4,8,2,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024,2048] decoder.depth_list=[0,5,10,2,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu,silu]\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def dc_ae_f128c512(name: str, pretrained_path: Optional[str] = None) -> DCAEConfig:\n",
        "    if name in [\"dc-ae-f128c512-in-1.0\", \"dc-ae-f128c512-mix-1.0\"]:\n",
        "        cfg_str = (\n",
        "            \"latent_channels=512 \"\n",
        "            \"encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"encoder.width_list=[128,256,512,512,1024,1024,2048,2048] encoder.depth_list=[0,4,8,2,2,2,2,2] \"\n",
        "            \"decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU,EViT_GLU] \"\n",
        "            \"decoder.width_list=[128,256,512,512,1024,1024,2048,2048] decoder.depth_list=[0,5,10,2,2,2,2,2] \"\n",
        "            \"decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu,silu,silu]\"\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    cfg = OmegaConf.from_dotlist(cfg_str.split(\" \"))\n",
        "    cfg: DCAEConfig = OmegaConf.to_object(OmegaConf.merge(OmegaConf.structured(DCAEConfig), cfg))\n",
        "    cfg.pretrained_path = pretrained_path\n",
        "    return cfg\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}